# 一、分布式事务

## **01 为什么需要分布式事务**

由于近十年互联网的发展非常迅速，很多网站的访问越来越大，集中式环境已经不能满足业务的需要了，只能按照业务为单位进行数据拆分(包含：垂直拆分与水平拆分)，以及按照业务为单位提供服务，从早期的集中式转变为面向服务架构的分布式应用环境。

举一个典型的例子，阿里的淘宝网站随着访问量越来越大，只能按照商品、订单、用户、店铺等业务为单位进行数据库拆分，以及按照业务为单位提供服务接口。

![img](https://pic2.zhimg.com/80/v2-886860e07964f3e7cecd0c439aec5789_720w.jpg)

这个时候 为了完成一个简单的业务功能，比如：购买商品后扣款，有可能需要横跨多个服务，涉及用户订单、商品库存、支付等多个数据库，而这些操作又需要在同一个事务中完，这就涉及到到了分布式事务。

本质上来说，分布式事务就是为了保证不同资源服务器的数据一致性。

## **02 分布式的一致性理论**

最早加州大学伯克利分校 Eric Brewer教授提出一个分布式系统特性的CAP理论。

**1.CAP 理论的不可能三角**

![img](https://pic3.zhimg.com/80/v2-e6b9d06542ea240216b7bcd51512b086_720w.jpg)

- 一致性（Consistency）
- 可用性（Availability）
- 分区容错性（Partition tolerance）

在分布式系统中，是不存在同时满足一致性 Consistency、可用性 Availability和分区容错性 Partition Tolerance三者的。

**一句话总结：一致性、可用性和分区容错在分布式事务中不可兼得。**

在绝大多数的场景，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。

这也是是后来发展出的BASE理论的基础。

**2.BASE 理论**

![img](https://pic2.zhimg.com/80/v2-127fb35f49eedcb5e8c4acd4dab4e319_720w.jpg)

- Basically Available（基本可用）
- Soft state（柔软状态）
- Eventually consistent（最终一致性）三个短语的简写。

BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。

高并发下如何保证接口的幂等性：https://mp.weixin.qq.com/s/7P2KbWjjX5YPZCInoox-xQ





## 03 分布式事务的解决方案

**1.基于XA协议的两阶段提交 2PC(2-phase commit protocol)**

XA是一个分布式事务协议，XA中大致分为两部分：事务管理器和本地资源管理器,其中本地资源管理器往往由数据库实现，而事务管理器作为全局的调度者，负责各个本地资源的提交和回滚。

![img](https://pic2.zhimg.com/80/v2-1caab0d161ae0af9a40bf5972a7e8f95_720w.jpg)



**大致的流程：**

- 第一阶段是表决阶段，所有参与者都将本事务能否成功的信息反馈发给协调者；
- 第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交或者回滚。

**优缺点**

尽量保证了数据的强一致，实现成本较低，在各大主流数据库都有自己实现，存在单点故障问题、性能问题、跨数据库问题。

**2.事务补偿TCC模式**

TCC方案其实是两阶段提交的一种改进，将整个业务逻辑的每个分支显式的分成了Try、Confirm、Cancel三个操作。

Try部分完成业务的准备工作，confirm部分完成业务的提交，cancel部分完成事务的回滚，基本原理如下图所示：

![img](https://pic1.zhimg.com/80/v2-ef8229600f354ace4104e736a7657a98_720w.jpg)

**优缺点**

对代码有侵入性，降低了锁冲突，提高了吞吐量，缺点是有时候并没有那么好实现。

**案例**

蚂蚁金服的DTS(prepare、commit、rollback)

**3.消息队列最终一致性方案**

干脆不用本地的消息表了，直接基于MQ来实现事务。比如阿里的RocketMQ就支持消息事务!


**流程**：

1.A系统先发送一个prepared消息到MQ，如果这个prepared消息发送失败,那么就直接取消操作,不执行了

2.如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉MQ发送确认消息，如果失败就告诉MQ回滚消息

3.如果发送了确认消息，那么此时B系统会接收到确认消息，然后执行本地的事务

4.MQ会自动定时轮询所有prepared消息回调你的接口，问你这个消息是不是本地事务处理失败了，所有没发送确认的消息,是继续重试还是回滚？
这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，别确认消息发送失败了。

5.如果系统B的事务失败了咋办？
重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚；或者是发送报警由人工来手工回滚和补偿



可靠消息最终一致性方案![img](http://img3.sycdn.imooc.com/5ef0104c00013f0a10310494.jpg)






总之，分布式系统中事务更多的是对CAP权衡，在实际应用中，根据业务要求、开发人员情况以及所用框架不同进行调整。

本文介绍了分布式事务的一些特性和解决方案，后续将持续输出成体系的分布式系统相关系列文章。



# 分布式一致性算法

* 集中式与分布式
  * 集中式
  * 分布式
  * 分布式事务
* 一致性协议
  * 2PC：Two-Phase Commit二阶段提交协议
  * 3PC:Three-phase Commit 三阶段提交协议
  * Paxos算法
  * RAFT算法
* 总结

集中式与分布式

**集中式**

就是将所有的业务都部署在一个中心主机（节点）上，所有的功能都由这个主机集中处理。

**特点**

**部署结构简单、不需要考虑多个主机之间的分布式协作问题。**

**分布式**

分布式系统：指将**硬件**或者**软件组件部署在不同的网络计算机上**，彼此之间仅**仅通过消息传递**进行通信和协调的系统。

**特点**

**1.分布性**：多台计算机可空间上随意分布，跨机房、跨城市都可以。

**2.对等性**：分布式系统中没有主/从之分，都是对等的节点或者服务。

* **副本**：指分布式系统对**数据或服务冗余**，以此提供高可用。
* **数据副本**：是指在不同的节点上持久化一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取到该数据，这是分布式系统数据丢失问题最为有效的手段。
* **服务副本**：指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理。

**3.并发性：**分布式系统中的多个节点，可能会并发地操作一些共享资源，诸如数据库或分布式存储等。

**4.缺乏全局时钟：**一个典型的分布式系统是由一系列在空间上随意分布的进程组成，进程彼此之间通过消息进行通信。因此，无法判断两个事件谁先谁后。**可使用逻辑时钟。**

**5.故障总是会发生：**除非需求指标允许，在系统设计时不能放过任何异常情况。如宕机、网络分区、网络超时等。

每一次分布式系统的请求与响应三态：**成功，失败，超时。**

**超时情况：**

1.没有成功发送到接收方，在发送过程中发生信息丢失。

2.成功发送到接收方，并成功处理，但返回给发送方过程中发生信息丢失。**所以需要有幂等。**

**分布式事务**

分布式事务是指**事务的参与者**，**支持事务的服务器**，**资源服务器**以及**事务管理器**分别位于分布式系统的**不同节点之上。**通常一个分布式事务中会涉及对多个数据源或业务系统的操作。分布式事务也可以被定义为一种嵌套型的事务，同时也就具有了ACID事务的特性。

**CAP理论**

* Consistency(**一致性**)：数据一致更新，所有数据变动都是同步的（强一致性）。
* Availability(**可用性**)：好的响应性能
* Partition tolerance(**分区容错性**) ：可靠性

**分区容错性**：分布式系统在遇到任何网络分区故障的时候，任然需要保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。

**网络分区**：是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个网络的环境被切成了若干个孤立的区域。

![img](https://pics5.baidu.com/feed/b8014a90f603738dddc2af174b60b456f919ecfa.jpeg?token=75a1c2e87c86ed0a5ca1b32f9192eb75)

定理：**任何分布式系统只可同时满足二点，没法三者兼顾。**

需要根据实际业务进行取舍。

![img](https://pics6.baidu.com/feed/d788d43f8794a4c21bd8a29af98f1fd2ac6e390e.jpeg?token=cec1e198f76574e808c2f93da2d0a191)

* **CA系统（放弃P）**：指将所有数据（或者仅仅是那些与事务相关的数据）都放在一个分布式节点上，就不会存在网络分区。所以强一致性以及可用性得到满足。
* **CP系统（放弃A）**：如果要求数据在各个服务器上是强一致的，然而网络分区会导致同步时间无限延长，那么如此一来可用性就得不到保障了。坚持事务ACID（原子性、一致性、隔离性和持久性）的传统数据库以及对结果一致性非常敏感的应用通常会做出这样的选择。
* **AP系统（放弃C）**：这里所说的放弃一致性，并不是完全放弃数据一致性，而**是放弃数据的强一致性，而保留数据的最终一致性。**如果即要求系统高可用又要求分区容错，那么就要放弃一致性了。因为一旦发生网络分区，节点之间将无法通信，为什么满足高可用，每个节点只能用本地数据提供服务，这样就会导致数据不一致。一些遵守BASE原则数据库，（如：Cassandra、CouchDB等）往往会放宽对一致性的要求（满足最终一致性即可），一次来获取基本的可用性。

**BASE理论**

* **Basically Available基本可用**：指分布式系统在出现不可预知的故障的时候，允许损失部分可用性——但不是系统不可用。
  * **响应时间上的损失**：假如正常一个在线搜索0.5秒之内返回，但由于故障（机房断电或网络不通），查询结果的响应时间增加到1—2秒。
  * **功能上的损失**：如果流量激增或者一个请求需要多个服务间配合，而此时有的服务发生了故障，这时需要进行服务降级，进而保护系统稳定性。
* **Soft state软状态**：允许系统在不同节点的数据副本之间进行数据同步的过程存在延迟。
* **Eventually consistent最终一致**：最终数据是一致的就可以了，而不是时时高一致。



**一致性协议**

一致性协议：为了使基于分布式系统架构下的所有节点进行事务处理过程中能够保持原子性和一致性而设计的一种算法。通常有**二阶段**提交协议、**三阶段**提交协议、**Paxos**、**Zookeeper的ZAB协议、Raft、Pbft**等。

2PC、3PC引入了两个概念。

**协调者：**负责统一调度分布式节点的执行逻辑

**参与者**：被调度的分布式节点

**2PC：Two-Phase Commit二阶段提交协议**

**二阶段**主要采取：**先尝试，后提交。**

![img](https://pics2.baidu.com/feed/79f0f736afc379319f01d7dd17bfb34242a9115f.jpeg?token=64d711682b8e3c2a7097c23454473220)

![img](https://pics5.baidu.com/feed/5366d0160924ab18fd45a71acb81e2ca7a890bca.jpeg?token=ac31b4fbeae7a772a322084706bfc804)

![img](https://pics0.baidu.com/feed/810a19d8bc3eb135c73f42885065acd4fc1f4488.jpeg?token=f5a54cb50b566d79bdaa84f10884e7f6)

**2PC优缺点**

*  **二阶段优点**：原理简单，实现方便；解决分布式事务的原子性，要么全部执行成功，要么全部执行失败
* **二阶段缺点**：
  * **1.同步阻塞**：在提交执行过程中，各个参与者都在等待其他参与者响应的过程中，将无法执行其他操作
  * **2.单点问题**：只有一个协调者，协调者挂掉，整个二阶段提交流程无法执行；更为严重是，在阶段二时，协调者出现问题，那参与者将会一直处于锁定事务状态中，无法继续完成事务操作。
  * **3.数据不一致**：在阶段二，协调者发送了Commit请求后，发生了网络故障，导致只有部分参与者收到commit请求，并执行提交操作，就导致数据不一致问题。
  * **4.太过保守**：阶段一中，若参与者出现故障，协调者无法收到参与者的询问反馈，只能通过自身超时机制来中断事务。这样的策略显得过于保守。

**3PC:Three-phase Commit 三阶段提交协议**

因为2PC有很多问题，所以在2PC基础上，改进为**3PC：canCommit、preCommit、doCommit三个阶段。**

**改进点：**

1.3PC是将2PC的第一阶段分为两个阶段，先发起事务询问，再执行事务。

2.同时在协调者、参与者中引入超时机制。

![img](https://pics5.baidu.com/feed/0e2442a7d933c895ceea603c2b6877f783020067.jpeg?token=1eeb8f1ca0d55fc14847d3f85d32bba4)

3PC-第一阶段

![img](https://pics0.baidu.com/feed/b812c8fcc3cec3fdc9660a222cf3d0388694278b.jpeg?token=d4a855fe221d20a1cc6fc0014df86f9c)

![img](https://pics1.baidu.com/feed/203fb80e7bec54e7a110de924d439f574dc26ac5.jpeg?token=66ce72e32d0d315b591c6f34224c9943)

3PC-事务中断1

![img](https://pics2.baidu.com/feed/7acb0a46f21fbe09681cb562901b08348644ad8c.jpeg?token=e9b79d525ad052ebce84e7ffefaadac4)

3PC-第三阶段

![img](https://pics4.baidu.com/feed/4034970a304e251f38ded9a959fdcd107f3e533c.jpeg?token=fd7b73cad8ee6b5777a35d4af5a2dc4e)

3PC-事务中断2

**3PC优缺点**

*  **三阶段优点：**
  * **降低了二阶段的同步阻塞范围**（在第二阶段，只要参与者收到preCommit请求，就会执行事务，此后，不管能不能收到协调者的doCommit请求,都会执行事务提交，不会出现阻塞问题）
  * **解决单点问题**：进入阶段三会出现两种情况： 1：协调者出现问题； 2：协调者与参与者之间出现网络故障；
  * **都导致参与者无法收到doCommit请求，但参与者在超时之后都会提交事务**
* **三阶段缺点：**
  * **数据不一致**：参与者收到preCommit请求，此时如果出现网络分区，协调者与参与者之间无法进行正常网络通信，参与者在超时之后还是会进行事务提交，就会出现数据不一致。

所以2PC、3PC各有优缺点，可根据实际业务场景进行选择。既然2PC、3PC都会产生数据不一致。下面我们来看一看分布式领域常用的一致性算法。

**Paxos算法**

Paxos算法是**莱斯利·兰伯特(Leslie Lamport)\**1990年提出的\**基于消息传递**且**具有高度容错特性的一致性算法**，是目前公认的解决分布式一致性问题最有效的算法之一。 **Paxos算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。**

**Paxos以及下面的RAFT都假设不存在拜占庭将军问题，只考虑节点宕机、网络分区、消息不可靠等问题。属于CFT（Crash Fault Tolerance）算法。**

系统中有三种角色proposers，acceptors，和 learners。可以一个节点多个角色。

**1.proposers** 提出提案，提案信息包括提案编号和提议的 value；

**2.acceptor** 收到提案后可以接受（accept）提案，若提案获得多数派（majority）的 acceptors 的接受，则称该提案被批准（chosen）；

**3.learners** 只能“学习”被批准的提案。

**多数派：指 n / 2 +1 。n为总节点数量。**

Paxos算法分为**两个阶段**。具体如下：

* **阶段一：**

  * Proposer选择一个**提案编号N**，然后向**半数以上**的Acceptor发送编号为N的**Prepare请求**。

  * 如果一个Acceptor收到一个编号为N的Prepare请求，且N**大于**该Acceptor已经**响应过的**所有**Prepare请求**的编号，那么它就会将它已经**接受过的编号最大的提案（如果有的话）\**作为响应反馈给Proposer，同时该Acceptor承诺\**不再接受**任何**编号小于N的提案**。例如：一个acceptor已经响应过的所有prepare请求对应的提案编号分别为1、2、。。。。5和7，那么该acceptor在接收到一个编号为8的prepare请求后，就会将编号为7的提案作为响应反馈给Proposer。

* **阶段二**

  * 如果Proposer收到**半数以上**Acceptor对其发出的编号为N的Prepare请求的**响应**，那么它就会发送一个针对**[N,V]**提案**的**Accept请求**给**半数以上**的Acceptor。注意：V就是阶段一收到的响应中编号最大的提案的value**，如果响应中**不包含任何提案**，那么V就由Proposer**自己决定（任意值）**。
  * **如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor**没有**对编号**大于N**的**Prepare请求做出过**响应**，它就**接受该提案**。

![img](https://pics1.baidu.com/feed/a6efce1b9d16fdfa1244240343f4885395ee7bb2.jpeg?token=c7116c2646a45fd8b28d7d28e257de3f)

注意：Proposer可以随时丢弃提案，并且提出新的提案；Acceptor也可以随时响应，接受编号更大的提案。

**思考：如果两个Proposer还处于第一阶段时，互相提出编号更大的提案？会发生什么？**

这时候会出现“活锁”状态，陷入了无限死循环中（破坏了算法活性）。

**那需要怎么防止呢？**

可以选出一个主Proposer，只有主Proposer可以提出提案。

至于怎么选择，不属于Paxos的范畴，可以参考RAFT使用竞选，谁快谁当选；也可以参考PBFT的依次成为leader等。

**RAFT算法**

RAFT算法分为两个阶段：Leader选举，日志复制。也有三种角色，分别为：

**1.Leader（领导者）**：负责发送要进行共识的数据，如果客户端发送的数据不是发送到Leader而是其他角色，其他角色会进行转发至Leader。

**2.Follower（追随者）**：参与共识的角色

**3.Candidate（候选者）**：如果Follower没有收到Leader的心跳响应超过150——300ms，会进行Leader选举。

每个节点的身份都可以是以上三种中的其一。

* **Leader选举阶段**：
  * 所有节点初始状态为Follower状态，此时没有Leader，肯定会与Leader的心跳超时（一般150——300ms，随机的，这样就是想谁先发出竞选，谁当选leader），此时Candidate就会发出leader竞选给其他节点（大家快选我啊，leader挂掉了）；其他节点收到竞选请求，会响应同意，当一个Candidate收到大多数（n/2 + 1）节点的回复，就成为leader。然后与Candidate保持心跳连接。Raft有个Term（任期）的概念，只有在发生Leader选举阶段，term+1，表示新的leader产生，挂掉的节点，或者挂掉的leader重启后，会发现自己的term小于最新的，此时就会切换到日志复制，去同步之前丢失的消息。
  * 如果同时有多个Candidate发出竞选，并且都没有获得大多数投票，会一直进行竞选，直到选出leader

* **日志复制（是一个2PC提交）**
  * leader收到客户端或者其他节点转发过来需要共识的值，会跟随心跳一起广播给其他节点，进行写入
  * 其他节点写入后响应成功给leader，当leader收到大多数的follower响应的成功，发出commit命令
  * 其他节点收到commit后，进行事务提交，响应成功为leader，leader收到大多数的commit成功，Raft完成。

**如果leader没有挂掉，或者发生网络分区，就会一直是这个leader进行事务发起。**我这里只是对于算法正常流程的描述，强烈推荐动画版RAFT（看不懂算我输，不过记得回来点个赞，哈哈哈）



# 二、分布式锁

###  redis实现分布式锁




应用场景：互联网秒杀  抢购卷

模拟订单服务

##### 1.模拟搭建分布式服务

![](C:/Users/xule/Desktop/云端文档/04小课堂/medias/d03f5208a74ee2b6cedc94d884f1818.png)

##### 2.搭建nginx负载均衡

修改配置文件
1). 配置需要负载均衡的服务器
vi  nginx/conf/upstream.conf


```python
upstream LoadBalance{
        server 192.168.0.104:9083 weight=1 max_fails=2 fail_timeout=30s;
        server 192.168.0.104:9085 weight=1 max_fails=2 fail_timeout=30s;
}
```

2).配置反向代理
vi  nginx/conf/vhosts.conf

```python
 location /LoadBalance {
        proxy_set_header        X-Real-IP $remote_addr;
        proxy_set_header        Host $host;
        proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass  http://LoadBalance/;
    }

```



代理后的ip：http://117.48.214.8/LoadBalance/iov/redis/test_syn



##### 3.setnx实现分布式锁

> Redis  API ： http://redisdoc.com

 SETNX key value

可用版本： >= 1.0.0

时间复杂度： O(1)

只在键 key 不存在的情况下， 将键 key 的值设置为 value 。

若键 key 已经存在， 则 SETNX 命令不做任何动作。

SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。



扩展

```shell
1、SET key value
含义：

         将字符串值 value 关联到 key 。

         如果 key 已经持有其他值， SET 就覆写旧值，无视类型。

2、SETEX key seconds value

含义：

          将值 value 关联到 key ，并将 key 的生存时间设为 seconds (以秒为单位)。

          如果 key 已经存在， SETEX 命令将覆写旧值。

返回值：

          设置成功时返回 OK 。

          当 seconds 参数不合法时，返回一个错误。

3、SETNX key value

含义：

          将 key 的值设为 value ，当且仅当 key 不存在。

          若给定的 key 已经存在，则 SETNX 不做任何动作。

          SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。

返回值：

          设置成功，返回 1 。

          设置失败，返回 0 。

4、GETSET key value

含义：

         将给定 key 的值设为 value ，并返回 key 的旧值(old value)。

         当 key 存在但不是字符串类型时，返回一个错误。

返回值：

          返回给定 key 的旧值。

          当 key 没有旧值时，也即是， key 不存在时，返回 null 。
```





返回值

命令在设置成功时返回 1 ， 设置失败时返回 0 。


代码一：

```java
@RequestMapping(value = "/testRedis")
	public String testRedis(){
	    String key = "stock";
	    Integer value = 500;
	    //获取锁
        
	    String clinetId=UUID.randomUUID().toString();
	    //插入缓存
	    redisTemplate.opsForValue().set(key, value.toString());
//	    //取缓存
	   try {
//		   Boolean result = redisTemplate.opsForValue().setIfAbsent("lockKey", "xule");
//		   redisTemplate.expire(key, 10,	TimeUnit.SECONDS);
		   Boolean result = redisTemplate.opsForValue().setIfAbsent(key, clinetId,10,TimeUnit.SECONDS);
		    if(!result) {
		    	return "error";
		    }
		    lock.lock();
		    	 int stock=Integer.parseInt(redisTemplate.opsForValue().get("stock"));
		 	    if(stock>0) {
		 	    	int realStock=stock-1;
		 	    	 redisTemplate.opsForValue().set(key, realStock+"");
		 	    	 System.out.println("85扣减成功，库存剩余："+realStock);
		 	    	
		 	    }else {
		 	    	 System.out.println("扣减失败，库存不足");
		 	    }
		   return  	"end";
		
	} finally {
	if(clinetId.equals(redisTemplate.opsForValue().get("key"))) {
		  redisTemplate.delete(key);
	}
	}
	    
	}
```



##### 4.redission实现分布式锁



实现原理	

 ![](C:/Users/xule/Desktop/云端文档/04小课堂/medias/61bedf404e509525ed54974babd1acd.png)



1）引入依赖：

```xml
<dependency>
    <groupId>org.redisson</groupId>
    <artifactId>redisson</artifactId>
    <version>3.5.4</version>
</dependency>

```

2）配置application.yml

```java
    redisson:
        address: redis://117.48.214.8:7379
         password:superman
 
```




代码实现：

```java
/**Redission实现分布式锁
	 * @return
	 */
	@RequestMapping(value = "/testRedission")
	public String testRedission(){
	    String key = "stock";
        RLock lock = redisson.getLock("k2");
	   try {
		   //执行具体逻辑...
		    lock.lock();
		    	 int stock=Integer.parseInt(redisTemplate.opsForValue().get("stock"));
		 	    if(stock>0) {
		 	    	int realStock=stock-1;
		 	    	 redisTemplate.opsForValue().set(key, realStock+"");
		 	    	 System.out.println("9001扣减成功，库存剩余："+realStock);
		 	    	
		 	    }else {
		 	    	 System.out.println("扣减失败，库存不足");
		 	    }
		   return  	"end";
		
	} finally {
		 lock.unlock();
	}
	    
	}
```

相关博客：https://www.cnblogs.com/chenkeyu/p/8514250.html

 

### Zookeeper实现分布式锁



##### 1.相关概念

- 有序节点：顾名思义就是有顺序的节点。zk会在生成节点时根据现有的节点数量添加整数序号。比如已经存在节点`/lock/node-0000000000`，下一个节点就是`/lock/node-0000000001`。
- 临时节点：临时节点只在zk会话期间存在，会话结束或超时时会被zk自动删除。
- 事件监听：通过zk的事件监听机制可以让客户端收到节点状态变化。主要的事件类型有节点数据变化、节点的删除和创建。

##### 2.实现步骤

了解完上面的三个概念，下面介绍具体实现。
 算法流程如下：
 1、每个客户端创建**临时有序**节点
 2、客户端获取节点列表，判断自己是否列表中的第一个节点，如果是就获得锁，如果不是就监听自己前面的节点，等待前面节点被删除。
 3、如果获取锁就进行正常的业务流程，执行完释放锁。
 上述步骤2中，有人可能担心如果节点发现自己不是序列最小的节点，准备添加监听器，但是这个时候前面节点正好被删除，这时候添加监听器是永远不起作用的，其实zk的API可以保证**读取和添加监听器是一个原子操作**。
 为什么要监听前一个节点而不是所有的节点呢？这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（**羊群效应**），而我们只希望它的后一个子节点收到通知。

Zookeeper 实现分布式锁的示意图如下：



![img](https:////upload-images.jianshu.io/upload_images/5611237-60ef4633a92e8870.png?imageMogr2/auto-orient/strip|imageView2/2/w/434/format/webp)

image.png

上图中左边是Zookeeper集群， lock是数据节点，node_1到node_n表示一系列的顺序临时节点，右侧client_1到client_n表示要获取锁的客户端。Service是互斥访问的服务。



 

使用zookeeper创建临时序列节点来实现分布式锁，适用于顺序执行的程序，大体思路就是创建临时序列节点，找出最小的序列节点，获取分布式锁，程序执行完成之后此序列节点消失，通过watch来监控节点的变化，从剩下的节点的找到最小的序列节点，获取分布式锁，执行相应处理，依次类推……

##### 3.代码实现



1)引入依赖：

```xml
<dependency>
            <groupId>com.101tec</groupId>
            <artifactId>zkclient</artifactId>
            <version>0.10</version>
        </dependency>
```

2)代码案例：

```java
package com.cennavi.vehicle_networking_data.controller;

import java.io.IOException;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.Watcher.Event.KeeperState;
import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.data.Stat;

public class ZooKeeperDistributedLock implements Watcher{
	
    private ZooKeeper zk;
    private String locksRoot= "/locks";
    private String productId;
    private String waitNode;
    private String lockNode;
    private CountDownLatch latch;
    private CountDownLatch connectedLatch = new CountDownLatch(1);
private int sessionTimeout = 30000; 

    public ZooKeeperDistributedLock(String productId){
        this.productId = productId;
         try {
	   String address = "192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181";
            zk = new ZooKeeper(address, sessionTimeout, this);
            connectedLatch.await();
        } catch (IOException e) {
            throw new LockException(e);
        } catch (InterruptedException e) {
            throw new LockException(e);
        }
    }
	@Override
    public void process(WatchedEvent event) {
        if(event.getState()==KeeperState.SyncConnected){
            connectedLatch.countDown();
            return;
        }

        if(this.latch != null) {  
            this.latch.countDown(); 
        }
    }

    public void acquireDistributedLock() {   
        try {
            if(this.tryLock()){
                return;
            }
            else{
                waitForLock(waitNode, sessionTimeout);
            }
        } catch (KeeperException e) {
            throw new LockException(e);
        } catch (InterruptedException e) {
            throw new LockException(e);
        } 
}

    public boolean tryLock() {
        try {
 		// 传入进去的locksRoot + “/” + productId
		// 假设productId代表了一个商品id，比如说1
		// locksRoot = locks
		// /locks/10000000000，/locks/10000000001，/locks/10000000002
            lockNode = zk.create(locksRoot + "/" + productId, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
   
            // 看看刚创建的节点是不是最小的节点
	 	// locks：10000000000，10000000001，10000000002
            List<String> locks = zk.getChildren(locksRoot, false);
            Collections.sort(locks);
	
            if(lockNode.equals(locksRoot+"/"+ locks.get(0))){
                //如果是最小的节点,则表示取得锁
                return true;
            }
	
            //如果不是最小的节点，找到比自己小1的节点
	  int previousLockIndex = -1;
            for(int i = 0; i < locks.size(); i++) {
		if(lockNode.equals(locksRoot + "/" + locks.get(i))) {
	         	    previousLockIndex = i - 1;
		    break;
		}
	   }
	   
	   this.waitNode = locks.get(previousLockIndex);
        } catch (KeeperException e) {
            throw new LockException(e);
        } catch (InterruptedException e) {
            throw new LockException(e);
        }
        return false;
    }
     
    private boolean waitForLock(String waitNode, long waitTime) throws InterruptedException, KeeperException {
        Stat stat = zk.exists(locksRoot + "/" + waitNode, true);
        if(stat != null){
            this.latch = new CountDownLatch(1);
            this.latch.await(waitTime, TimeUnit.MILLISECONDS);            	   this.latch = null;
        }
        return true;
}

    public void unlock() {
        try {
		// 删除/locks/10000000000节点
		// 删除/locks/10000000001节点
            System.out.println("unlock " + lockNode);
            zk.delete(lockNode,-1);
            lockNode = null;
            zk.close();
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (KeeperException e) {
            e.printStackTrace();
        }
}

    public class LockException extends RuntimeException {
        private static final long serialVersionUID = 1L;
        public LockException(String e){
            super(e);
        }
        public LockException(Exception e){
            super(e);
        }
}

	 

// 如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁，后面的每个人都会去监听排在自己前面的那个人创建的node上，一旦某个人释放了锁，排在自己后面的人就会被zookeeper给通知，一旦被通知了之后，就ok了，自己就获取到了锁，就可以执行代码了

}  


```

相关博客：https://www.jianshu.com/p/91976b27a188

# 三、分布式session 

##  cookie 	

在网站中，http请求是无状态的。也就是说即使第一次和服务器连接后并且登录成功后，第二次请求服务器依然不能知道当前请求是哪个用户。cookie的出现就是为了解决这个问题，第一次登录后服务器返回一些数据（cookie）给浏览器，然后浏览器保存在本地，当该用户发送第二次请求的时候，就会自动的把上次请求存储的cookie数据自动的携带给服务器，服务器通过浏览器携带的数据就能判断当前用户是哪个了。cookie存储的数据量有限，不同的浏览器有不同的存储大小，但一般不超过4KB。因此使用cookie只能存储一些小量的数据。

##  session 

session和cookie的作用有点类似，都是为了存储用户相关的信息。不同的是，cookie是存储在本地浏览器，而session存储在服务器。存储在服务器的数据会更加的安全，不容易被窃取。但存储在服务器也有一定的弊端，就是会占用服务器的资源，但现在服务器已经发展至今，一些session信息还是绰绰有余的。

 

**集群部署时的分布式session如何实现？**

####  前言



> 由于Http连接是无状态的，所以使用Tomcat做服务器的时候Tomcat内部会维护一个叫做Session的东东用来保存客户端的状态，一般情况下每个客户端都有一个cookie里面保存着叫jsessionid的cookie，每次访问tomcat的时候都会携带上，Tomcat可以根据这个jsessionid找到对应的session。就像你去超市买东西，门口的储物柜可以视作一个session容器，而打出的二维码条就是cookie。
> 在分布式系统中，对于同一个客户端，访问哪个Tomcat服务器就会在哪个Tomcat里面创建session。简单来说我做一个登录功能，即第一次访问Tomcat的时候需要输入用户名密码，访问成功后就在自己的sesison里面写入用户名，那么我下次访问的时候直接检测session里是否有自己的用户名来判断自己是否处于登录状态了。现在问题来了，如果我第一次访问的是TomcatA，登录成功后由于nginx的负载均衡第二次访问打到了TomcatB上，那么TomcatB里面并没有我的用户名信息，所以我还需要重新登录。一个最直观的想法就是把TomcatA和TomcatB的session抽出来放到某一个位置，这样不管访问TomcatA还是TomcatB都会从同一个Session里面获取用户信息。
> SpringBoot以一个非常简洁易用的方式帮我们实现了分布式Session，我们需要做的仅仅是1个注解，几行配置，几行代码。

负载 ：  http://114.67.207.193/LoadBalance/GIS/getPort

http://114.67.200.2:8081/GIS/set?name=%22xule2%22

http://114.67.200.2:8081/GIS/get

####  示意图 

​	



 ![](C:/Users/xule/Desktop/云端文档/04小课堂/media/01_Distributed_session.png)

 



- 使用cookie来完成（很明显这种不安全的操作并不可靠）
- 使用Nginx中的ip绑定策略，同一个ip只能在指定的同一个机器访问（不支持负载均衡）
- 利用数据库同步session（效率不高）
- 使用tomcat内置的session同步 
- 使用token代替session
- **我们使用spring-session以及集成好的解决方案，存放在Redis中**



其实方法很多，但是常见常用的是三种：

 #### 实现方式



##### 1.tomcat + redis



> 这个其实还挺方便的，就是使用session的代码跟以前一样，还是基于tomcat原生的session支持即可，然后就是用一个叫做Tomcat RedisSessionManager的东西，让所有我们部署的tomcat都将session数据存储到redis即可。

 

在tomcat的配置文件中，配置一下

```xml
<Valve className="com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve" />

<Manager className="com.orangefunction.tomcat.redissessions.RedisSessionManager"
         host="{redis.host}"
         port="{redis.port}"
         database="{redis.dbnum}"
         maxInactiveInterval="60"/>


```



搞一个类似上面的配置即可，你看是不是就是用了RedisSessionManager，然后指定了redis的host和 port就ok了。

 ```xml
<Valve className="com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve" />
<Manager className="com.orangefunction.tomcat.redissessions.RedisSessionManager"
	 sentinelMaster="mymaster"
	 sentinels="<sentinel1-ip>:26379,<sentinel2-ip>:26379,<sentinel3-ip>:26379"
	 maxInactiveInterval="60"/>


 ```



 

> 还可以用上面这种方式基于redis哨兵支持的redis高可用集群来保存session数据，都是ok的

 

##### 2.spring session + redis

 

> 分布式会话的这个东西重耦合在tomcat中，如果我要将web容器迁移成jetty，难道你重新把jetty都配置一遍吗？

 

> 因为上面那种tomcat + redis的方式好用，但是会严重依赖于web容器，不好将代码移植到其他web容器上去，尤其是你要是换了技术栈咋整？比如换成了spring cloud或者是spring boot之类的。还得好好思忖思忖。

 

>  所以现在比较好的还是基于java一站式解决方案，spring了。人家spring基本上包掉了大部分的我们需要使用的框架了，spirng cloud做微服务了，spring boot做脚手架了，所以用sping session是一个很好的选择。

 

**pom.xml**

 ```xml
<dependency>
  <groupId>org.springframework.session</groupId>
  <artifactId>spring-session-data-redis</artifactId>
  <version>1.2.1.RELEASE</version>
</dependency>
<dependency>
  <groupId>redis.clients</groupId>
  <artifactId>jedis</artifactId>
  <version>2.8.1</version>
</dependency>

 ```

 

 

**spring配置文件中**

```xml
<bean id="redisHttpSessionConfiguration"
     class="org.springframework.session.data.redis.config.annotation.web.http.RedisHttpSessionConfiguration">
    <property name="maxInactiveIntervalInSeconds" value="600"/>
</bean>

<bean id="jedisPoolConfig" class="redis.clients.jedis.JedisPoolConfig">
    <property name="maxTotal" value="100" />
    <property name="maxIdle" value="10" />
</bean>

<bean id="jedisConnectionFactory"
      class="org.springframework.data.redis.connection.jedis.JedisConnectionFactory" destroy-method="destroy">
    <property name="hostName" value="${redis_hostname}"/>
    <property name="port" value="${redis_port}"/>
    <property name="password" value="${redis_pwd}" />
    <property name="timeout" value="3000"/>
    <property name="usePool" value="true"/>
    <property name="poolConfig" ref="jedisPoolConfig"/>
</bean>

```

 

**web.xml**

```xml
<filter>
    <filter-name>springSessionRepositoryFilter</filter-name>
    <filter-class>org.springframework.web.filter.DelegatingFilterProxy</filter-class>
</filter>
<filter-mapping>
    <filter-name>springSessionRepositoryFilter</filter-name>
    <url-pattern>/*</url-pattern>
</filter-mapping>

```

  

**示例代码**

```java
@Controller
@RequestMapping("/test")
public class TestController {

@RequestMapping("/putIntoSession")
@ResponseBody
    public String putIntoSession(HttpServletRequest request, String username){
        request.getSession().setAttribute("name",  “leo”);

        return "ok";
    }

@RequestMapping("/getFromSession")
@ResponseBody
    public String getFromSession(HttpServletRequest request, Model model){
        String name = request.getSession().getAttribute("name");
        return name;
    }
}

```



 

> 上面的代码就是ok的，给sping session配置基于redis来存储session数据，然后配置了一个spring session的过滤器，这样的话，session相关操作都会交给spring session来管了。接着在代码中，就用原生的session操作，就是直接基于spring sesion从redis中获取数据了。

 

实现分布式的会话，有很多种很多种方式，我说的只不过比较常见的两种方式，tomcat + redis早期比较常用；近些年，解耦 tomcat，通过spring session来实现。

##### 3.SpringBoot利用Redis管理分布式Session



**配置maven**



```xml
<dependencies>
    <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-redis</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.session</groupId>
            <artifactId>spring-session-data-redis</artifactId>
        </dependency>
</dependencies>
```

**配置application.yml**

```yaml
spring:
  session:
    store-type: redis
    timeout: 3600s
    redis:
      flush-mode: on_save
      namespace: spring:session
  redis:
    host: 192.168.99.100
    port: 6379
    timeout: 5000ms

```



**代码示例**

```java
 	 	//主类首先开启EnableRedisHttpSession注解
@SpringBootApplication
@EnableRedisHttpSession
public class DistributeSessionApplication {

    public static void main(String[] args) {
        SpringApplication.run(DistributeSessionApplication.class, args);
    }
}

```



```java
//编写controller，set用于向session添加属性，get用于从session获取属性
package com.example.distributesession.controller;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

import javax.servlet.http.HttpServletRequest;
import java.util.HashMap;
import java.util.Map;

/**
 * @Author: 小混蛋
 * @CreateDate: 2018/12/11 17:20
 */
@RestController
public class TestController {

    @GetMapping("/set")
    public void test(HttpServletRequest request){
        request.getSession().setAttribute("message",request.getQueryString());
    }
    @GetMapping("/get")
    public Map<String,Object> two(HttpServletRequest request){
        Map<String,Object> map = new HashMap<>();
        map.put("sessionId",request.getSession().getId());
        map.put("message",request.getSession().getAttribute("message"));
        return map;
    }
}
```



测试：

> http://localhost:8081/GIS/set?name=%22xule2%22

> http://localhost:8082/GIS/get



##### 4. Spring Session Redis的原理简析

看了上面的配置，我们知道开启Redis Session的“秘密”在@EnableRedisHttpSession这个注解上。打开@EnableRedisHttpSession的源码：



```java
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.TYPE)
@Documented
@Import(RedisHttpSessionConfiguration.class)
@Configuration
public @interface EnableRedisHttpSession {
    //Session默认过期时间,秒为单位，默认30分钟
	int maxInactiveIntervalInSeconds() default MapSession.DEFAULT_MAX_INACTIVE_INTERVAL_SECONDS;
    //配置key的namespace，默认的是spring:session，如果不同的应用共用一个redis，应该为应用配置不同的namespace，这样才能区分这个Session是来自哪个应用的
	String redisNamespace() default RedisOperationsSessionRepository.DEFAULT_NAMESPACE;
    //配置刷新Redis中Session的方式，默认是ON_SAVE模式，只有当Response提交后才会将Session提交到Redis
    //这个模式也可以配置成IMMEDIATE模式，这样的话所有对Session的更改会立即更新到Redis
	RedisFlushMode redisFlushMode() default RedisFlushMode.ON_SAVE;
    //清理过期Session的定时任务默认一分钟一次。
	String cleanupCron() default RedisHttpSessionConfiguration.DEFAULT_CLEANUP_CRON;
}
```



> 分布式session介绍：https://www.cnblogs.com/54chensongxia/p/12096493.html





## 四、分布式缓存