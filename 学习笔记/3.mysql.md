# **一、数据库基础知识**

## 1、数据库三大范式是什么？

- **第一范式：** 每个列都不可以再拆分。
- **第二范式：** 在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。
- **第三范式：** 在第二范式的基础上，非主键需要**直接依赖**于主键,不能存在依赖传递,即不能存在:非主键列A依赖于非主键列B,非主键B依赖于主键的情况.


在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。

反三范式， 故名思义，跟范式所要求的正好相反，在反范式的设计模式，我们可以允许适当的数据的冗余，用这个冗余去取操作数据时间的缩短。也就是利用空间来换取时间,把数据冗余在多个表中，当查询时可以减少或者是避免表之间的关联；

## 2、mysql有关权限的表都有哪几个？ 

MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。

**下面分别介绍一下这些表的结构和内容：**

- user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。
- db权限表：记录各个帐号在各个数据库上的操作权限。
- table_priv权限表：记录数据表级的操作权限。
- columns_priv权限表：记录数据列级的操作权限。
- host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。



## 3、MySQL的binlog有有几种录入格式？分别有什么区别？ 

**有三种格式，statement，row和mixed。**

- statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。
- row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。
- mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。

此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。

## 4、binlog 、undolog 、relaylog 、redolog 的区别

### **1.binlog 是做什么的?**

binlog 是归档日志，属于 Server 层的日志，是一个二进制格式的文件，用于 **「记录用户对数据库更新的SQL语句信息」**。

主要作用

- 主从复制
- 数据恢复

### **2.undolog 是做什么的?**

undolog 是 InnoDB 存储引擎的日志，用于保证数据的原子性，**「保存了事务发生之前的数据的一个版本，也就是说记录的是数据是修改之前的数据，可以用于回滚」**，同时可以提供多版本并发控制下的读（MVCC）。

主要作用

- 事务回滚
- 实现多版本控制(MVCC)

### **3.relaylog 是做什么的?**

relaylog 是中继日志，**「在主从同步的时候使用到」**，它是一个中介临时的日志文件，用于存储从master节点同步过来的binlog日志内容。

![图片](https://mmbiz.qpic.cn/mmbiz_png/9dwzhvuGc8aP3ZujaVhV3NucRZQWuMRAn8PkN3k75QWLXcJvvIQRad52fZsxTcOUj4oJfeSod4xx4StbpmgKEg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

master 主节点的 binlog 传到 slave 从节点后，被写入 relay log 里，从节点的 slave sql 线程从 relaylog 里读取日志然后应用到 slave 从节点本地。从服务器 I/O 线程将主服务器的二进制日志读取过来记录到从服务器本地文件，然后 SQL 线程会读取 relay-log 日志的内容并应用到从服务器，从而 **「使从服务器和主服务器的数据保持一致」**。

### **4.redolog 是做什么的?**

redolog 是 **「InnoDB 存储引擎所特有的一种日志」**，用于记录事务操作的变化，记录的是数据修改之后的值，不管事务是否提交都会记录下来。

可以做 **「数据恢复并且提供 crash-safe 能力」**

*CrashSafe*指MySQL服务器宕机重启后，能够保证：  
1.所有已经提交的事务的数据仍然存在。  
2.所有没有提交的事务的数据自动回滚

当有增删改相关的操作时，会先记录到 Innodb 中，并修改缓存页中的数据，**「等到 mysql 闲下来的时候才会真正的将 redolog 中的数据写入到磁盘当中」**。

**redolog 是怎么记录日志的?**

![图片](https://mmbiz.qpic.cn/mmbiz_png/9dwzhvuGc8aP3ZujaVhV3NucRZQWuMRAxDuUiawbdP7Qoico8e6DjhCLUkNI1cEdYjIjtnaflhKJHMbYGBIJpYng/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

InnoDB 的 redolog 是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么总共就可以记录4GB的操作。**「从头开始写，写到末尾就又回到开头循环写」**。

所以，如果数据写满了但是还没有来得及将数据真正的刷入磁盘当中，那么就会发生 **「内存抖动」** 现象，从肉眼的角度来观察会发现 mysql 会宕机一会儿，此时就是正在刷盘了。

**redolog 和 binlog 的区别是什么?**

![图片](https://mmbiz.qpic.cn/mmbiz_png/9dwzhvuGc8aP3ZujaVhV3NucRZQWuMRABQbMLWKTDYE3u1tS6BVZLnP6f4ObUibLsM0KYmPm0hn5UTIX5SpbHUg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 1.**「redolog」** 是 **「Innodb」** 独有的日志，而 **「binlog」** 是 **「server」** 层的，所有的存储引擎都有使用到
- 2.**「redolog」** 记录了 **「具体的数值」**，对某个页做了什么修改，**「binlog」** 记录的 **「操作内容」**
- 3.**「binlog」** 大小达到上限或者 flush log **「会生成一个新的文件」**，而 **「redolog」** 有固定大小 **「只能循环利用」**
- 4.**「binlog 日志没有 crash-safe 的能力」**，只能用于归档。而 redolog 有 crash-safe 能力。



## 5.MySQL数据库主从同步原理

### 1.什么是mysql的主从复制？

 MySQL 主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL 默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。

### 2.mysql为什么需要主从同步？

1、在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。

2、做数据的热备

3、架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。

### 3. 数据库主从复制原理

**主从复制原理**，简言之，分三步曲进行：


详细的主从复制过程如图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzVFmLO1ibDOLCEzlt7LUb5aVtZtBlPl3ibOgFOgibiakusgmPibTplmmGm6uaHlgTq5lsoia1hNYOsYXWA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

整个复制流程主要由 **主库（Master）** 和 **从库（Slave）** 共同完成，其中 **从库主动拉取主库的更新**。

#### **复制流程**
1. **主库写入 Binlog 日志**
  - 当主库发生 **数据变更（INSERT、UPDATE、DELETE）** 时，MySQL 会把这些变更写入 **二进制日志（Binlog）**。
  - Binlog 记录的是 **SQL 语句或行级变更的日志**，用于后续从库的同步。
2. **从库的 I/O 线程主动拉取 Binlog**
  - 从库会 **主动连接主库**，并启动一个 **I/O 线程**，请求主库的 Binlog。
  - 主库的 **Binlog Dump 线程** 读取 Binlog，并 **推送** 给从库的 I/O 线程。
3. **从库写入 Relay Log（中继日志）**
  - 从库的 **I/O 线程** 接收到 Binlog 后，不会直接执行，而是先写入 **中继日志（Relay Log）**。
4. **从库的 SQL 线程重放日志**
  - 从库的 **SQL 线程** 读取 **Relay Log**，并在 **从库上执行相同的 SQL 语句**，使数据保持同步。
#### **主从复制是否是主库通知从库？**

**不是主库主动通知从库，而是从库主动拉取更新**，原因如下：
- **主库只负责写入 Binlog，并不主动推送变更**。
- **从库会定期请求主库的 Binlog，并同步到自己的数据库**。
- **从库的 I/O 线程会主动连接主库，拉取日志**。

#### **总结**

- **MySQL 复制采用 "主库写 Binlog → 从库主动拉取 Binlog" 机制**。
- **主库不会主动通知从库，而是从库定期请求主库更新**。
- **从库的 I/O 线程拉取 Binlog，并由 SQL 线程执行同步**。


### 4.Mysql 主从之间是怎么同步数据的?

**「同步策略」**：

- 1.**「全同步复制」**：主库强制同步日志到从库，等全部从库执行完才返回客户端，性能差
- 2.**「半同步复制」**：主库收到至少一个从库确认就认为操作成功，从库写入日志成功返回ack确认

## **MySQL 复制技术分类**

MySQL 复制主要分为 **全同步复制、半同步复制、异步复制** 三大类，而 **并行复制** 作为一种优化手段，可以用于这三种复制方式，以提升从库的 SQL 执行效率。

------

## **1. 全同步复制（Fully Synchronous Replication）**

### **特点**：

- **主库必须等待所有从库完成数据更新后，才能提交事务。**
- **强一致性**，不会有主从延迟，但性能最差。
- 适用于**金融交易、银行核心系统等高一致性场景**。

### **优缺点**：

✅ **数据一致性最高**，主库与从库始终同步。  
❌ **性能最差**，写入延迟极高，主库等待所有从库完成事务后才能继续。  
❌ **不常见**，MySQL 本身不支持，需要借助 **MySQL NDB Cluster** 等技术实现。

### **应用场景**：

- **MySQL NDB Cluster**（适用于高可用、高一致性要求的系统）。

------

## **2. 半同步复制（Semi-Synchronous Replication）**

### **特点**：

- **主库在事务提交时，至少等待一个从库收到 Binlog 后，才返回成功。**
- **从库不需要立即执行 SQL，只需保证收到 Binlog 并确认**，主库即可继续操作。
- **比异步复制更安全，但仍可能有数据延迟**。

### **优缺点**：

✅ **降低数据丢失风险**，至少有一个从库接收到 Binlog。  
✅ **性能比全同步复制好**，主库无需等待所有从库执行完成。  
❌ **仍然存在主从延迟**，因为从库只是收到 Binlog，未必立即执行 SQL。

### **应用场景**：

- **高可用复制架构**，例如 MySQL **Group Replication**。
- **防止主库宕机导致数据丢失**。

------

## **3. 异步复制（Asynchronous Replication）**

### **特点**：

- **主库提交事务后，立即返回成功，不等待从库**。
- **从库异步拉取 Binlog 并执行 SQL**，可能会出现数据延迟或数据丢失情况。
- **默认的 MySQL 复制模式**，适用于读写分离架构。

### **优缺点**：

✅ **写入性能最高**，主库不受从库影响。  
❌ **数据一致性最弱**，如果主库宕机，部分事务可能未同步到从库，导致数据丢失。  
❌ **主从延迟可能较大**，尤其是在高并发场景下。

### **应用场景**：

- **读写分离架构**（主库处理写请求，从库处理读请求）。
- **大规模数据备份**（日志归档、数据分析等）。

------

## **4. 并行复制（Parallel Replication）**

### **并行复制的作用**

- **默认情况下（单线程复制），从库只能按顺序执行 SQL，导致主从延迟**。
- **并行复制可以同时执行多个 SQL，提高从库同步速度，减少延迟**。

### **并行复制与三种复制方式的关系**

| **复制方式**   | **是否支持并行复制？** | **作用**                                            |
| -------------- | ---------------------- | --------------------------------------------------- |
| **全同步复制** | ✅ 可能支持             | 加快从库 SQL 执行，减少主库等待时间                 |
| **半同步复制** | ✅ 支持                 | 在主库等待从库确认 Binlog 后，提高从库 SQL 执行效率 |
| **异步复制**   | ✅ 支持                 | 减少主从延迟，提高从库吞吐量                        |

### **并行复制的几种模式**

| **模式**              | **特点**                                               | **适用场景**   |
| --------------------- | ------------------------------------------------------ | -------------- |
| **单线程复制**        | 默认方式，从库 SQL 线程单线程执行，效率低              | 低并发场景     |
| **数据库级并行复制**  | MySQL 5.7 引入，不同数据库（Schema）下的事务可并行执行 | 多数据库环境   |
| **WRITESET 并行复制** | MySQL 8.0 引入，事务间无冲突时可并行执行，提高吞吐量   | 高并发写入场景 |

------

## **5. 总结**

### **复制方式对比**

| **复制方式**   | **主库提交事务时是否等待从库？**              | **一致性** | **性能** | **数据丢失风险** |
| -------------- | --------------------------------------------- | ---------- | -------- | ---------------- |
| **全同步复制** | **等待所有从库完成 SQL 执行**                 | 最强       | 最差     | 无               |
| **半同步复制** | **等待至少一个从库收到 Binlog（但不等执行）** | 强         | 适中     | 低               |
| **异步复制**   | **主库提交事务后立即返回，不等待从库**        | 弱         | 最好     | 高               |

### **并行复制的作用**

- 并行复制是 **提升从库 SQL 执行速度的优化手段**，适用于异步复制、半同步复制甚至某些全同步复制方案。
- **高并发、大量写入场景推荐使用 "半同步复制 + 并行复制"**，可以兼顾数据安全和高性能。

------

### **推荐方案**

✅ **异步复制 + 并行复制** → 适用于 **读写分离、大规模数据分析**。  
✅ **半同步复制 + 并行复制** → 适用于 **高可用架构，减少数据丢失**。  
✅ **全同步复制 + 并行复制** → 适用于 **高一致性场景，如银行、金融系统**。

🚀 **如果追求高性能，推荐 "半同步复制 + 并行复制" 方案，兼顾一致性与效率！**
### 5.主从延迟要怎么解决?

- 1.MySQL 5.6 版本以后，提供了一种 **「并行复制」** 的方式，通过将 SQL 线程转换为多个 work 线程来进行重放
- 2.**「提高机器配置」**(王道)
- 3.在业务初期就选择合适的分库、分表策略， **「避免单表单库过大」** 带来额外的复制压力
- 4.**「避免长事务」**
- 5.**「避免让数据库进行各种大量运算」**
- 6.对于一些对延迟很敏感的业务 **「直接使用主库读」**

## 6. 聊聊数据的库高可用方案

- 双机主备
- 一主一从
- 一主多从
- MariaDB同步多主机
- 数据库中间件

### 6.1 双机主备高可用

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzVFmLO1ibDOLCEzlt7LUb5awssRLatT7YbdjrNqa6EI1ic8icMS5xxP1DVhsou7BgYtR6gVbqABbDkg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **架构描述**：两台机器A和B，A为主库，负责读写，B为备库，只备份数据。如果A库发生故障，B库成为主库负责读写。修复故障后，A成为备库，主库B同步数据到备库A
- **优点**：一个机器故障了可以自动切换，操作比较简单。
- **缺点**：只有一个库在工作，读写压力大，未能实现读写分离，**并发也有一定限制**

### 6.2 一主一从

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzVFmLO1ibDOLCEzlt7LUb5a87jCskHTmPFao2r7aC7b7LG0GbvfrDZsXZFFyjLhfSKs6KzfOrdxKA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **架构描述**: 两台机器A和B，A为主库，负责读写，B为从库，负责读数据。如果A库发生故障，B库成为主库负责读写。修复故障后，A成为从库，主库B同步数据到从库A。
- **优点**：从库支持读，分担了主库的压力，提升了并发度。一个机器故障了可以自动切换，操作比较简单。
- **缺点**：一台从库，并发支持还是不够，并且一共两台机器，**还是存在同时故障的机率，不够高可用**。

### 6.3 一主多从

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzVFmLO1ibDOLCEzlt7LUb5aIoVD2EhrMfDV1e7m6s6Jf6PIWLnfT9RYqmzjfzuJKhyhAo67pqSbpA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **架构描述**: 一台主库多台从库，A为主库，负责读写，B、C、D为从库，负责读数据。如果A库发生故障，B库成为主库负责读写，C、D负责读。修复故障后，A也成为从库，主库B同步数据到从库A。
- **优点**：多个从库支持读，分担了主库的压力，明显提升了读的并发度。
- **缺点**：只有台主机写，因此写的并发度不高

### 6.4 MariaDB同步多主机集群

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzVFmLO1ibDOLCEzlt7LUb5adj3UePCxxS5jialDRJr84XOicxdCf1jrWYWX3gfHLOPoPK3EDmoneRnQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **架构描述**:有代理层实现负载均衡，多个数据库可以同时进行读写操作；各个数据库之间可以通过`Galera Replication`方法进行数据同步，每个库理论上数据是完全一致的。
- **优点**：读写的并发度都明显提升，可以任意节点读写，可以自动剔除故障节点，具有较高的可靠性。
- **缺点**：数据量不支持特别大。要避免大事务卡死，如果集群节点一个变慢，其他节点也会跟着变慢。

### 6.5 数据库中间件

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzVFmLO1ibDOLCEzlt7LUb5aGvMbOG0Nl6ibPILiamAf4CSfvf7tOG1kEhhZCbbMvvcwGwJwHxySTLrQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- 架构描述：mycat分片存储，每个分片配置一主多从的集群。
- 优点：解决高并发高数据量的高可用方案
- 缺点：维护成本比较大。

# mysql和PostgreSql的区别

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/postgis.png)





# 二、MySQL的事务特性



### 1.什么是事务？百度百科对于事务的定义如下：

事务（Transaction），一般是指要做的或所做的事情。在计算机术语中是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。事务通常由高级数据库操纵语言或编程语言（如SQL，C++或Java）书写的用户程序的执行所引起，并用形如begin transaction和end transaction语句（或函数调用）来界定。事务由事务开始(begin transaction)和事务结束(end transaction)之间执行的全体操作组成。—— [百度百科]

可以看出，事务是一个操作单元，当然，在关系型数据库系统中，事务就是一次SQL的操作，要么都成功，要么都失败。

### 2.事务特性（ACID）：

* 原子性（Atomicity）：原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生；
* 一致性（Consistency）：事务前后数据的完整性必须保持一致；
* 隔离性（Isolation）：数据库允许多个事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同的级别，包括读未提交(Read uncommitted)、读已提交(Read committed)、可重复读(repeateable read)和串行化(Serializable).
* 持久性（Durability）：持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响

**ACID是靠什么保证的？**

原子性：由undolog日志来保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql

一致性：由其他三大特性保证，程序代码要保证业务上的一致性

隔离性：是由MVCC来保证

持久性：由redolog来保证，mysql修改数据的时候会在redolog中记录一份日志数据，就算数据没有保存成功，只要日志保存成功了，数据仍然不会丢失

### 3.不考虑事务的隔离性会发生什么问题？	

1、`脏读`：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据就是脏数据

2、`不可重复读`：事务A多次读取同一事物，事务B在事务A多次读取的过程中，对数据做了更新并提交，导致事务A多次读取同一数据时，结果不一致。

3、`幻读`：一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）



小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表



### 4.隔离级别

对于上述不考虑事务隔离性产生的四个问题我们可以通过事务的隔离级别来解决。

#### 事务的隔离级别有哪些？

| 事务隔离级别                 | 脏读 | 不可重复读 | 幻读 |
| :--------------------------- | :--- | :--------- | :--- |
| 读未提交（read-uncommitted） | 是   | 是         | 是   |
| 读已提交（read-committed）   | 否   | 是         | 是   |
| 可重复读（repeatable-read）  | 否   | 否         | 是   |
| 串行化（serializable）       | 否   | 否         | 否   |

串行化是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。

这里需要注意的是：

* Mysql 默认采用的 REPEATABLE_READ隔离级别

* Oracle 默认采用的 READ_COMMITTED隔离级别

事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC(多版本并发控制)，通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。

因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ(可重读)**并不会有任何性能损失。

InnoDB 存储引擎在 分布式事务 的情况下一般会用到**SERIALIZABLE(可串行化)**隔离级别。

​	


事务控制语句：

- `BEGIN或START TRANSACTION`：显式的开启一个事物。
- `COMMIT`：也可以使用COMMIT WORK，不过二者是等价的。COMMIT会提交事务，并使已对数据库进行的所有修改成为永久性的。
- `Rollback`：也可以使用Rollback work，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。
- `SAVEPOINT identifier`：SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有很多个SAVEPOINT；
- `RELEASE SAVEPOINT identifier`：删除一个事物的保存点,当没有指定的保存点时，执行该语句会抛出一个异常。
- `ROLLBACK TO inditifier`：把事务回滚到标记点。
- `SET TRANSACTION`:用来设置事务的隔离级别。InnoDB存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ和SERLALIZABLE。





#### 如何设置事务的隔离级别？

**MySQL**

查看事务隔离级别

> SELECT @@tx_isolation

设置事务级别

> set session transaction isolation level #设置事务隔离级别



**JDBC**

> 在jdbc中设置事务隔离级别可以使用java.sql.Connection接口中提供的方法void setTransactionIsolation(int level) throws SQLExceptionlevel取以下Connection 常量之一：
> Connection.TRANSACTION_READ_UNCOMMITTED
> Connection.TRANSACTION_READ_COMMITTED
> Connection.TRANSACTION_REPEATABLE_READ
> Connection.TRANSACTION_SERIALIZABLE
> 注：不能使用 Connection.TRANSACTION_NONE，因为它指定了不受支持的事务。



### 5.丢失更新

如何解决丢失更新的问题？我们可以采用两种方式解决丢失更新。

#### **悲观锁**

悲观锁 （假设丢失更新一定会发生 ） —– 利用数据库内部锁机制，管理事务提供的锁机制，其有两种锁机制：

1、共享锁

> select * from table lock in share mode（读锁、共享锁）

2、排他锁

> select * from table for update （写锁、排它锁）



注：update语句默认添加排它锁

#### **乐观锁**

乐观锁 （假设丢失更新不会发生）——- 采用程序中添加版本字段解决丢失更新问题，在数据表添加版本字段。每次修改过记录后，版本字段都会更新，如果读取是版本字段，与修改时版本字段不一致，说明别人进行修改过数据 （重改） 。



> create table product (
> id int(8),
> name varchar(20),
> updatetime timestamp
> );
>
> insert into product values(1,’冰箱’,null);
> update product set name=’洗衣机’ where id = 1;



### 	6.演示

#### 脏读

```sql
一个事务读取到另一个事务的为提交数据
设置A,B事务隔离级别为Read uncommitted
set session transaction isolation level read uncommitted;

1.在A事务中
    start transaction;
    update account set money=money-500 where name='aaa';
    update account set money=money+500 where name='bbb';

2.在B事务中
    start transaction;
    select * from account;
    这时，B事务读取时，会发现，钱已经汇完。那么就出现了脏读。
当A事务提交前，执行rollback，在commit， B事务在查询，就会发现，钱恢复成原样  
            也出现了两次查询结果不一致问题，出现了不可重复读.

```





#### 解决脏读问题

```sql
将事务的隔离级别设置为 read committed来解决脏读
设置A,B事务隔离级别为   Read committed
set session transaction isolation level  read committed;

1.在A事务中
    start transaction;
    update account set money=money-500 where name='aaa';
    update account set money=money+500 where name='bbb';

2.在B事务中
    start transaction;
    select * from account;
这时B事务中，读取信息时，是不能读到A事务未提交的数据的，也就解决了脏读。

让A事务，提交数据 commit;   
这时，在查询，这次结果与上一次查询结果又不一样了，还存在不可重复读。

```



#### 解决不可重复读

```sql
将事务的隔离级别设置为Repeatable read来解决不可重复读。
设置A,B事务隔离级别为Repeatable read;
set session transaction isolation level  Repeatable read;

1.在A事务中
    start transaction;
    update account set money=money-500 where name='aaa';
    update account set money=money+500 where name='bbb';

2.在B事务中
    start transaction;
    select * from account;

当A事务提交后commit;B事务在查询，与上次查询结果一致，解决了不可重复读。

```

#### 设置事务隔离级别 Serializable ,它可以解决所有问题

```sql
set session transaction isolation level Serializable;

如果设置成这种隔离级别，那么会出现锁表。也就是说，一个事务在对表进行操作时，其它事务操作不了。

```

### 7.表锁行锁

![MySQL锁机制](https://pic1.zhimg.com/v2-164ea9bd88bc4ccbc43f3003f48d1d14_1440w.jpg?source=172ae18b)

 **MYSQL 表级锁 行级锁 页面锁区别** 

myisam存储引擎默认是表级锁

innodb存储引擎默认是行级锁

DBD存储引擎默认是页面锁

 

**表级锁:** 开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 

**行级锁:** 开锁大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。

**页面锁:** 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。

 

从上述特点可见，很难笼统的说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度来说：表级锁更于以查询为主，只有少量按索引条件更新数据的应用，如WEB应用；而行级锁则更适合于有大理按索引发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理系统，

 

MYSQL数据库——UPDATE和INSERT操作是锁表还是锁行

**概述：** Update和Insert是锁表还是锁行，会影响到程序中并发程序的设计。

**总结：**[详情](https://www.freesion.com/article/6214655801/)

（1）Update时，where中的过滤条件列，如果用索引，锁行，无法用索引，锁表。**按照索引规则，如果能使用索引，锁行，不能使用索引，锁表。**

（2）Insert时，锁行。

 

### 8.MVCC

1、MVCC

 MVCC，全称Multi-Version Concurrency Control，即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。

MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读。

2、当前读

 像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。

3、快照读（提高数据库的并发查询能力）

 像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本

4、当前读、快照读、MVCC关系

 MVCC多版本并发控制指的是维持一个数据的多个版本，使得读写操作没有冲突，快照读是MySQL为实现MVCC的一个非阻塞读功能。MVCC模块在MySQL中的具体实现是由三个隐式字段，undo日志、read view三个组件来实现的。

#### MVCC解决的问题是什么？

 数据库并发场景有三种，分别为：

 1、读读：不存在任何问题，也不需要并发控制

 2、读写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读、幻读、不可重复读

 3、写写：有线程安全问题，可能存在更新丢失问题

 MVCC是一种用来解决读写冲突的无锁并发控制，也就是为事务分配单项增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照，所以MVCC可以为数据库解决一下问题：

 1、在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能

 2、解决脏读、幻读、不可重复读等事务隔离问题，但是不能解决更新丢失问题

#### MVCC实现原理是什么？

 mvcc的实现原理主要依赖于记录中的三个隐藏字段，undolog，read view来实现的。

 **隐藏字段**

 每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段

 DB_TRX_ID

 6字节，最近修改事务id，记录创建这条记录或者最后一次修改该记录的事务id

 DB_ROLL_PTR

 7字节，回滚指针，指向这条记录的上一个版本,用于配合undolog，指向上一个旧版本

 DB_ROW_JD

 6字节，隐藏的主键，如果数据表没有主键，那么innodb会自动生成一个6字节的row_id

 记录如图所示：

![image-20210225233929554](https://gitee.com/jiajiales/learningNotes/raw/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/media/%E6%95%B0%E6%8D%AE%E6%A1%88%E4%BE%8B.png)

 在上图中，DB_ROW_ID是数据库默认为该行记录生成的唯一隐式主键，DB_TRX_ID是当前操作该记录的事务ID，DB_ROLL_PTR是一个回滚指针，用于配合undo日志，指向上一个旧版本

 **undo log**

 undolog被称之为回滚日志，表示在进行insert，delete，update操作的时候产生的方便回滚的日志

 当进行insert操作的时候，产生的undolog只在事务回滚的时候需要，并且在事务提交之后可以被立刻丢弃

 当进行update和delete操作的时候，产生的undolog不仅仅在事务回滚的时候需要，在快照读的时候也需要，所以不能随便删除，只有在快照读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除（当数据发生更新和删除操作的时候都只是设置一下老记录的deleted_bit，并不是真正的将过时的记录删除，因为为了节省磁盘空间，innodb有专门的purge线程来清除deleted_bit为true的记录，如果某个记录的deleted_id为true，并且DB_TRX_ID相对于purge线程的read view 可见，那么这条记录一定时可以被清除的）

 **下面我们来看一下undolog生成的记录链**

 1、假设有一个事务编号为1的事务向表中插入一条记录，那么此时行数据的状态为：

![image-20210225235444975](https://gitee.com/jiajiales/learningNotes/raw/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/media/1.png)

 2、假设有第二个事务编号为2对该记录的name做出修改，改为lisi

 在事务2修改该行记录数据时，数据库会对该行加排他锁

 然后把该行数据拷贝到undolog中，作为 旧记录，即在undolog中有当前行的拷贝副本

 拷贝完毕后，修改该行name为lisi，并且修改隐藏字段的事务id为当前事务2的id，回滚指针指向拷贝到undolog的副本记录中

 事务提交后，释放锁

![image-20210313220450629](https://gitee.com/jiajiales/learningNotes/raw/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/media/2.png)

 3、假设有第三个事务编号为3对该记录的age做了修改，改为32

 在事务3修改该行数据的时，数据库会对该行加排他锁

 然后把该行数据拷贝到undolog中，作为旧纪录，发现该行记录已经有undolog了，那么最新的旧数据作为链表的表头，插在该行记录的undolog最前面

 修改该行age为32岁，并且修改隐藏字段的事务id为当前事务3的id，回滚指针指向刚刚拷贝的undolog的副本记录

 事务提交，释放锁

![image-20210313220337624](https://gitee.com/jiajiales/learningNotes/raw/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/media/3.png)

 从上述的一系列图中，大家可以发现，不同事务或者相同事务的对同一记录的修改，会导致该记录的undolog生成一条记录版本线性表，即链表，undolog的链首就是最新的旧记录，链尾就是最早的旧记录。

 **Read View**

 上面的流程如果看明白了，那么大家需要再深入理解下read view的概念了。

 Read View是事务进行快照读操作的时候生产的读视图，在该事务执行快照读的那一刻，会生成一个数据系统当前的快照，记录并维护系统当前活跃事务的id，事务的id值是递增的。

 其实Read View的最大作用是用来做可见性判断的，也就是说当某个事务在执行快照读的时候，对该记录创建一个Read View的视图，把它当作条件去判断当前事务能够看到哪个版本的数据，有可能读取到的是最新的数据，也有可能读取的是当前行记录的undolog中某个版本的数据

 Read View遵循的可见性算法主要是将要被修改的数据的最新记录中的DB_TRX_ID（当前事务id）取出来，与系统当前其他活跃事务的id去对比，如果DB_TRX_ID跟Read View的属性做了比较，不符合可见性，那么就通过DB_ROLL_PTR回滚指针去取出undolog中的DB_TRX_ID做比较，即遍历链表中的DB_TRX_ID，直到找到满足条件的DB_TRX_ID,这个DB_TRX_ID所在的旧记录就是当前事务能看到的最新老版本数据。

 Read View的可见性规则如下所示：

 首先要知道Read View中的三个全局属性：

 trx_list:一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID（1,2,3）

 up_limit_id:记录trx_list列表中事务ID最小的ID（1）

 low_limit_id:Read View生成时刻系统尚未分配的下一个事务ID，（4）

 具体的比较规则如下：

 1、首先比较DB_TRX_ID < up_limit_id,如果小于，则当前事务能看到DB_TRX_ID所在的记录，如果大于等于进入下一个判断

 2、接下来判断DB_TRX_ID >= low_limit_id,如果大于等于则代表DB_TRX_ID所在的记录在Read View生成后才出现的，那么对于当前事务肯定不可见，如果小于，则进入下一步判断

 3、判断DB_TRX_ID是否在活跃事务中，如果在，则代表在Read View生成时刻，这个事务还是活跃状态，还没有commit，修改的数据，当前事务也是看不到，如果不在，则说明这个事务在Read View生成之前就已经开始commit，那么修改的结果是能够看见的。

7、MVCC的整体处理流程

假设有四个事务同时在执行，如下图所示：

| 事务1    | 事务2    | 事务3    | 事务4        |
| -------- | -------- | -------- | ------------ |
| 事务开始 | 事务开始 | 事务开始 | 事务开始     |
| ......   | ......   | ......   | 修改且已提交 |
| 进行中   | 快照读   | 进行中   |              |
| ......   | ......   | ......   |              |

从上述表格中，我们可以看到，当事务2对某行数据执行了快照读，数据库为该行数据生成一个Read View视图，可以看到事务1和事务3还在活跃状态，事务4在事务2快照读的前一刻提交了更新，所以，在Read View中记录了系统当前活跃事务1，3，维护在一个列表中。同时可以看到up_limit_id的值为1，而low_limit_id为5，如下图所示：

![image-20210520143604440](https://gitee.com/jiajiales/learningNotes/raw/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/media/image-20210520143604440.png)

在上述的例子中，只有事务4修改过该行记录，并在事务2进行快照读前，就提交了事务，所以该行当前数据的undolog如下所示：

![image-20210520143717928](https://gitee.com/jiajiales/learningNotes/raw/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/media/image-20210520143717928.png)

 当事务2在快照读该行记录的是，会拿着该行记录的DB_TRX_ID去跟up_limit_id,lower_limit_id和活跃事务列表进行比较，判读事务2能看到该行记录的版本是哪个。

 具体流程如下：先拿该行记录的事务ID（4）去跟Read View中的up_limit_id相比较，判断是否小于，通过对比发现不小于，所以不符合条件，继续判断4是否大于等于low_limit_id,通过比较发现也不大于，所以不符合条件，判断事务4是否处理trx_list列表中，发现不再次列表中，那么符合可见性条件，所以事务4修改后提交的最新结果对事务2 的快照是可见的，因此，事务2读取到的最新数据记录是事务4所提交的版本，而事务4提交的版本也是全局角度的最新版本。如下图所示：

![image-20210520143742317](https://gitee.com/jiajiales/learningNotes/raw/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/media/image-20210520143742317.png)

当上述的内容都看明白了的话，那么大家就应该能够搞清楚这几个核心概念之间的关系了，下面我们讲一个不同的隔离级别下的快照读的不同。

8、RC、RR级别下的InnoDB快照读有什么不同

 因为Read View生成时机的不同，从而造成RC、RR级别下快照读的结果的不同

 1、在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照即Read View,将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View,所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View,所以对之后的修改不可见

 2、在RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动和事务的快照，这些事务的修改对于当前事务都是不可见的，而早于Read View创建的事务所做的修改均是可见

 3、在RC级别下，事务中，每次快照读都会新生成一个快照和Read View,这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因。

 **总结：在RC隔离级别下，是每个快照读都会生成并获取最新的Read View,而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View，之后的快照读获取的都是同一个Read View.**




# 三、mysql数据类型





![img](https://pic4.zhimg.com/80/v2-ed4cc2d902a4fe3a73279cdb0cefb34f_720w.jpg)





![img](https://pic4.zhimg.com/80/v2-2d0ee27282d3a76f9f93c399d809fe93_720w.jpg)







### **1.整数类型，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT。**

- 分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。
- **长度：整数类型可以被指定长度。** 例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配合使用才有意义。
- **例子：** 假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数据库实际存储数据为00012。

### **2.实数类型，包括FLOAT、DOUBLE、DECIMAL。**

- DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。
- 而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。
- 计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。

### **3.字符串类型，包括VARCHAR、CHAR、TEXT、BLOB。**

- VARCHAR用于存储可变长字符串，它比定长类型更节省空间。
- VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2字节表示。
- VARCHAR存储的内容超出设置的长度时，内容会被截断。
- CHAR是定长的，根据定义的字符串长度分配足够的空间。
- CHAR会根据需要使用空格进行填充方便比较。
- CHAR适合存储很短的字符串，或者所有值都接近同一个长度。
- CHAR存储的内容超出设置的长度时，内容同样会被截断。

**使用策略：**

对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。

对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。

使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。

尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。

### **4.枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。**

- 有时可以使用ENUM代替常用的字符串类型。
- ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。
- ENUM在内部存储时，其实存的是整数。
- 尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。
- 排序是按照内部存储的整数

### **5.日期和时间类型，尽量使用timestamp，空间效率高于datetime，** 用整数保存时间戳通常不方便处理。

- 如果需要存储微妙，可以使用bigint存储。
- 看到这里，这道真题是不是就比较容易回答了。



# 四、引擎



## 1.MySQL存储引擎MyISAM与InnoDB区别 



存储引擎Storage engine：MySQL中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。



**常用的存储引擎有以下：**

- Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。
- MyIASM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。
- MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。



## 2.MyISAM与InnoDB区别 



![img](https://pic3.zhimg.com/80/v2-9f3fd70636717718cc776a96e51b1cda_720w.jpg)





## 3.MyISAM索引与InnoDB索引的区别？ 



- InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。
- InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。
- MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。
- InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。



## 4.InnoDB引擎的4大特性 



- 插入缓冲（insert buffer)
- 二次写(double write)
- 自适应哈希索引(ahi)
- 预读(read ahead)

## 5.存储引擎选择 



**如果没有特别的需求，使用默认的Innodb即可。**

- MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。
- Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如OA自动化办公系统。

  

#  五、索引

### 1.索引是什么？

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvJn8odMXyQlKrYfmst1Z9kfTTfIWqa61xobo4JElTKBwpY8dCdLRvsw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 索引是一种能提高数据库查询效率的数据结构。它可以比作一本字典的目录，可以帮你快速找到对应的记录。
- 索引一般存储在磁盘的文件中，它是占用物理空间的。
- 正所谓水能载舟，也能覆舟。适当的索引能提高查询效率，过多的索引会影响数据库表的插入和更新功能。

### 2.索引有哪些类型类型

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvUdjUicXoOSApdvxdNllOAhMkRN6nu8OJ7tGsdgInTHxjibsc4JN8sYkw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### 数据结构维度

- B+树索引：所有数据存储在叶子节点，复杂度为O(logn)，适合范围查询。
- 哈希索引:  适合等值查询，检索效率高，一次到位。
- 全文索引：MyISAM和InnoDB中都支持使用全文索引，一般在文本类型char,text,varchar类型上创建。
- R-Tree索引: 用来对GIS数据类型创建SPATIAL索引

#### 物理存储维度

- 聚集索引：聚集索引就是以主键创建的索引，在叶子节点存储的是表中的数据。
- 非聚集索引：非聚集索引就是以非主键创建的索引，在叶子节点存储的是主键和索引列。

#### 逻辑维度

- 主键索引：一种特殊的唯一索引，不允许有空值。
- 普通索引：MySQL中基本索引类型，允许空值和重复值。
- 联合索引：多个字段创建的索引，使用时遵循最左前缀原则。
- 唯一索引：索引列中的值必须是唯一的，但是允许为空值。
- 空间索引：MySQL5.7之后支持空间索引，在空间索引这方面遵循OpenGIS几何数据模型规则。



###  3.主键索引和非主键索引有什么区别？



例如对于下面这个表(其实就是上面的表中增加了一个k字段),且ID是主键。

![img](https://img2018.cnblogs.com/blog/1644694/201905/1644694-20190505155016157-1108127109.png)

 

主键索引和非主键索引的示意图如下：

![img](https://img2018.cnblogs.com/blog/1644694/201905/1644694-20190505155026646-1387513390.png)

 

其中R代表一整行的值。

从图中不难看出，主键索引和非主键索引的区别是：非主键索引的叶子节点存放的是**主键的值**，而主键索引的叶子节点存放的是**整行数据**，其中非主键索引也被称为**二级索引**，而主键索引也被称为**聚簇索引**。

根据这两种结构我们来进行下查询，看看他们在查询上有什么区别。

1、如果查询语句是 select * from table where ID = 100,即主键查询的方式，则只需要搜索 ID 这棵 B+树。

2、如果查询语句是 select * from table where k = 1，即非主键的查询方式，则先搜索k索引树，得到ID=100,再到ID索引树搜索一次，这个过程也被称为回表。



### 4.为什么选择B+树作为索引结构

可以从这几个维度去看这个问题，查询是否够快，效率是否稳定，存储数据多少，以及查找磁盘次数等等。为什么不是哈希结构？为什么不是二叉树，为什么不是平衡二叉树，为什么不是B树，而偏偏是B+树呢？

我们写业务SQL查询时，大多数情况下，都是范围查询的，如下SQL

```
select * from employee where age between 18 and 28;
```

#### 为什么不使用哈希结构？

我们知道哈希结构，类似k-v结构，也就是，key和value是一对一关系。它用于**「等值查询」**还可以，但是范围查询它是无能为力的哦。

#### 为什么不使用二叉树呢？

先回忆下二叉树相关知识啦~ 所谓**「二叉树，特点如下：」**

- 每个结点最多两个子树，分别称为左子树和右子树。
- 左子节点的值小于当前节点的值，当前节点值小于右子节点值
- 顶端的节点称为根节点，没有子节点的节点值称为叶子节点。

我们脑海中，很容易就浮现出这种二叉树结构图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvIzqtnAG7q0kGVqVmOCD97sLO8LAwfjzqKLicDUATfQ2T1Gjn4V36N4w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

但是呢，有些特殊二叉树，它可能这样的哦：

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGv9HiaTEQl6pItpgh4KD0clUYvd5BIZSUtgxJZnXF6XzqP7Y8MosicyHnQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果二叉树特殊化为一个链表，相当于全表扫描。那么还要索引干嘛呀？因此，一般二叉树不适合作为索引结构。

#### 为什么不使用平衡二叉树呢？

平衡二叉树特点：它也是一颗二叉查找树，任何节点的两个子树高度最大差为1。所以就不会出现特殊化一个链表的情况啦。

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvIzqtnAG7q0kGVqVmOCD97sLO8LAwfjzqKLicDUATfQ2T1Gjn4V36N4w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

但是呢：

- 平衡二叉树插入或者更新时，需要左旋右旋维持平衡，维护代价大
- 如果数量多的话，树的高度会很高。因为数据是存在磁盘的，以它作为索引结构，每次从磁盘读取一个节点，操作IO的次数就多啦。

#### **为什么 B+树 而不是红黑树**

红黑树基本都是存储在内存中才会使用的数据结构。

　　在大规模数据存储的时候，红黑树往往出现由于**树的深度过大**而造成磁盘 IO 读写过于频繁，进而导致效率低下的情况。

　　B+ 树的深度更小，IO较少，效率更高

**B+tree 和红黑树**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/9mQQWOf4KRIsYJvGP4MRo9Jf98qQJ4NqXI8esnc9de05HicvSd19cuFwWEqxTO4ibmnVsGxt5d97bgmXntxSVF3Q/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

对于有 N 个叶子节点的 B+tree，搜索复杂度为 **「O(logdN)」** ,d 是指 degree 是指 B+tree 的度，表示节点允许的最大子节点个数为 d 个，在实际的运用中 d 值是大于 100 的，即使数据达到千万级别时候 B+tree 的高度依然维持在 3-4 左右，保证了 3-4 次磁盘 I/O 就能查到目标数据。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/9mQQWOf4KRIsYJvGP4MRo9Jf98qQJ4Nq5DCkq9icXGfQ2v1HreQvsHPAlHf5vmJJiazqAhSibOic0FNPtYCGCqu5UQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从上图中可以看出红黑树是二叉树，节点的子节点个数最多为 2 个，意味着其搜索复杂度为**「O(logN)」** ，比 B+ 树高出不少，因此红黑树检索到目标数据所需经理的磁盘 I/O 次数更多。



#### 为什么不使用B树呢？

数据量大的话，平衡二叉树的高度会很高，会增加IO嘛。那为什么不选择同样数据量，**「高度更矮的B树」**呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvc1ePFmBK3HTBYkgYE1Lk1ddAnH2p1icuJwKafZsu1uQBYdkPiaicWcDYg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

B树相对于平衡二叉树，就可以存储更多的数据，高度更低。但是最后为甚选择B+树呢？因为B+树是B树的升级版：

* B+树非叶子节点上是不存储数据的，仅存储键值，而B树节点中不仅存储键值，也会存储数据。innodb中页的默认大小是16KB，如果不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，如此一来我们查找数据进行磁盘的IO次数有会再次减少，数据查询的效率也会更快。

* B+树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的，链表连着的。那么B+树使得范围查找，排序查找，分组查找以及去重查找变得异常简单。

两棵树的插入实现：[B-tree](https://www.cs.usfca.edu/~galles/visualization/BTree.html)  [B+tree](https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html) 
各种图树算法的演示：[算法](https://www.cs.usfca.edu/~galles/visualization/Algorithms.html)

在B树中，你可以将键和值存放在内部节点和叶子节点；但在B+树中，内部节点都是键，没有值，叶子节点同时存放键和值。

B+树的叶子节点有一条链相连，而B树的叶子节点各自独立。



![img](https://pic3.zhimg.com/80/v2-5307acbcb413f6ddd76db5438b93f1e2_720w.jpg)



**使用B树的好处**

B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。



**使用B+树的好处**

由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间

### 5.一次B+树索引搜索过程

**「面试官：」** 假设有以下表结构，并且有这几条数据

```
CREATE TABLE `employee` (
  `id` int(11) NOT NULL,
  `name` varchar(255) DEFAULT NULL,
  `age` int(11) DEFAULT NULL,
  `date` datetime DEFAULT NULL,
  `sex` int(1) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_age` (`age`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

insert into employee values(100,'小伦',43,'2021-01-20','0');
insert into employee values(200,'俊杰',48,'2021-01-21','0');
insert into employee values(300,'紫琪',36,'2020-01-21','1');
insert into employee values(400,'立红',32,'2020-01-21','0');
insert into employee values(500,'易迅',37,'2020-01-21','1');
insert into employee values(600,'小军',49,'2021-01-21','0');
insert into employee values(700,'小燕',28,'2021-01-21','1');
```

**「面试官：」** 如果执行以下的查询SQL，需要执行几次的树搜索操作？可以画下对应的索引结构图~

```
select * from Temployee where age=32;
```

**「解析：」** 其实这个，面试官就是考察候选人是否熟悉B+树索引结构图。可以像酱紫回答~

- 先画出`idx_age`索引的索引结构图，大概如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvZBdQyHHHgFPeBWKniaicCO42RCxg94PJ7yZps92jX2ibeu1bCcxyVS17Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 再画出id主键索引，我们先画出聚族索引结构图，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvmyS6ofx81UKe4t2nmKictM32XkMszoCiavu6aPaEOvd5DiaToFY3EYXKg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此，这条 SQL 查询语句执行大概流程就是酱紫：

- 1. 搜索`idx_age`索引树，将磁盘块1加载到内存，由于32<37,搜索左路分支，到磁盘寻址磁盘块2。

     

  2. 将磁盘块2加载到内存中，在内存继续遍历，找到age=32的记录，取得id = 400.

     

  3. 拿到id=400后，回到id主键索引树。

     

  4. 搜索`id主键`索引树，将磁盘块1加载内存，在内存遍历，找到了400，但是B+树索引非叶子节点是不保存数据的。索引会继续搜索400的右分支，到磁盘寻址磁盘块3.

     

  5. 将磁盘块3加载内存，在内存遍历，找到id=400的记录，拿到R4这一行的数据，好的，大功告成。

因此，这个SQL查询，执行了几次树的搜索操作，是不是一步了然了呀。**「特别的」**，在`idx_age`二级索引树找到主键`id`后，回到id主键索引搜索的过程,就称为回表。

> ❝
>
> 什么是回表？拿到主键再回到主键索引查询的过程，就叫做**「回表」**
>
> ❞

### 6.覆盖索引

**「面试官：」** 如果不用`select *`, 而是使用`select id,age`，以上的题目执行了几次树搜索操作呢？

**「解析：」** 这个问题，主要考察候选人的覆盖索引知识点。回到`idx_age`索引树，你可以发现查询选项id和age都在叶子节点上了。因此，可以直接提供查询结果啦，根本就不需要再回表了~

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGv92TnuqU86R7ryO7vo6femMJwgTQtAoLGrIdb2eYFSjXxp1AZQu1Owg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

> ❝
>
> 覆盖索引：在查询的数据列里面，不需要回表去查，直接从索引列就能取到想要的结果。换句话说，你SQL用到的索引列数据，覆盖了查询结果的列，就算上覆盖索引了。
>
> ❞

所以，相对于上个问题，就是省去了回表的树搜索操作。

### 7.索引失效

**「面试官：」** 如果我现在给`name`字段加上普通索引，然后用个like模糊搜索，那会执行多少次查询呢？SQL如下：

```
select * from employee where name like '%杰伦%';
```

**「解析：」** 这里考察的知识点就是，like是否会导致不走索引，看先该SQL的explain执行计划吧。其实like 模糊搜索，会导致不走索引的，如下:

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvX46kfxLrRFtDtMrKffeVRT8rNQaYsGsGhiboSiblgjhrtCuPU4D4beaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因此，这条SQL最后就全表扫描啦~日常开发中，这几种骚操作都可能会导致索引失效，如下：

- 1.查询条件包含or，可能导致索引失效，where 语句里面如果带有or条件, myisam表能用到索引， innodb不行。innodb可用in来替换or (IN肯定会走索引，但是当IN的取值范围较大时会导致索引失效，走全表扫描)
- 2.如何字段类型是字符串，where时一定用引号括起来，否则索引失效
- 3.like通配符可能导致索引失效。
- 4.联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。
- 5.在索引列上使用mysql的内置函数（Count()、Sum()、Max()、Min()、Avg()等），索引失效。
- 6.对索引列运算（如，+、-、*、/），索引失效。
- 7.索引字段上使用（！= 或者 < >，not in）时，可能会导致索引失效。 
- 8.索引字段上使用is null， is not null，可能导致索引失效。
- 9.左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。
- 10.mysql估计使用全表扫描要比使用索引快,则不使用索引。





### 8.联合索引之最左前缀原则

**「面试官：」** 如果我现在给name,age字段加上联合索引索引，以下SQL执行多少次树搜索呢？先画下索引树？

```
select * from employee where name like '小%' order by age desc;
```

**「解析：」** 这里考察联合索引的最左前缀原则以及like是否中索引的知识点。组合索引树示意图大概如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGv50Rhl86ABicHbnefEsOiafNtEoiahB6fMUBHzQEVmicklLrZCD9SfBpLsQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

联合索引项是先按姓名name从小到大排序，如果名字name相同，则按年龄age从小到大排序。面试官要求查所有名字第一个字是“小”的人，SQL的like '小%'是可以用上`idx_name_age`联合索引的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvNiaiaR7w5LicGQ4ulyv4cyCvxS1PWMUKJ91ccibvsCRgrH1LtNCHa8j21w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

该查询会沿着idx_name_age索引树，找到第一个字是小的索引值，因此依次找到`小军、小伦、小燕、`，分别拿到Id=`600、100、700`，然后回三次表，去找对应的记录。这里面的最左前缀`小`，就是字符串索引的最左M个字符。实际上，

- 这个最左前缀可以是联合索引的最左N个字段。比如组合索引（a,b,c）可以相当于建了（a），（a,b）,(a,b,c)三个索引，大大提高了索引复用能力。
- 最左前缀也可以是字符串索引的最左M个字符。

[**联合索引（a，b，c ）索引，查询不同的顺序是否走索引详情分析**](https://www.jianshu.com/p/8b07df465d11)

[**联合索引(a,b,c)，怎么单独检索b用上索引**](https://blog.csdn.net/qq_40262372/article/details/119212422?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-1-119212422.pc_agg_new_rank&utm_term=a+a+b+c+c%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95&spm=1000.2123.3001.4430)

### 9.索引下推

**「面试官：」** 我们还是居于组合索引 idx_name_age，以下这个SQL执行几次树搜索呢？

```
select * from employee where name like '小%' and age=28 and sex='0';
```

**「解析：」** 这里考察索引下推的知识点，如果是**「Mysql5.6之前」**，在idx_name_age索引树，找出所有名字第一个字是“小”的人，拿到它们的主键id，然后回表找出数据行，再去对比年龄和性别等其他字段。如图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvXh87Z9mvt5KElSib5lEIVtueaZpMvribGROeuiciax9FlRybx6NLQJ1rJw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

有些朋友可能觉得奇怪，（name,age)不是联合索引嘛？为什么选出包含“小”字后，不再顺便看下年龄age再回表呢，不是更高效嘛？所以呀，MySQL 5.6 就引入了**「索引下推优化」**，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

因此，MySQL5.6版本之后，选出包含“小”字后，顺表过滤age=28，,所以就只需一次回表。

![		](https://mmbiz.qpic.cn/mmbiz_png/sMmr4XOCBzFf5QJ3PsZT6VlERVkFaQGvLj0vKZQ3JezKaFiaVGTyFy2icZiaDkVwkE8zorNoRbXmwnIfugWtAfI4w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 10.大表添加索引

**「面试官：」** 如果一张表数据量级是千万级别以上的，那么，给这张表添加索引，你需要怎么做呢？

**「解析：」** 我们需要知道一点，给表添加索引的时候，是会对表加锁的。如果不谨慎操作，有可能出现生产事故的。可以参考以下方法：

- 1.先创建一张跟原表A数据结构相同的新表B。
- 2.在新表B添加需要加上的新索引。
- 3.把原表A数据导到新表B
- 4.rename新表B为原表的表名A，原表A换别的表名；

### 11.什么是聚簇索引？何时使用聚簇索引与非聚簇索引 

**1.什么是聚簇索引？**

聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据

非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因

**澄清一个概念**：innodb中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值

**2.何时使用聚簇索引与非聚簇索引？**



![img](https://pic3.zhimg.com/80/v2-fd11f44ea53d4475e262ee648022731a_720w.jpg)





**3.非聚簇索引一定会回表查询吗？**

不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。

举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age < 20的查询时，在索引的叶子节点上，已经包含了age信息，不会再次进行回表查询。

**注**：如果是通过非主键索引进行查询，select所要获取的字段不能通过非主键索引获取到，需要通过非主键索引获取到的主键，从聚集索引再次查询一遍，获取到所要查询的记录，这个查询的过程就是回表



### 12.Hash索引和B+树所有有什么区别或者说优劣呢? 

**首先要知道Hash索引和B+树索引的底层实现原理：**

hash索引底层就是hash表，进行查找时，调用一次hash函数就可以获取到相应的键值，之后进行回表查询获得实际数据。

B+树底层实现是多路平衡查找树。

对于每一次的查询都是从根节点出发，查找到叶子节点方可以获得所查键值，然后根据查询判断是否需要回表查询数据。



**那么可以看出他们有以下的不同：**

hash索引进行等值查询更快(一般情况下)，但是却无法进行范围查询。

因为在hash索引中经过hash函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询。

而B+树的的所有节点皆遵循(左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。

Hash仅支持=、>、>=、<、<=、between。BTree可以支持like模糊查询

BTree索引是最常用的mysql数据库索引算法，因为它不仅可以被用在=,>,>=,<,<=和between这些比较操作符上，而且还可以用于like操作符

**hash索引不支持使用索引进行排序，原理同上。**

hash索引不支持模糊查询以及多列索引的最左前缀匹配。原理也是因为hash函数的不可预测。AAAA和AAAAB的索引没有相关性。

hash索引任何时候都避免不了回表查询数据，而B+树在符合某些条件(聚簇索引，覆盖索引等)的时候可以只通过索引完成查询。

hash索引虽然在等值查询上较快，但是不稳定。性能不可预测，当某个键值存在大量重复的时候，发生hash碰撞，此时效率可能极差。而B+树的查询效率比较稳定，对于所有的查询都是从根节点到叶子节点，且树的高度较低。

因此，在大多数情况下，直接选择B+树索引可以获得稳定且较好的查询速度。而不需要使用hash索引。

### 总结与练习

本文主要讲解了索引的9大关键面试考点，希望对大家有帮助。接下来呢，给大家出一道，有关于我最近业务开发遇到的加索引SQL，看下大家是怎么回答的，有兴趣可以联系我，一起讨论哈~题目如下：

```
select * from A where type ='1' and status ='s' order by create_time desc;
```

假设type有9种类型，区分度还算可以，status的区分度不高（有3种类型），那么你是如何加索引呢？

- 是给type加单索引
- 还是（type，status，create_time）联合索引
- 还是（type，create_time）联合索引呢？





# 六、设计表时要注意：

1 表字段避免null值出现，null值很难查询优化且占用额外的索引空间，推荐默认数字0代替null。

2 尽量使用INT而非BIGINT，如果非负则加上UNSIGNED(这样数值容量会扩大一倍)，当然能使用TINYINT、SMALLINT、MEDIUM_INT更好。

3 使用枚举或整数代替字符串类型

4 尽量使用TIMESTAMP而非DATETIME

5 单表不要有太多字段，建议在20以内

6 用整型来存IP

##### 索引

1 索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描

2 应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描

3 值分布很稀少的字段不适合建索引，例如"性别"这种只有两三个值的字段

4 字符字段只建前缀索引

5 字符字段最好不要做主键

6 不用外键，由程序保证约束

7 尽量不用UNIQUE，由程序保证约束

8 使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引

简言之就是使用合适的数据类型，选择合适的索引



##### 选择合适的数据类型

(1)使用可存下数据的最小的数据类型，整型 < date,time < char,varchar < blob

(2)使用简单的数据类型，整型比字符处理开销更小，因为字符串的比较更复杂。如，int类型存储时间类型，bigint类型转ip函数

(3)使用合理的字段属性长度，固定长度的表会更快。使用enum、char而不是varchar

(4)尽可能使用not null定义字段

(5)尽量少用text，非用不可最好分表

##### 选择合适的索引列

(1)查询频繁的列，在where，group by，order by，on从句中出现的列

(2)where条件中<，<=，=，>，>=，between，in，以及like 字符串+通配符(%)出现的列

(3)长度小的列，索引字段越小越好，因为数据库的存储单位是页，一页中能存下的数据越多越好

(4)离散度大(不同的值多)的列，放在联合索引前面。查看离散度，通过统计不同的列值来实现，count越大，离散程度越高：

##### sql的编写需要注意优化

1 使用limit对查询结果的记录进行限定

2 避免select *，将需要查找的字段列出来

3 使用连接(join)来代替子查询

4 拆分大的delete或insert语句

5 可通过开启慢查询日志来找出较慢的SQL

6 不做列运算：SELECT id WHERE age + 1 = 10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边

7 sql语句尽可能简单：一条sql只能在一个cpu运算;大语句拆小语句，减少锁时间;一条大sql可以堵死整个库

8 OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内

> or/in 效率对比：  https://blog.csdn.net/qq_25135655/article/details/77542023

>  多数据库服务器都只把IN()看作多个OR的同义词，因为它们在逻辑上是相等的。MYSQL不是这样的，它会对IN()里面的数据进行排序，然后用二分法查找个是否在列表中，这个算法的效率是Ｏ（logn),而等同的OR子句的查找效率是Ｏ(n)。在列表很大的时候，OR子句就会变得慢得多。 

9 不用函数和触发器，在应用程序实现

10 避免%xxx式查询

11 少用JOIN

12 使用同类型进行比较，比如用'123'和'123'比，123和123比

13 尽量避免在WHERE子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描

14 对于连续数值，使用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5

15 列表数据不要拿全表，要使用LIMIT来分页，每页数量也不要太大

##### 引擎

目前广泛使用的是MyISAM和InnoDB两种引擎：

##### MyISAM

MyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是：

1 不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁
2 不支持事务
3 不支持外键
4 不支持崩溃后的安全恢复
5 在表有读取查询的同时，支持往表中插入新纪录
6 支持BLOB和TEXT的前500个字符索引，支持全文索引
7 支持延迟更新索引，极大提升写入性能
8 对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用

##### InnoDB

InnoDB在MySQL 5.5后成为默认索引，它的特点是：
1.支持行锁，采用MVCC来支持高并发
2.支持事务
3.支持外键
4.支持崩溃后的安全恢复
5.不支持全文索引

总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表

MyISAM速度可能超快，占用存储空间也小，但是程序要求事务支持，故InnoDB是必须的，故该方案无法执行，放弃!



# 七、说说分库分表？

随着用户量的激增和时间的堆砌，存在数据库里面的数据越来越多，此时的数据库就会产生瓶颈，出现资源报警、查询慢等场景。

首先单机数据库所能承载的连接数、I/O及网络的吞吐等都是有限的，所以当并发量上来了之后，数据库就渐渐顶不住了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/eSdk75TK4nEWAm9e6WibicVVnudWpTXSpbLKZHAvclSNUEknzcbZW02kVoUtBInLibrATW08HISruNjxULAicJSiakw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

再则，如果单表的数据量过大，查询的性能也会下降。因为数据越多 B+ 树就越高，树越高则查询 I/O 的次数就越多，那么性能也就越差。

因为上述的原因，不得已就得上分库分表了。

把以前存在一个数据库实例里的数据拆分成多个数据库实例，部署在不同的服务器中，这是分库。

把以前存在一张表里面的数据拆分成多张表，这是分表。

一般而言：

- 分表：是为了解决由于单张表数据量多大，而导致查询慢的问题。大致三、四千万行数据就得拆分，不过具体还是得看每一行的数据量大小，有些字段都很小的可能支持更多行数，有些字段大的可能一千万就顶不住了。
- 分库：是为了解决服务器资源受单机限制，顶不住高并发访问的问题，把请求分配到多台服务器上，降低服务器压力。

# 你们一般怎么分库的？

一般分库都是按照业务划分的，比如订单库、用户库等等。

有时候会针对一些特殊的库再作切分，比如一些活动相关的库都做了拆分。

因为做活动的时候并发可能会比较高，怕影响现有的核心业务，所以即使有关联，也会单独做拆分。

![图片](https://mmbiz.qpic.cn/mmbiz_png/eSdk75TK4nEWAm9e6WibicVVnudWpTXSpb6yoRNpHXmcfQqEqlOKaXxPqjotTEiaib2uCoXtWA3AebKEgiapcW6f3Aw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

# 那你觉得分库会带来什么问题呢？

> 首先是事务的问题。

我们使用关系型数据库，有很大一点在于它保证事务完整性。

而分库之后单机事务就用不上了，必须使用分布式事务来解决，而分布式事务基本的都是残缺的(我之前文章把分布式事务汇总了一波，后台搜索分布式事务就有了)。

这是很重要的一点需要考虑。

> 连表 JOIN 问题

在一个库中的时候我们还可以利用 JOIN 来连表查询，而跨库了之后就无法使用 JOIN 了。

此时的解决方案就是在业务代码中进行关联，也就是先把一个表的数据查出来，然后通过得到的结果再去查另一张表，然后利用代码来关联得到最终的结果。

这种方式实现起来稍微比较复杂，不过也是可以接受的。

还有可以适当的冗余一些字段。比如以前的表就存储一个关联 ID，但是业务时常要求返回对应的 Name 或者其他字段。这时候就可以把这些字段冗余到当前表中，来去除需要关联的操作。

# 那你们怎么分表的？

分表其实有两种：

- 垂直分表
- 水平分表

垂直分表，来看个图，很直观：

![图片](https://mmbiz.qpic.cn/mmbiz_png/eSdk75TK4nEWAm9e6WibicVVnudWpTXSpb4eaOkdIoQsksMWPEyDJvonV78gfmqqelicN2FFfb7RZ7OJwFF6cSdEg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

垂直分表就是把一些不常用的大字段剥离出去。

像上面的例子：用户名是很常见的搜索结果，性别和年龄占用的空间又不大，而地址和个人简介占用的空间相对而言就较大，我们都知道一个数据页的空间是有限的，把一些无用的数据拆分出去，一页就能存放更多行的数据。

内存存放更多有用的数据，就减少了磁盘的访问次数，性能就得到提升。

水平分表，则是因为一张表内的数据太多了，上文也提到了数据越多 B+ 树就越高，访问的性能就差，所以进行水平拆分。

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其实不管这些，浅显的理解下，在一百个数据里面找一个数据快，还是在一万个数据里面找一个数据快？

即使有索引，那厚的书目录多，翻目录也慢~

# 那分表会有什么问题？

垂直分表还好，就是需要关联一下，而水平分表就有点麻烦了。

> 排序、count、分页问题

如果一个用户的数据被拆分到多个表中，那查询结果分页就不像以前单张表那样直接就能查出来了，像 count 操作也是一样的。

只能由业务代码来实现或者用中间件将各表中的数据汇总、排序、分页然后返回。

像 count 操作的结果其实可以缓存下来，然后每次数据增删都更新计数。

> 路由问题

分表的路由可以分：

- Hash 路由
- 范围路由
- 路由表

Hash 路由，其实就是选择表中的某一列，然后进行 Hash 运算，将 Hash 运算得到的结果再对子表数进行取模，这样就能均匀的将数据分到不同的子表上。

这跟 HashMap 选哪个桶是一样的原理。

优点就是数据分布均匀。

缺点就是增加子表的时候麻烦，想想 HashMap的扩容，是不是得搬迁数据？这个分表也是一样的，我们可都知道，数据迁移一件麻烦事！

范围路由，其实很简单，可以是时间，也可以是地址，表示一定的范围的即可。

比如本来一张 User 表，我可以分 User_HZ、User_BJ、User_SH，按照地名来划分 User。

再比如 log 表，我可以将表分为 log_202103、 log_202104，把日志按照年月来划分。

优点就是相对而言比较容易扩展，比如现在来个 GZ，那就加个 User_GZ。如果到了 5 月，那就建个 log_202105。

缺点就是数据可能分布不均匀，例如 BJ 的用户特别多或者某个月搞了促销，日志量特别大，等等。

路由表，就是专门搞个表来记录路由信息，来看个图就很清楚了。

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

从图中我们就能得知，UserID 为 2 的用户数据在要去 User_3 这个用户表查询。

优点就是灵活咯，如果要迁移数据，直接迁移然后路由表一改就完事儿了~

缺点就是得多查一次，每次查询都需要访问路由表，不过这个一般会做缓存的。

> 全局主键问题

以前单表的时候很简单，就是主键自增，现在分表了之后就有点尴尬了。

所以需要一些手段来保证全局主键唯一。

1. 还是自增，只不过自增步长设置一下。比如现在有三张表，步长设置为3，三张表 ID 初始值分别是1、2、3。这样第一张表的 ID 增长是 1、4、7。第二张表是2、5、8。第三张表是3、6、9，这样就不会重复了。
2. UUID，这种最简单，但是不连续的主键插入会导致严重的页分裂，性能比较差。
3. 分布式 ID，比较出名的就是 Twitter 开源的 sonwflake 雪花算法，具体就不展开了，不然就又是一篇文章了，简单点利用 redis 来递增也行。

# 那上面说的路由问题的 Sharding-Key 如何设计呢？

我们分表是按照某个列来拆分的，那个列就是 Sharding-Key，查询的时候必须带上这个列才行。

例如上面提到的  log_202103，那表明查询条件一定得带上日期，这样才能找到正确的表。

所以设计上得考虑查询的条件来作为 Sharding-Key。

举个常常会被问的订单表 Sharding-Key 例子。

你想着查找订单的时候会通过订单号去找，所以应该利用订单 ID 来作为 Sharding-Key。

但是你想想，你打开外卖软件想查找你的历史订单的时候，你是没有订单 ID 的，你只有你的 UserID，那此时只能把所有子表都通过 UserID 遍历一遍，这样效率就很低了！

所以你想着那用 UserID 来作为 Sharding-Key 吧！

但是，商家呢？商家肯定关心自己今天卖了多少单，所以他也要查找订单，但他只有自己的商家 ID，所以如果要查询订单，只能把所有子表都通过商家 ID 遍历一遍，这样效率就很低了！

所以 Sharding-Key 是满足不了所有查询需求的，只能曲线救国。

一般做法就是冗余数据。

将订单同步到另一张表中给商家使用，这个表按商家 ID 来作为 Sharding-Key，也可以将数据同步到 ES 中。一般而言这里的数据同步都是异步处理，不会影响正常流程。





# 八、在日常工作中怎么做MySQL优化的？



**MySQL常见的优化手段分为下面几个方面：**

SQL优化、设计优化，硬件优化等，其中每个大的方向中又包含多个小的优化点

![图片](https://mmbiz.qpic.cn/mmbiz_png/hC3oNAJqSRyHzQFLFcTsrVbcYguC9bVrFxWgMe5HPnMqoXwEBbPFPl3o1aBakiahmoD4Q1lZqphbiamncOKK4ZUA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



# 1.SQL优化

此优化方案指的是通过优化 SQL 语句以及索引来提高 MySQL 数据库的运行效率，具体内容如下：

## 分页优化

例如：

```
select * from table where type = 2 and level = 9 order by id asc limit 190289,10;
```

优化方案：

- 延迟关联

  先通过where条件提取出主键，在将该表与原数据表关联，通过主键id提取数据行，而不是通过原来的二级索引提取数据行

  例如：

```
select a.* from table a, (select id from table where type = 2 and level = 9 order by id asc limit 190289,10 ) b where a.id = b.id
```

- 书签方式

  书签方式说白了就是找到limit第一个参数对应的主键值，再根据这个主键值再去过滤并limit

  例如：

```
select * from table where id > (select * from table where type = 2 and level = 9 order by id asc limit 190289, 1) limit 10;
```

## 索引优化

**正确使用索引**

假如我们没有添加索引，那么在查询时就会触发全表扫描，因此查询的数据就会很多，并且查询效率会很低，为了提高查询的性能，我们就需要给最常使用的查询字段上，添加相应的索引，这样才能提高查询的性能

> 建立覆盖索引

InnoDB使用辅助索引查询数据时会回表，但是如果索引的叶节点中已经包含要查询的字段，那它没有必要再回表查询了，这就叫覆盖索引

例如对于如下查询：

```
select name from test where city='上海'
```

我们将被查询的字段建立到联合索引中，这样查询结果就可以直接从索引中获取

```
alter table test add index idx_city_name (city, name);
```

> 在 MySQL 5.0 之前的版本尽量避免使用or查询

在 MySQL 5.0 之前的版本要尽量避免使用 or 查询，可以使用 union 或者子查询来替代，因为早期的 MySQL 版本使用 or 查询可能会导致索引失效，在 MySQL 5.0 之后的版本中引入了索引合并

索引合并简单来说就是把多条件查询，比如or或and查询对多个索引分别进行条件扫描，然后将它们各自的结果进行合并，因此就不会导致索引失效的问题了

如果从Explain执行计划的type列的值是`index_merge`可以看出MySQL使用索引合并的方式来执行对表的查询

> 避免在 where 查询条件中使用 != 或者 <> 操作符

SQL中，不等于操作符会导致查询引擎放弃索引索引，引起全表扫描，即使比较的字段上有索引

解决方法：通过把不等于操作符改成or，可以使用索引，避免全表扫描

例如，把`column<>’aaa’，改成column>’aaa’ or column<’aaa’`，就可以使用索引了

> 适当使用前缀索引

MySQL 是支持前缀索引的，也就是说我们可以定义字符串的一部分来作为索引

我们知道索引越长占用的磁盘空间就越大，那么在相同数据页中能放下的索引值也就越少，这就意味着搜索索引需要的查询时间也就越长，进而查询的效率就会降低，所以我们可以适当的选择使用前缀索引，以减少空间的占用和提高查询效率

比如，邮箱的后缀都是固定的“`@xxx.com`”，那么类似这种后面几位为固定值的字段就非常适合定义为前缀索引

```
alter table test add index index2(email(6));
```

使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本

需要注意的是，前缀索引也存在缺点，MySQL无法利用前缀索引做order by和group by 操作，也无法作为覆盖索引

> 查询具体的字段而非全部字段

要尽量避免使用`select *`，而是查询需要的字段，这样可以提升速度，以及减少网络传输的带宽压力

> 优化子查询

尽量使用 Join 语句来替代子查询，因为子查询是嵌套查询，而嵌套查询会新创建一张临时表，而临时表的创建与销毁会占用一定的系统资源以及花费一定的时间，同时对于返回结果集比较大的子查询，其对查询性能的影响更大

> 小表驱动大表

我们要尽量使用小表驱动大表的方式进行查询，也就是如果 B 表的数据小于 A 表的数据，那执行的顺序就是先查 B 表再查 A 表，具体查询语句如下：

```
select name from A where id in (select id from B);
```

> 不要在列上进行运算操作

不要在列字段上进行算术运算或其他表达式运算，否则可能会导致查询引擎无法正确使用索引，从而影响了查询的效率

```
select * from test where id + 1 = 50;
select * from test where month(updateTime) = 7;
```

一个很容易踩的坑：隐式类型转换：

```
select * from test where skuId=123456
```

skuId这个字段上有索引，但是explain的结果却显示这条语句会全表扫描

原因在于skuId的字符类型是varchar(32)，比较值却是整型，故需要做类型转换

**适当增加冗余字段**

增加冗余字段可以减少大量的连表查询，因为多张表的连表查询性能很低，所有可以适当的增加冗余字段，以减少多张表的关联查询，这是以空间换时间的优化策略

**正确使用联合索引**

使用了 B+ 树的 MySQL 数据库引擎，比如 InnoDB 引擎，在每次查询复合字段时是从左往右匹配数据的，因此在创建联合索引的时候需要注意索引创建的顺序

例如，我们创建了一个联合索引是`idx(name,age,sex)`，那么当我们使用，姓名+年龄+性别、姓名+年龄、姓名等这种最左前缀查询条件时，就会触发联合索引进行查询；然而如果非最左匹配的查询条件，例如，性别+姓名这种查询条件就不会触发联合索引

## Join优化

MySQL的join语句连接表使用的是nested-loop join算法，这个过程类似于嵌套循环，简单来说，就是遍历驱动表（外层表），每读出一行数据，取出连接字段到被驱动表（内层表）里查找满足条件的行，组成结果行

要提升join语句的性能，就要尽可能减少嵌套循环的循环次数

一个显著优化方式是对被驱动表的join字段建立索引，利用索引能快速匹配到对应的行，避免与内层表每一行记录做比较，极大地减少总循环次数。另一个优化点，就是连接时用小结果集驱动大结果集，在索引优化的基础上能进一步减少嵌套循环的次数

如果难以判断哪个是大表，哪个是小表，可以用inner join连接，MySQL会自动选择**小表去驱动大表**

**避免使用JOIN关联太多的表**

对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由`join_buffer_size`参数进行设置

在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大

如果程序中大量的使用了多表关联的操作，同时`join_buffer_size`设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性

## 排序优化

**利用索引扫描做排序**

MySQL有两种方式生成有序结果：其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的

但是如果索引不能覆盖查询所需列，就不得不每扫描一条记录回表查询一次，这个读操作是随机IO，通常会比顺序全表扫描还慢

因此，在设计索引时，尽可能使用同一个索引既满足排序又用于查找行

例如：

```
--建立索引（date,staff_id,customer_id）
select staff_id, customer_id from test where date = '2010-01-01' order by staff_id,customer_id;
```

只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向都一样时，才能够使用索引来对结果做排序

## UNION优化

MySQL处理union的策略是先创建临时表，然后将各个查询结果填充到临时表中最后再来做查询，很多优化策略在union查询中都会失效，因为它无法利用索引

最好手工将where、limit等子句下推到union的各个子查询中，以便优化器可以充分利用这些条件进行优化

此外，除非确实需要服务器去重，一定要使用union all，如果不加all关键字，MySQL会给临时表加上distinct选项，这会导致对整个临时表做唯一性检查，代价很高

## 慢查询日志

出现慢查询通常的排查手段是先使用慢查询日志功能，查询出比较慢的 SQL 语句，然后再通过 Explain 来查询 SQL 语句的执行计划，最后分析并定位出问题的根源，再进行处理

慢查询日志指的是在 MySQL 中可以通过配置来开启慢查询日志的记录功能，超过`long_query_time`值的 SQL 将会被记录在日志中

我们可以通过设置`“slow_query_log=1”`来开启慢查询

需要注意的是，在开启慢日志功能之后，会对 MySQL 的性能造成一定的影响，因此在生产环境中要慎用此功能







# 2.设计优化

**尽量避免使用NULL**

NULL在MySQL中不好处理，存储需要额外空间，运算也需要特殊的运算符，含有NULL的列很难进行查询优化

应当指定列为not null，用0、空串或其他特殊的值代替空值，比如定义为int not null default 0

**最小数据长度**

越小的数据类型长度通常在磁盘、内存和CPU缓存中都需要更少的空间，处理起来更快

**使用最简单数据类型**

简单的数据类型操作代价更低，比如：能使用 int 类型就不要使用 varchar 类型，因为 int 类型比 varchar 类型的查询效率更高

**尽量少定义 text 类型**

text 类型的查询效率很低，如果必须要使用 text 定义字段，可以把此字段分离成子表，需要查询此字段时使用联合查询，这样可以提高主表的查询效率

**适当分表、分库策略**

分表是指当一张表中的字段更多时，可以尝试将一张大表拆分为多张子表，把使用比较高频的主信息放入主表中，其他的放入子表，这样我们大部分查询只需要查询字段更少的主表就可以完成了，从而有效的提高了查询的效率

分库是指将一个数据库分为多个数据库。比如我们把一个数据库拆分为了多个数据库，一个主数据库用于写入和修改数据，其他的用于同步主数据并提供给客户端查询，这样就把一个库的读和写的压力，分摊给了多个库，从而提高了数据库整体的运行效率

## 常见类型选择

**整数类型宽度设置**

MySQL可以为整数类型指定宽度，例如int(11)，实际上并没有意义，它并不会限制值的范围，对于存储和计算来说，int(1)和int(20)是相同的

**VARCHAR和CHAR类型**

char类型是定长的，而varchar存储可变字符串，比定长更省空间，但是varchar需要额外1或2个字节记录字符串长度，更新时也容易产生碎片

需要结合使用场景来选择：如果字符串列最大长度比平均长度大很多，或者列的更新很少，选择varchar较合适；如果要存很短的字符串，或者字符串值长度都相同，比如MD5值，或者列数据经常变更，选择使用char类型

**DATETIME和TIMESTAMP类型**

datetime的范围更大，能表示从1001到9999年，timestamp只能表示从1970年到2038年。datetime与时区无关，timestamp显示值依赖于时区。在大多数场景下，这两种类型都能良好地工作，但是建议使用timestamp，因为datetime占用8个字节，timestamp只占用了4个字节，timestamp空间效率更高

**BLOB和TEXT类型**

blob和text都是为存储很大数据而设计的字符串数据类型，分别采用二进制和字符方式存储

在实际使用中，要慎用这两种类型，它们的查询效率很低，如果字段必须要使用这两种类型，可以把此字段分离成子表，需要查询此字段时使用联合查询，这样可以提高主表的查询效率

## 范式化

当数据较好范式化时，修改的数据更少，而且范式化的表通常要小，可以有更多的数据缓存在内存中，所以执行操作会更快

缺点则是查询时需要更多的关联

第一范式：字段不可分割，数据库默认支持

第二范式：消除对主键的部分依赖，可以在表中加上一个与业务逻辑无关的字段作为主键，比如用自增id

第三范式：消除对主键的传递依赖，可以将表拆分，减少数据冗余

# 3.硬件优化

MySQL 对硬件的要求主要体现在三个方面：磁盘、网络和内存

**磁盘**

磁盘应该尽量使用有高性能读写能力的磁盘，比如固态硬盘，这样就可以减少 I/O 运行的时间，从而提高了 MySQL 整体的运行效率

磁盘也可以尽量使用多个小磁盘而不是一个大磁盘，因为磁盘的转速是固定的，有多个小磁盘就相当于拥有多个并行运行的磁盘一样

**网络**

保证网络带宽的通畅（低延迟）以及够大的网络带宽是 MySQL 正常运行的基本条件，如果条件允许的话也可以设置多个网卡，以提高网络高峰期 MySQL 服务器的运行效率

**内存**

MySQL 服务器的内存越大，那么存储和缓存的信息也就越多，而内存的性能是非常高的，从而提高了整个 MySQL 的运行效率



# 4.数据库配置优化

 

### 4.1 全局配置

**（1）max_connections**
**最大连接数。默认值是151，最多2000**。如果服务器的并发连接请求量比较大，建议调高此值，以增加并行连接数量。但是如果连接数越多，介于MySQL会为每个连接提供连接缓冲区，就会开销越多的内存，所以要适当调整该值。
查看最大连接数

```
mysql> SHOW VARIABLES LIKE 'max_connections';
```

查看响应的连接数

```
mysql> SHOW STATUS LIKE 'max%connections';
```

**max_used_connections / max_connections \* 100% （理想值≈85%）** 
**如果max_used_connections跟max_connections相同 那么就是max_connections设置过低或者超过服务器负载上限了**，**低于10%则设置过大。**
**（2）back_log**
**MySQL能暂存的连接数量，默认值是80，最多512，可设置为128**。如果MySQL的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，***该堆栈的数量即back_log\***。如果等待连接的数量超过back_log，将不被授予连接资源。当主要MySQL线程在一个很短时间内得到非常多的连接请求，这就起作用。**类似于线程池**
**（3）key_buffer_size**
索引缓冲区的大小，它决定索引处理的速度，尤其是索引读的速度。
通过检查状态值Key_read_requests和Key_reads，可以知道key_buffer_size设置是否合理。

```
mysql> SHOW STATUS LIKE 'key_read%';
+-------------------+----------+
| Variable_name     | Value    |
+-------------------+----------+
| Key_read_requests | 90585564 |
| Key_reads         | 97031    |
+-------------------+----------+
```

计算索引未命中缓存的概率：
key_cache_miss_rate = Key_reads / Key_read_requests * 100%，设置在1/1000左右较好
key_buffer_size只对MyISAM表起作用。即使你不使用MyISAM表，但是内部的临时磁盘表是MyISAM表，也要使用该值。
默认配置数值是8388608(8M)，主机有4GB内存，可改为268435456(256M)
**（4）query_cache_size**
使用查询缓存(query cache)，MySQL将查询结果存放在缓冲区中，今后对于同样的SELECT语句（区分大小写），将直接从缓冲区中读取结果。
**最佳选项是将其从一开始就停用，设为0**（现在MySQL 5.6的默认值）并利用其他方法加速查询：优化索引、增加拷贝分散负载或者启用额外的缓存（比如Redis或Memcached）。
通过检查状态值qcache_*，可以知道query_cache_size设置是否合理

```
mysql> SHOW STATUS LIKE 'qcache%';
+-------------------------+----------+
| Variable_name           | Value    |
+-------------------------+----------+
| Qcache_free_blocks      | 1        |
| Qcache_free_memory      | 1031360  |
| Qcache_hits             | 0        |
| Qcache_inserts          | 0        |
| Qcache_lowmem_prunes    | 0        |
| Qcache_not_cached       | 10302865 |
| Qcache_queries_in_cache | 0        |
| Qcache_total_blocks     | 1        |
+-------------------------+----------+
```

查询缓存碎片率 = Qcache_free_blocks / Qcache_total_blocks * 100%
如果查询缓存碎片率超过20%，可以用FLUSH QUERY CACHE整理缓存碎片，或者试试减小query_cache_min_res_unit，如果你的查询都是小数据量的话。
查询缓存利用率 = (query_cache_size – Qcache_free_memory) / query_cache_size * 100%
查询缓存利用率在25%以下的话说明query_cache_size设置的过大，可适当减小；查询缓存利用率在80％以上而且Qcache_lowmem_prunes > 50的话说明query_cache_size可能有点小，要不就是碎片太多。
查询缓存命中率 = (Qcache_hits – Qcache_inserts) / Qcache_hits * 100%
如果Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况，如果Qcache_hits的值也非常大，则表明查询缓冲使用非常频繁，此时需要增加缓冲大小；如果Qcache_hits的值不大，则表明你的查询重复率很低，这种情况下使用查询缓冲反而会影响效率，那么可以考虑不用查询缓冲。此外，在SELECT语句中加入SQL_NO_CACHE可以明确表示不使用查询缓冲。
与查询缓冲有关的参数还有query_cache_type、query_cache_limit、query_cache_min_res_unit。

* query_cache_type：指定是否使用查询缓冲，可以设置为0、1、2，该变量是SESSION级的变量。
* query_cache_limit：指定单个查询能够使用的缓冲区大小，缺省为1M。
* query_cache_min_res_unit：指定分配缓冲区空间的最小单位，缺省为4K。检查状态Qcache_free_blocks，如果该值非常大，则表明缓冲区中碎片很多，这就表明查询结果都比较小，此时需要减小query_cache_min_res_unit。



**（5）read_buffer_size**
是MySQL读入缓冲区的大小，将对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区，read_buffer_size变量控制这一缓冲区的大小，如果对表的顺序扫描非常频繁，并你认为频繁扫描进行的太慢，可以通过增加该变量值以及内存缓冲区大小提高其性能。
**默认数值是131072(128K)，可改为16773120(16M)**
**（6）read_rnd_buffer_size**
随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySQL会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySQL会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。
默认数值是262144(256K)，可改为16777208(16M)
**（7）sort_buffer_size**
每个需要进行排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY或GROUP BY操作。
默认数值是10485760(1M)，可改为16777208(16M)
**（8）join_buffer_size**
联合查询操作所能使用的缓冲区大小
read_buffer_size，read_rnd_buffer_size，sort_buffer_size，join_buffer_size为每个线程独占，也就是说，如果有100个线程连接，则占用为16M*100
**（9）table_open_cache**
表高速缓存的大小。每当MySQL访问一个表时，如果在表缓冲区中还有空间，该表就被打开并放入其中，这样可以更快地访问表内容。
通过检查峰值时间的状态值Open_tables和Opened_tables，可以决定是否需要增加table_cache的值。

```
mysql> SHOW STATUS LIKE 'open%tables';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| Open_tables   | 2000  |
| Opened_tables | 0     |
+---------------+-------+
```

如果open_tables等于table_cache，并且opened_tables在不断增长，那么就需要增加table_cache的值了。注意，不能盲目地把table_cache设置成很大的值。如果设置得太高，可能会造成文件描述符不足，从而造成性能不稳定或者连接失败。
1G内存机器，推荐值是128-256。内存在4GB左右的服务器该参数可设置为256M或384M。
**（10）max_heap_table_size**
用户可以创建的内存表(memory table)的大小。这个值用来计算内存表的最大行数值。
这个变量和tmp_table_size一起限制了内部内存表的大小。如果某个内部heap（堆积）表大小超过tmp_table_size，MySQL可以根据需要自动将内存中的heap表改为基于硬盘的MyISAM表。
**（11）tmp_table_size**
临时表的大小，例如做高级GROUP BY操作生成的临时表。如果调高该值，MySQL同时将增加heap表的大小，可达到提高联接查询速度的效果，建议尽量优化查询，要确保查询过程中生成的临时表在内存中，避免临时表过大导致生成基于硬盘的MyISAM表。

```
mysql> SHOW GLOBAL STATUS LIKE 'created_tmp%';
+-------------------------+----------+
| Variable_name           | Value    |
+-------------------------+----------+
| Created_tmp_disk_tables | 2884297  |
| Created_tmp_files       | 870      |
| Created_tmp_tables      | 15899696 |
+-------------------------+----------+
```

每次创建临时表，Created_tmp_tables增加，如果临时表大小超过tmp_table_size，则是在磁盘上创建临时表，Created_tmp_disk_tables也增加。
Created_tmp_files表示MySQL服务创建的临时文件文件数，比较理想的配置是：
Created_tmp_disk_tables / Created_tmp_tables * 100% <= 25%
**（12）thread_cache_size**
线程缓存。当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁（前提是缓存数未达上限）。

```
mysql> SHOW STATUS LIKE 'threads%';
+-------------------+---------+
| Variable_name     | Value   |
+-------------------+---------+
| Threads_cached    | 5       |
| Threads_connected | 13      |
| Threads_created   | 1095313 |
| Threads_running   | 1       |
+-------------------+---------+
```

Threads_cached :代表当前此时此刻线程缓存中有多少空闲线程。如果过大，表明MySQL服务器一直在创建线程，这也是比较耗资源，可以适当增加thread_cache_size
Threads_connected :代表当前已建立连接的数量，因为一个连接就需要一个线程，所以也可以看成当前被使用的线程数。
Threads_created :代表从最近一次服务启动，已创建线程的数量。
Threads_running :代表当前激活的（非睡眠状态）线程数。并不是代表正在使用的线程数，有时候连接已建立，但是连接处于sleep状态，这里相对应的线程也是sleep状态。
建议设置接近Threads_connected值，再结合物理内存：1G-8；2G-16；3G-32 综合考虑一下值。
**（13）interactive_timeout**
一个交互连接在被服务器在关闭前等待行动的秒数。默认值是28800（8小时），可设置为7200。
**（14）wait_timeout**
一个非交互连接在被服务器在关闭前等待行动的秒数。要同时设置interactive_timeout和wait_timeout才会生效。

### 4.2 InnoDB配置

**（1）innodb_buffer_pool_size**
**缓冲池的大小，缓存数据和索引**，**对InnoDB整体性能影响较大，****相当于MyISAM的key_buffer_size。如果只用Innodb，可以把这个值设为内存的70%-80%。越大越好，这能保证你在大多数的读取操作时使用的是内存而不是硬盘。**
**（2）innodb_log_buffer_size** 
**尚未执行的事务的缓存大小，默认值为8M，**一般8M-16M。**如果你有很多事务的更新，插入或删除操作，通过这个参数会大量的节省了磁盘I/O**。但是如果你的事务中包含有二进制大对象或者大文本字段的话，这点缓存很快就会被填满并触发额外的I/O操作。看看Innodb_log_waits状态变量，如果它不是0，应该增大这个值。但太大了也是浪费内存，因为1秒钟总会flush一次，所以不需要设到超过1秒的需求。
**（3）innodb_flush_log_at_trx_commit**
把log buffer的数据写入日志文件并flush磁盘的策略，该值对插入数据的速度影响非常大。取值分别为0、1(默认值)、2(推荐值)
0：事务提交时，不写入磁盘，而是每秒把log buffer的数据写入日志文件，并且flush(刷到磁盘)。速度最快，但不安全。mysqld进程的崩溃会导致上一秒钟所有事务数据的丢失。
1：每次事务提交时把log buffer的数据写入日志文件，并且flush(刷到磁盘)。最安全，但也最慢。确保了事务的ACID。
2：每次事务提交时把log buffer的数据写入日志文件，每秒flush(刷到磁盘)。速度较快，比0安全。操作系统崩溃或者系统断电会导致上一秒钟所有事务数据的丢失。
**（4）innodb_log_file_size**
在一个日志组每个日志文件的大小，用于确保写操作快速而可靠并且在崩溃时恢复。一般用64M-512M，具体取决于服务器的空间。大的文件提供更高的性能，但数据库恢复时会用更多的时间。
**（5）innodb_additional_mem_pool_size**
存储数据字典和其他内部数据结构的内存池大小。默认为1M，对于2G内存的机器，推荐值是20M，通常不用太大，应该与表结构的复杂度有关系。如果不够用，MySQL会在错误日志中写入一条警告信息。
**（6）innodb_buffer_pool_instances**
可以开启多个内存缓冲池，这样可以并行的内存读写。默认为8，一般为1-8。最常1s就会刷新一次，故不用太大。对于较大的事务，可以增大缓存大小。如果InnoDB缓存池被划分成多个区域，建议每个区域不小于1GB的空间。



# 九、保证redis和mysql读写一致

## **前言**

四月份的时候，有位好朋友去美团面试。他说，被问到Redis与MySQL双写一致性如何保证？这道题其实就是在问缓存和数据库在双写场景下，一致性是如何保证的？本文将跟大家一起来探讨如何回答这个问题。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRHLOO7rkqKpuUVApd6OqOhyTyQrRYrE4cOmo5HgPZunajON2lb9E6ZA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 公众号：**捡田螺的小男孩**

## **1.谈谈一致性**

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRS99F4S1uorz4BGI1I5sv0WYenLTgREsCMMtMwxEHs5Adykt6xZP47g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。

- **强一致性**：这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大
- **弱一致性**：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态
- **最终一致性**：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型

## **2.三个经典的缓存模式**

缓存可以提升性能、缓解数据库压力，但是使用缓存也会导致数据**不一致性**的问题。一般我们是如何使用缓存呢？有三种经典的缓存使用模式：

- Cache-Aside Pattern
- Read-Through/Write-through
- Write-behind

### 2.1 Cache-Aside Pattern

Cache-Aside Pattern，即**旁路缓存模式**，它的提出是为了尽可能地解决缓存与数据库的数据不一致问题。

**Cache-Aside读流程**

**Cache-Aside Pattern**的读请求流程如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRAZ68JX9LyG7LT6bJfOHgWhbeXypQd0RE3LcicVxian4UtLW1enj6icCkw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Cache-Aside读请求

1. 读的时候，先读缓存，缓存命中的话，直接返回数据
2. 缓存没有命中的话，就去读数据库，从数据库取出数据，放入缓存后，同时返回响应。

Cache-Aside 写流程

**Cache-Aside Pattern**的写请求流程如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCR6HyLYMqNc3o0JAQaSaLqhThRAtNml8mNqBpTDRNAicAXOkakeE00bkA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Cache-Aside写请求

更新的时候，先**更新数据库，然后再删除缓存**。

### 2.2 Read-Through/Write-Through（读写穿透）

**Read/Write-Through**模式中，服务端把缓存作为主要数据存储。应用程序跟数据库缓存交互，都是通过**抽象缓存层**完成的。

**Read-Through**

**Read-Through**的简要流程如下

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRNg934JicTpmXwHn9orTn4dSYzxF4nA5bAvS78OSylNrTBSr9A7icL74Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Read-Through简要流程

1. 从缓存读取数据，读到直接返回
2. 如果读取不到的话，从数据库加载，写入缓存后，再返回响应。

这个简要流程是不是跟**Cache-Aside**很像呢？其实**Read-Through**就是多了一层**Cache-Provider**而已，流程如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRcUc7P99fMX2P9zHice4nB6S4crMdIOKt7euG6tPFbdRdshVPT7wSDWA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Read-Through流程

Read-Through实际只是在**Cache-Aside**之上进行了一层封装，它会让程序代码变得更简洁，同时也减少数据源上的负载。

Write-Through

**Write-Through**模式下，当发生写请求时，也是由**缓存抽象层**完成数据源和缓存数据的更新,流程如下：![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRcOlpgny0miaY8Cll4uAsv96WO1JdBjiaqYUv2TX2XiaP2ZicSDJ99GOS1A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.3 Write-behind （异步缓存写入）

**Write-behind** 跟Read-Through/Write-Through有相似的地方，都是由**Cache Provider**来负责缓存和数据库的读写。它们又有个很大的不同：**Read/Write-Through**是同步更新缓存和数据的，**Write-Behind**则是只更新缓存，不直接更新数据库，通过**批量异步**的方式来更新数据库。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRfbHibY85ws4ejO2iaCEGuJicXYKIqt4gpdxVsqdxFksutMl0TwGKicdrrw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Write behind流程

这种方式下，缓存和数据库的一致性不强，**对一致性要求高的系统要谨慎使用**。但是它适合频繁写的场景，MySQL的**InnoDB Buffer Pool机制**就使用到这种模式。

## **3.操作缓存的时候，到底是删除缓存呢，还是更新缓存？**

日常开发中，我们一般使用的就是**Cache-Aside**模式。有些小伙伴可能会问， **Cache-Aside**在写入请求的时候，为什么是**删除缓存而不是更新缓存**呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRjKRUkFM9hicTx41HbEF3ag8E1w3sTSAPArSrWukm2mgHBHqiczEK29Pg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Cache-Aside写入流程

我们在操作缓存的时候，到底应该删除缓存还是更新缓存呢？我们先来看个例子：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRQ6w0X82sktmFkLHHUK7ib6giczGKYgflkUJC0flDDGsficibpCLU6B1u4g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1. 线程A先发起一个写操作，第一步先更新数据库
2. 线程B再发起一个写操作，第二步更新了数据库
3. 由于网络等原因，线程B先更新了缓存
4. 线程A更新缓存。

   这时候，缓存保存的是A的数据（老数据），数据库保存的是B的数据（新数据），数据**不一致**了，脏数据出现啦。如果是**删除缓存取代更新缓存**则不会出现这个脏数据问题。

**更新缓存相对于删除缓存**，还有两点劣势：

- 如果你写入的缓存值，是经过复杂计算才得到的话。更新缓存频率高的话，就浪费性能啦。
- 在写数据库场景多，读数据场景少的情况下，数据很多时候还没被读取到，又被更新了，这也浪费了性能呢(实际上，写多的场景，用缓存也不是很划算的,哈哈)

## **4.双写的情况下，先操作数据库还是先操作缓存？**

`Cache-Aside`缓存模式中，有些小伙伴还是会有疑问，在写请求过来的时候，为什么是**先操作数据库呢**？为什么**不先操作缓存**呢？

假设有A、B两个请求，请求A做更新操作，请求B做查询读取操作。![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCR5feBdDendzWqLo1AjBdmoPPiaVROOOnvbyia7QxeomOy863JlrlxHTBw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1. 线程A发起一个写操作，第一步del cache
2. 此时线程B发起一个读操作，cache miss
3. 线程B继续读DB，读出来一个老数据
4. 然后线程B把老数据设置入cache
5. 线程A写入DB最新的数据

酱紫就有问题啦，**缓存和数据库的数据不一致了。缓存保存的是老数据，数据库保存的是新数据**。因此，Cache-Aside缓存模式，选择了先操作数据库而不是先操作缓存。

- 个别小伙伴可能会问，先操作数据库再操作缓存，不一样也会导致数据不一致嘛？它俩又不是原子性操作的。这个是**会的**，但是这种方式，一般因为删除缓存失败等原因，才会导致脏数据，这个概率就很低。小伙伴们可以画下操作流程图，自己先分析下哈。接下来我们再来分析这种**删除缓存失败**的情况，**如何保证一致性**。

## **5.数据库和缓存数据保持强一致，可以嘛？**

实际上，没办法做到数据库与缓存**绝对的一致性**。

- 加锁可以嘛？并发写期间加锁，任何读操作不写入缓存？
- 缓存及数据库封装CAS乐观锁，更新缓存时通过lua脚本？
- 分布式事务，3PC？TCC？

其实，这是由**CAP理论**决定的。缓存系统适用的场景就是非强一致性的场景，它属于CAP中的AP。**个人觉得，追求绝对一致性的业务场景，不适合引入缓存**。

> ★
>
> CAP理论，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。
>
> ”

但是，通过一些方案优化处理，是可以**保证弱一致性，最终一致性**的。

## **6. 3种方案保证数据库与缓存的一致性**

### 缓存延时双删

有些小伙伴可能会说，并不一定要先操作数据库呀，采用**缓存延时双删**策略，就可以保证数据的一致性啦。什么是延时双删呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRx1UG6aQZibAnC9yS5UezHXE4H8QXN9ahFyVxbPwiaPRXtm96logZpSAA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)延时双删流程

1. 先删除缓存
2. 再更新数据库
3. 休眠一会（比如1秒），再次删除缓存。

这个休眠一会，一般多久呢？都是1秒？

> ★
>
> 这个休眠时间 =  读业务逻辑数据的耗时 + 几百毫秒。为了确保读请求结束，写请求可以删除读请求可能带来的缓存脏数据。
>
> ”

这种方案还算可以，只有休眠那一会（比如就那1秒），可能有脏数据，一般业务也会接受的。但是如果**第二次删除缓存失败**呢？缓存和数据库的数据还是可能不一致，对吧？给Key设置一个自然的expire过期时间，让它自动过期怎样？那业务要接受**过期时间**内，数据的不一致咯？还是有其他更佳方案呢？

### 删除缓存重试机制

不管是**延时双删**还是**Cache-Aside的先操作数据库再删除缓存**，都可能会存在第二步的删除缓存失败，导致的数据不一致问题。可以使用这个方案优化：删除失败就多删除几次呀,保证删除缓存成功就可以了呀~ 所以可以引入**删除缓存重试机制**

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRDvhO050QGHAEyVgsvtmSBg6OeNsDLt0PEWajMHwzElgKLpqcAfZ1xA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)删除缓存重试流程

1. 写请求更新数据库
2. 缓存因为某些原因，删除失败
3. 把删除失败的key放到消息队列
4. 消费消息队列的消息，获取要删除的key
5. 重试删除缓存操作

### 读取biglog异步删除缓存

重试删除缓存机制还可以吧，就是会造成好多**业务代码入侵**。其实，还可以这样优化：通过数据库的**binlog来异步淘汰key**。

![	](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRM182GqMXbibp67wye0xXvUqrAEQiby4VLrIiaLfia3VRYm6CHXraJuXiaXQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

以mysql为例吧

- 可以使用阿里的canal将binlog日志采集发送到MQ队列里面
- 然后通过ACK机制确认处理这条更新消息，删除缓存，保证数据缓存一致性





# 十、MySQL 分库分表及其平滑扩容方案

众所周知，数据库很容易成为应用系统的瓶颈。单机数据库的资源和处理能力有限，在高并发的分布式系统中，可采用分库分表突破单机局限。本文总结了分库分表的相关概念、全局ID的生成策略、分片策略、平滑扩容方案、以及流行的方案。

### 1 分库分表概述

在业务量不大时，单库单表即可支撑。
当数据量过大存储不下、或者并发量过大负荷不起时，就要考虑分库分表。

#### 1.1 分库分表相关术语

- 读写分离: 不同的数据库，同步相同的数据，分别只负责数据的读和写；
- 分区: 指定分区列表达式，把记录拆分到不同的区域中(必须是同一服务器，可以是不同硬盘)，应用看来还是同一张表，没有变化；
- 分库：一个系统的多张数据表，存储到多个数据库实例中；
- 分表: 对于一张多行(记录)多列(字段)的二维数据表，又分两种情形：
  (1) 垂直分表: 竖向切分，不同分表存储不同的字段，可以把不常用或者大容量、或者不同业务的字段拆分出去；
  (2) 水平分表(最复杂): 横向切分，按照特定分片算法，不同分表存储不同的记录。

#### 1.2 真的要采用分库分表？

需要注意的是，分库分表会为数据库维护和业务逻辑带来一系列复杂性和性能损耗，`除非预估的业务量大到万不得已，切莫过度设计、过早优化`。
规划期内的数据量和性能问题，尝试能否用下列方式解决：

- 当前数据量：如果没有达到几百万，通常无需分库分表；
- 数据量问题：增加磁盘、增加分库(不同的业务功能表，整表拆分至不同的数据库)；
- 性能问题：升级CPU/内存、读写分离、优化数据库系统配置、优化数据表/索引、优化 SQL、分区、数据表的垂直切分；
- 如果仍未能奏效，才考虑最复杂的方案：数据表的水平切分。

### 2 全局ID生成策略

#### 2.1 自动增长列

优点：数据库自带功能，有序，性能佳。
缺点：单库单表无妨，分库分表时如果没有规划，ID可能重复。解决方案：

##### 2.1.1 设置自增偏移和步长

```
### 假设总共有 10 个分表
### 级别可选: SESSION(会话级), GLOBAL(全局)
SET @@SESSION.auto_increment_offset = 1; ### 起始值, 分别取值为 1~10
SET @@SESSION.auto_increment_increment = 10; ### 步长增量
```

如果采用该方案，在扩容时需要迁移已有数据至新的所属分片。

##### 2.1.2 全局ID映射表

在全局 Redis 中为每张数据表创建一个 ID 的键，记录该表当前最大 ID；
每次申请 ID 时，都自增 1 并返回给应用；
Redis 要定期持久至全局数据库。

#### 2.2 UUID(128位)

在一台机器上生成的数字，它保证对在同一时空中的所有机器都是唯一的。通常平台会提供生成UUID的API。
UUID 由4个连字号(-)将32个字节长的字符串分隔后生成的字符串，总共36个字节长。形如：550e8400-e29b-41d4-a716-446655440000。
UUID 的计算因子包括：以太网卡地址、纳秒级时间、芯片ID码和许多可能的数字。
UUID 是个标准，其实现有几种，最常用的是微软的 GUID(Globals Unique Identifiers)。

优点：简单，全球唯一；
缺点：存储和传输空间大，无序，性能欠佳。

#### 2.3 COMB(组合)

参考资料：The Cost of GUIDs as Primary Keys
组合 GUID(10字节) 和时间(6字节)，达到有序的效果，提高索引性能。

#### 2.4 Snowflake(雪花) 算法

参考资料：twitter/snowflake，Snowflake 算法详解
Snowflake 是 Twitter 开源的分布式 ID 生成算法，其结果为 long(64bit) 的数值。
其特性是各节点无需协调、按时间大致有序、且整个集群各节点单不重复。
该数值的默认组成如下(符号位之外的三部分允许个性化调整)：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/OKUeiaP72uRxdGIzLGt01WkaaW1cHIIHR9xxriaHzFwibsibcb2wpCMfXsgxnF6AtbhQMl3l1AmuEWIDuG5ukYBoLg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

- 1bit: 符号位，总是 0(为了保证数值是正数)。
- 41bit: 毫秒数(可用 69 年)；
- 10bit: 节点ID(5bit数据中心 + 5bit节点ID，支持 32 * 32 = 1024 个节点)
- 12bit: 流水号(每个节点每毫秒内支持 4096 个 ID，相当于 409万的 QPS，相同时间内如 ID 遇翻转，则等待至下一毫秒)

### 3 分片策略

#### 3.1 连续分片

根据特定字段(比如用户ID、订单时间)的范围，值在该区间的，划分到特定节点。
优点：集群扩容后，指定新的范围落在新节点即可，无需进行数据迁移。
缺点：如果按时间划分，数据热点分布不均(历史数冷当前数据热)，导致节点负荷不均。

#### 3.3 ID取模分片

缺点：扩容后需要迁移数据。

#### 3.2 一致性Hash算法

优点：扩容后无需迁移数据。

#### 3.4 Snowflake 分片

优点：扩容后无需迁移数据。

### 4 分库分表引入的问题

#### 4.1 分布式事务

参见 分布式事务的解决方案
由于两阶段/三阶段提交对性能损耗大，可改用事务补偿机制。

#### 4.2 跨节点 JOIN

对于单库 JOIN，MySQL 原生就支持；
对于多库，出于性能考虑，不建议使用 MySQL 自带的 JOIN，可以用以下方案避免跨节点 JOIN：

- 全局表: 一些稳定的共用数据表，在各个数据库中都保存一份；
- 字段冗余: 一些常用的共用字段，在各个数据表中都保存一份；
- 应用组装：应用获取数据后再组装。

另外，某个 ID 的用户信息在哪个节点，他的关联数据(比如订单)也在哪个节点，可以避免分布式查询。

#### 4.3 跨节点聚合

只能在应用程序端完成。
但对于分页查询，每次大量聚合后再分页，性能欠佳。

#### 4.4 节点扩容

节点扩容后，新的分片规则导致数据所属分片有变，因而需要迁移数据。

### 5 节点扩容方案

相关资料: 数据库秒级平滑扩容架构方案

#### 5.1 常规方案

如果增加的节点数和扩容操作没有规划，那么绝大部分数据所属的分片都有变化，需要在分片间迁移：

- 预估迁移耗时，发布停服公告；
- 停服(用户无法使用服务)，使用事先准备的迁移脚本，进行数据迁移；
- 修改为新的分片规则；
- 启动服务器。

#### 5.2 免迁移扩容

采用双倍扩容策略，避免数据迁移。扩容前每个节点的数据，有一半要迁移至一个新增节点中，对应关系比较简单。
具体操作如下(假设已有 2 个节点 A/B，要双倍扩容至 A/A2/B/B2 这 4 个节点)：

- 无需停止应用服务器；
- 新增两个数据库 A2/B2 作为从库，设置主从同步关系为：A=>A2、B=>B2，直至主从数据同步完毕(早期数据可手工同步)；
- 调整分片规则并使之生效：
  原 `ID%2=0 => A` 改为 `ID%4=0 => A, ID%4=2 => A2`；
  原 `ID%2=1 => B` 改为 `ID%4=1 => B, ID%4=3 => B2`。
- 解除数据库实例的主从同步关系，并使之生效；
- 此时，四个节点的数据都已完整，只是有冗余(多存了和自己配对的节点的那部分数据)，择机清除即可(过后随时进行，不影响业务)。

### 6 分库分表方案

#### 6.1 代理层方式

部署一台代理服务器伪装成 MySQL 服务器，代理服务器负责与真实 MySQL 节点的对接，应用程序只和代理服务器对接。对应用程序是透明的。
比如 MyCAT，官网，源码，参考文档：MyCAT+MySQL 读写分离部署
MyCAT 后端可以支持 MySQL, SQL Server, Oracle, DB2, PostgreSQL等主流数据库，也支持MongoDB这种新型NoSQL方式的存储，未来还会支持更多类型的存储。
MyCAT 不仅仅可以用作读写分离，以及分表分库、容灾管理，而且可以用于多租户应用开发、云平台基础设施，让你的架构具备很强的适应性和灵活性。

#### 6.2 应用层方式

处于业务层和 JDBC 层中间，是以 JAR 包方式提供给应用调用，对代码有侵入性。主要方案有：
(1)淘宝网的 TDDL: 已于 2012 年关闭了维护通道，建议不要使用。
(2)当当网的 Sharding-JDBC: 仍在活跃维护中：
是当当应用框架 ddframe 中，从关系型数据库模块 dd-rdb 中分离出来的数据库水平分片框架，实现透明化数据库分库分表访问，实现了 Snowflake 分片算法；
Sharding-JDBC定位为轻量Java框架，使用客户端直连数据库，无需额外部署，无其他依赖，DBA也无需改变原有的运维方式。
Sharding-JDBC分片策略灵活，可支持等号、between、in等多维度分片，也可支持多分片键。
SQL解析功能完善，支持聚合、分组、排序、limit、or等查询，并支持Binding Table以及笛卡尔积表查询。

Sharding-JDBC直接封装JDBC API，可以理解为增强版的JDBC驱动，旧代码迁移成本几乎为零：

- 可适用于任何基于Java的ORM框架，如JPA、Hibernate、Mybatis、Spring JDBC Template或直接使用JDBC。
- 可基于任何第三方的数据库连接池，如DBCP、C3P0、 BoneCP、Druid等。
- 理论上可支持任意实现JDBC规范的数据库。虽然目前仅支持MySQL，但已有支持Oracle、SQLServer等数据库的计划。





# 十一、怎么解决MySQL死锁问题的？

```
话不多说，开整！
```

### 什么是死锁

死锁是并发系统中常见的问题，同样也会出现在数据库MySQL的并发读写请求场景中。当两个及以上的事务，双方都在等待对方释放已经持有的锁或因为加锁顺序不一致造成循环等待锁资源，就会出现“死锁”。常见的报错信息为 `Deadlock found when trying to get lock...`。

举例来说 A 事务持有 X1 锁 ，申请 X2 锁，B事务持有 X2 锁，申请 X1 锁。A 和 B 事务持有锁并且申请对方持有的锁进入循环等待，就造成了死锁。

![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicvj4hqx7uiaQTbEMKSGNDVx0pWFwljeSNO6gFvGRvRGMvcpGDTd0WiaOw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如上图，是右侧的四辆汽车资源请求产生了回路现象，即死循环，导致了死锁。

从死锁的定义来看，MySQL 出现死锁的几个要素为：

1. 两个或者两个以上事务
2. 每个事务都已经持有锁并且申请新的锁
3. 锁资源同时只能被同一个事务持有或者不兼容
4. 事务之间因为持有锁和申请锁导致彼此循环等待

### InnoDB 锁类型

为了分析死锁，我们有必要对 InnoDB 的锁类型有一个了解。![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicysIEia6kunTYUf3KQ6bxP2QCPByG7ctyoXl7hhAzicIJcqjxE6iaXiaxiaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

MySQL InnoDB 引擎实现了标准的`行级别锁：共享锁( S lock ) 和排他锁 ( X lock )`

> 1. 不同事务可以同时对同一行记录加 S 锁。
> 2. 如果一个事务对某一行记录加 X 锁，其他事务就不能加 S 锁或者 X 锁，从而导致锁等待。

如果事务 T1 持有行 r 的 S 锁，那么另一个事务 T2 请求 r 的锁时，会做如下处理:

> 1. T2 请求 S 锁立即被允许，结果 T1 T2 都持有 r 行的 S 锁
> 2. T2 请求 X 锁不能被立即允许

如果 T1 持有 r 的 X 锁，那么 T2 请求 r 的 X、S 锁都不能被立即允许，T2 必须等待 T1 释放 X 锁才可以，因为 X 锁与任何的锁都不兼容。共享锁和排他锁的兼容性如下所示：![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicv7A6f0pVWr7gUxckbJVnoliapMCZkudn6s5eHI5t8tY1t5Z2OYH8jEQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### 间隙锁( gap lock )

间隙锁锁住一个间隙以防止插入。假设索引列有2, 4, 8 三个值，如果对 4 加锁，那么也会同时对(2,4)和(4,8)这两个间隙加锁。其他事务无法插入索引值在这两个间隙之间的记录。但是，间隙锁有个例外:

> 1. 如果索引列是唯一索引，那么只会锁住这条记录(只加行锁)，而不会锁住间隙。
> 2. 对于联合索引且是唯一索引，如果 where 条件只包括联合索引的一部分，那么依然会加间隙锁。

#### next-key lock

next-key lock 实际上就是 行锁+这条记录前面的 gap lock 的组合。假设有索引值10,11,13和 20,那么可能的 next-key lock 包括:

> (负无穷,10],(10,11],(11,13],(13,20],(20,正无穷)

在 RR 隔离级别下，InnoDB 使用 next-key lock 主要是防止`幻读`问题产生。

#### 意向锁( Intention lock )

InnoDB 为了支持多粒度的加锁，允许行锁和表锁同时存在。为了支持在不同粒度上的加锁操作，InnoDB 支持了额外的一种锁方式，称之为意向锁( Intention Lock )。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度上进行加锁。意向锁分为两种:

> 1. 意向共享锁( IS )：事务有意向对表中的某些行加共享锁
> 2. 意向排他锁( IX )：事务有意向对表中的某些行加排他锁

由于 InnoDB 存储引擎支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫描以外的任何请求。表级意向锁与行级锁的兼容性如下所示:![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicrCcP1FwRFjey2TCeicMuZp2qLF90bibGD9el0lhIW6yPhjicMibibDlKk7A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### 插入意向锁( Insert Intention lock )

插入意向锁是在插入一行记录操作之前设置的一种间隙锁，这个锁释放了一种插入方式的信号，即多个事务在相同的索引间隙插入时如果不是插入间隙中相同的位置就不需要互相等待。假设某列有索引值2，6，只要两个事务插入位置不同(如事务 A 插入3，事务 B 插入4)，那么就可以同时插入。

#### 锁模式兼容矩阵

横向是已持有锁，纵向是正在请求的锁：![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicpCcfaPq9s5d7uF4srTfQq1eWaka2RWVXDEblK2vicObfdCiagvaOicgKA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 阅读死锁日志

在进行具体案例分析之前，咱们先了解下如何去读懂死锁日志，尽可能地使用死锁日志里面的信息来帮助我们来解决死锁问题。

后面测试用例的数据库场景如下:`MySQL 5.7 事务隔离级别为 RR`

表结构和数据如下:![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicrYtObLuGncGtTOMW6G1TF0twtRc0PdtImtwomLSfKtAuI96icQX2cHA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

测试用例如下:![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicER5sibmYUK4blLnmyWCLFEvJgtaVOPMtTs4tjOXLRz5qOfBOoY6xlhg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

通过执行show engine innodb status 可以查看到最近一次死锁的日志。

#### 日志分析如下:

1. ***** (1) TRANSACTION: TRANSACTION 2322, ACTIVE 6 sec starting index read

事务号为2322，活跃 6秒，starting index read 表示事务状态为根据索引读取数据。常见的其他状态有:![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicibZ6TX3se2a5KWa78YzAICxKeocwdFyAkGEGRY5gIbCsk4pHxTqrRicQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

`mysql tables in use 1` 说明当前的事务使用一个表。

`locked 1` 表示表上有一个表锁，对于 DML 语句为 LOCK_IX

```
LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s)
```

`LOCK WAIT` 表示正在等待锁，`2 lock struct(s)` 表示 trx->trx_locks 锁链表的长度为2，每个链表节点代表该事务持有的一个锁结构，包括表锁，记录锁以及自增锁等。本用例中 2locks 表示 IX 锁和lock_mode X (Next-key lock)

`1 row lock(s)` 表示当前事务持有的行记录锁/ gap 锁的个数。

```
MySQL thread id 37, OS thread handle 140445500716800, query id 1234 127.0.0.1 root updating
```

`MySQL thread id 37` 表示执行该事务的线程 ID 为 37 (即 show processlist; 展示的 ID )

`delete from student where stuno=5` 表示事务1正在执行的 sql，比较难受的事情是 `show engine innodb status` 是查看不到完整的 sql 的，通常显示当前正在等待锁的 sql。

***** (1) WAITING FOR THIS LOCK TO BE GRANTED:

RECORD LOCKS space id 11 page no 5 n bits 72 index idx_stuno of table cw****.****student trx id 2322 lock_mode X waiting

RECORD LOCKS 表示记录锁， 此条内容表示事务 1 正在等待表 student 上的 idx_stuno 的 X 锁，本案例中其实是 Next-Key Lock 。

事务2的 log 和上面分析类似:

1. ***** (2) HOLDS THE LOCK(S):

```
RECORD LOCKS space id 11 page no 5 n bits 72 index idx_stuno of table cw****.****student trx id 2321 lock_mode X
```

显示事务 2 的 insert into student(stuno,score) values(2,10) 持有了 a=5 的 Lock mode X

| LOCK_gap，不过我们从日志里面看不到事务2执行的 delete from student where stuno=5;

这点也是造成 DBA 仅仅根据日志难以分析死锁的问题的根本原因。

1. ***** (2) WAITING FOR THIS LOCK TO BE GRANTED:

```
RECORD LOCKS space id 11 page no 5 n bits 72 index idx_stuno of table cw****.****student trx id 2321 lock_mode X locks gap before rec insert intention waiting
```

表示事务 2 的 insert 语句正在等待插入意向锁 lock_mode X locks gap before rec insert intention waiting ( LOCK_X + LOCK_REC_gap )

### 经典案例分析

#### 案例一:事务并发 insert 唯一键冲突

表结构和数据如下所示:![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicw8EVmJ8BG8Khy2jYibfM5MwZMYpE5OXFlsQ9JD0rJveYJLTB8hT9Taw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicKAW3OPoFVWYoqnlzY7Kkwx6Tuib3P5jh88V1IZWMJ3nwjoU3kZx1bCA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)测试用例如下:![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibich9nJAgXMicgAwhLiactz1dhbM8knHhptCQdAqNFia0LK19c6mGGpw7ic4w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)日志分析如下:

1. 事务 T2 insert into t7(id,a) values (26,10) 语句 insert 成功，持有 a=10 的 `排他行锁( Xlocks rec but no gap )`
2. 事务 T1 insert into t7(id,a) values (30,10), 因为T2的第一条 insert 已经插入 a=10 的记录,事务 T1 insert a=10 则发生唯一键冲突,需要申请对冲突的唯一索引加上S Next-key Lock( 即 lock mode S waiting ) 这是一个`间隙锁`会申请锁住(,10],(10,20]之间的 gap 区域。
3. 事务 T2 insert into t7(id,a) values (40，9)该语句插入的 a=9 的值在事务 T1 申请的 `gap 锁4-10之间`， 故需事务 T2 的第二条 insert 语句要等待事务 T1 的 `S-Next-key Lock 锁`释放,在日志中显示 lock_mode X locks gap before rec insert intention waiting 。

#### 案例一:先 update 再 insert 的并发死锁问题

表结构如下，无数据:![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicmQsgsOgYeUx4AC9hlfcXzRjFITIQ07yd3c1mbnO6f8pPHUSaDKFrKQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)测试用例如下:![图片](https://mmbiz.qpic.cn/mmbiz_png/7VkkuTzAZPp08uy3ZghjeKeEiae1ooyibicVWZujhIYpkiaFzorMic6jWsVYm2lENaIwAvQKGEIicGibELibRpuTRRcJoA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)死锁分析:
可以看到两个事务 update 不存在的记录，先后获得`间隙锁( gap 锁)`，gap 锁之间是兼容的所以在update环节不会阻塞。两者都持有 gap 锁，然后去竞争插入`意向锁`。当存在其他会话持有 gap 锁的时候，当前会话申请不了插入意向锁，导致死锁。

### 如何尽可能避免死锁

1. 合理的设计索引，区分度高的列放到组合索引前面，使业务 SQL 尽可能通过索引`定位更少的行，减少锁竞争`。
2. 调整业务逻辑 SQL 执行顺序， 避免 update/delete 长时间持有锁的 SQL 在事务前面。
3. 避免`大事务`，尽量将大事务拆成多个小事务来处理，小事务发生锁冲突的几率也更小。
4. 以`固定的顺序`访问表和行。比如两个更新数据的事务，事务 A 更新数据的顺序为 1，2;事务 B 更新数据的顺序为 2，1。这样更可能会造成死锁。
5. 在并发比较高的系统中，不要显式加锁，特别是是在事务里显式加锁。如 select … for update 语句，如果是在事务里`（运行了 start transaction 或设置了autocommit 等于0）`,那么就会锁定所查找到的记录。
6. 尽量按`主键/索引`去查找记录，范围查找增加了锁冲突的可能性，也不要利用数据库做一些额外额度计算工作。比如有的程序会用到 “select … where … order by rand();”这样的语句，由于类似这样的语句用不到索引，因此将导致整个表的数据都被锁住。
7. 优化 SQL 和表设计，减少同时占用太多资源的情况。比如说，`减少连接的表`，将复杂 SQL `分解`为多个简单的 SQL。

![MySQL锁机制](https://pic1.zhimg.com/v2-164ea9bd88bc4ccbc43f3003f48d1d14_1440w.jpg?source=172ae18b)



## **十二、MySQL支持的锁**

```
从锁粒度上划分
```

![img](https://pic2.zhimg.com/80/v2-43c2c24556001358ceae6e23fd292b91_1440w.jpg)

```text
表级锁
行级锁(InnoDB存储引擎)
页级锁(BDB存储引擎)
从锁操作上划分
```

![img](https://pic2.zhimg.com/80/v2-a1d6caa2cda1d30dc99610e37c0ba811_1440w.jpg)

```
从实现方式上划分
```

![img](https://pic4.zhimg.com/80/v2-8368c6d1d1a91455a528533e273fccdf_1440w.jpg)

## **使用场景**

```
修改表结构
```

![img](https://pic4.zhimg.com/80/v2-4bb9deab199663259a4924abd1389603_1440w.jpg)

```text
修改数据库表结构会自动加表级锁（元数据锁）
行级锁升级表级锁
更新数据未使用索引
行级锁会上升为表级锁
```

![img](https://pic1.zhimg.com/80/v2-f0925081ff5149711235a245903f0420_1440w.jpg)

```
更新数据使用索引会使用行级锁
```

![img](https://pic1.zhimg.com/80/v2-b2a9bb878b603dbe494955e171829050_1440w.jpg)

```
select .... from update使用行级锁
```

![img](https://pic1.zhimg.com/80/v2-36b768108a94de2a260bf2d4b26c00b8_1440w.jpg)

## **MySQL锁分类**

![img](https://pic2.zhimg.com/80/v2-a0bd9da8ca3995b265b76617dc69a68d_1440w.jpg)

```text
分为乐观锁和悲观锁
```

### **乐观锁**

![img](https://pic1.zhimg.com/80/v2-120c4eb55d92168d9f9426f4c078397c_1440w.jpg)

```text
乐观锁是程序通过版本号或时间戳实现
```

### **悲观锁**

![img](https://pic2.zhimg.com/80/v2-3f6f4f1c3344707a8ffa47c23ed990a5_1440w.jpg)

### **表级锁**

```text
每次操作锁住整个表
锁定力度大
发生锁冲突的概率最高
并发度最低
应用在MyISAM、InnoDB、BDB等存储引擎中
```

![img](https://pic3.zhimg.com/80/v2-eab1242dac2b0668d81a16328ef62732_1440w.jpg)

```text
表级锁又分为表锁（MySQL layer层加锁）
元数据锁（MySQL layer层加锁）
意向锁(InnodB存储引擎层加锁) 内部使用的锁
```

### **表锁**

```text
需要手动加锁
```

![img](https://pic1.zhimg.com/80/v2-0d85d4d36413d328611626effe1bf0bc_1440w.jpg)

- read lock

```text
加读锁后还可以加读锁
不能加写锁
```

- write lock

```text
加写锁后不能加读锁也不能加写锁
```

### **元数据锁**

```text
自动加锁
元数据其实就是表结构
```

![img](https://pic3.zhimg.com/80/v2-2d34c6df0ae2a9c93d458fc524c328fa_1440w.jpg)

### **意向锁**

![img](https://pic3.zhimg.com/80/v2-a5dafc91f70831ea095ca9627f3374b2_1440w.jpg)

### **行级锁**

```text
每次操作锁住一行数据
锁定粒度最小
发生锁冲突的概率最低
并发读最高
是由InnoDB存储引擎实现的
```

![img](https://pic4.zhimg.com/80/v2-b8383ff2ea1637e8bbacfd370357809f_1440w.jpg)

### **共享读锁(S)**

```text
手动加锁
select .... lock in share mode
允许一个事务去读一行 
阻止其他事务获取相同数据集的排他锁
```

### **排他写锁(X)**

```text
自动加锁
允许获得排他写锁的事务更新数据
阻止其他事务取得相同数据集的共享读锁（不是普通读）
和排他写锁
```

- DML(insert、update、delete)
- select ... from udpate

```
整体分类
```

![img](https://pic3.zhimg.com/80/v2-8507d0ecc531c9f1e4b176cdde60cebe_1440w.jpg)

## **表级锁使用**

```
表读锁
```

![img](https://pic1.zhimg.com/80/v2-84f8e8e72eec098ee0c2dd0164bc2998_1440w.jpg)

```text
事务1给mylock表添加读锁
事务1查询mylock表可以读取到数据
事务1不可以查询其他表数据

事务2普通查询mylock表 
没有加锁 
可以查到 
通过MVCC机制查询数据的历史版本

事务2更新mylock表id为2的数据 
需要先获取这条数据的行锁
才可以进行修改
但此时是获取不到的
因为事务1还未释放mylock表的读锁
所以事务2只能等待事务1释放mylock表的读锁
才能够获取到行锁

事务1释放了mylock表的读锁
那么则可以查询其他表的数据了

事务2也获取到id为2的这条数据的行锁
执行更新操作
表写锁
```

![img](https://pic2.zhimg.com/80/v2-836e514f05ae68f4dad13e0e8d52cab5_1440w.jpg)

```text
事务1给mylock添加表写锁
事务1可以查询mylock数据
事务1不可以查询其他表数据
事务1更新mylock表id为2的数据
可以执行

事务2查询mylock数据
查询阻塞 
因为事务1还未释放mylock表的写锁

事务1释放mylock表的写锁

事务2的查询得以继续执行
事务1页可以访问其他表数据了
```

## **元数据锁的使用**

```
元数据读锁
```

![img](https://pic2.zhimg.com/80/v2-b396971cab390875c6f2907d6aaa72f5_1440w.jpg)

```text
事务1开启事务
事务1查询mylock表
此时会加一个MDL读锁

事务2修改mylock表结构
此时会被阻塞

事务1提交事务或回滚事务
事务2才得以修改完成
```

## **行级锁分类及使用**

![img](https://pic4.zhimg.com/80/v2-0c793982c08f9667a024cc228ad28247_1440w.jpg)

```
查询行级锁状态
show status like 'innodb_row_locks'
行级锁的使用
```

![img](https://pic2.zhimg.com/80/v2-fb1c6830eee3307e71999a6e1331dfe1_1440w.jpg)

```text
事务1 开始事务
事务1 查询mylock表id为1的数据
id列为索引列
加行读锁

事务2更新id为2的数据 
因未锁定该行 所以可以更新

事务2更新id为1的数据
该行的行读锁还未释放
此时修改被阻塞

事务1提交事务
事务2对id为1这条数据的更新才得以执行

"注"
使用索引加行锁
未锁定的行可以访问
行读锁升级为表锁
```

**未使用索引的行级锁会升级为表锁**

![img](https://pic2.zhimg.com/80/v2-f0b73dbc9e8c0b2c01e3a1bfc47d610d_1440w.jpg)

```text
事务1开始事务
事务1查询mylock表 查询条件是name='c'
并手动给name='c'添加行读锁
但name列并未使用索引
所以行读锁就会升级为表级锁

事务2更新mylock表id为2的数据
就会被阻塞

事务1提交或回滚事务 表级锁就会被释放
事务2的更新操作就可以获取到行级锁 
然后执行更新操作
行写锁
```

**主键索引产生记录锁**

![img](https://pic1.zhimg.com/80/v2-b77f30ae46dca114bc07d9a8de3175ec_1440w.jpg)

```text
事务1开始事务
事务1查询mylock表id为1的数据 
并添加行写锁

事务2查询mylock表id为2的数据 
可以访问

事务2查询mylock表id为1的数据
可以访问 不加锁

事务2 查询mylock表id为1的数据 添加读锁
此时被阻塞

事务1提交 释放了id为1的行写锁
事务2 加读锁获取id为1的数据得以执行
```

## **行锁原理**

```
主键加锁
id为主键索引
update t set name='x' where id =10;
加锁行为仅在id=10的主键索引记录上加X锁(记录锁)
```

![img](https://pic2.zhimg.com/80/v2-c5f5fa1c6d18b7c6afe139fa7ab7b7cd_1440w.jpg)

```
唯一键加锁
```

![img](https://pic4.zhimg.com/80/v2-95f1c8f48fd7d578f54f2f3450ff0297_1440w.jpg)

![img](https://pic2.zhimg.com/80/v2-a2b973ad17f63504d9ee72337c0a5ee1_1440w.jpg)

```text
id为唯一键索引
name为主键索引

先在唯一索引id上加id=10的x锁
再在id=10的主键索引记录上加x锁
非唯一键加锁
name是主键
id是索引（二级索引）

加锁行为：对满足id=10条件的记录和主键分别加x锁
然后在
(6,c)~(10,b)
(10,b)~(10,d)
（10,d)~(11,f)
间隙分别加Gap锁
```

![img](https://pic1.zhimg.com/80/v2-af99c355bf1e6717c34fb47cc75aafc4_1440w.jpg)

![img](https://pic1.zhimg.com/80/v2-957cd9815e7fccb1eab4d7d1a7227e5c_1440w.jpg)

![img](https://pic1.zhimg.com/80/v2-7d21b9a1ca31e0cd264055ad668fdee8_1440w.jpg)

```text
有了间隙锁 
这些区间内不允许其他事务做插入操作
无索引加锁
name是逐渐
id没有索引
select * from t where id =10; 
会导致全表扫描

加锁行为：表里所有行和间隙均加x锁
由于InnoDB引擎行锁机制基于索引实现记录锁定
因为没有索引时会导致全表锁定
```

![img](https://pic4.zhimg.com/80/v2-a84ea49e6c96f7875ff503f9861232af_1440w.jpg)

## **死锁**

```
死锁现象
```

- 表锁死锁
- 行级锁死锁
- 共享锁转换为排他锁

### **表级锁死锁**

![img](https://pic2.zhimg.com/80/v2-77b41a974c2b4bce195eebc5185879d1_1440w.jpg)

![img](https://pic1.zhimg.com/80/v2-3bc858ac263cc581b8967c758842eb4c_1440w.jpg)

```text
用户A先访问表A 对表A加了锁
然后再访问表B
用户B先访问表B 对表B加了锁
然后再访问表A
因表B被加了锁
所以用户A需等着用户B释放了表B的锁才可以对表B加锁
因表A被加了锁
所以用户B需等着用户A释放了表A的锁才可以对表A加锁
所以2者互相等待 从而死锁
解决方案
```

- 调整程序的逻辑

```text
把表A和表B当成同一个资源
```

![img](https://pic4.zhimg.com/80/v2-1e7e222a76819a9b2aa3399749bc69df_1440w.jpg)

```text
用户A访问完表A和表B之后
用户B再来访问表A和表B
```

- 尽量避免同时锁定2个资源

### **行级锁死锁**

```
产生原因1
在事务中执行了一条不满足for update的操作
则执行全表扫描
把行级锁上升为表级锁
多个这样的事务执行后
就很容易产生死锁和阻塞
```

![img](https://pic2.zhimg.com/80/v2-1abeb89a583408ae66ebfaaa53e78d7d_1440w.jpg)

```
解决方案
SQL语句中不要使用太复杂的关联表的查询 
优化索引
产生原因2
```

![img](https://pic4.zhimg.com/80/v2-3ea3b1eb794f5ad2a88a16a7cf075bc7_1440w.jpg)

```text
表中的2个数据id1和id2
事务1先访问id1 对id1加行锁
再访问id2
事务2先访问id2 对id2加行锁
再访问id1
事务1访问id2等待事务2释放id2的行锁
事务2访问id1等待事务1释放id1的行锁
所以事务1和事务2互相等待 阻塞
从而产生死锁

类似于2个表死锁
这个是表中的2个记录行级死锁
解决方案
```

- 同一个事务中 尽可能做到一次性锁定所有资源
- 按照id对资源排序 然后按顺序进行处理
- 采用MVCC机制处理 普通读 不会使用锁

### **共享锁转排他锁**

![img](https://pic2.zhimg.com/80/v2-54e0241d7aab711035d5080d56672b01_1440w.jpg)

```text
事务A查询一条记录 加共享读锁
事务A更新这条记录
事务B也更新这条记录 需要排他锁
事务B需等待事务A释放了共享锁
才可以获得排他锁进行更新
所以事务B进入了排队等待
事务A也需要排他锁进行更新操作
但是无法授予该 锁请求
因为事务B已经有了一个排他锁请求
并且等待事务A释放其共享锁
解决方案
```



- 避免引发对同一条记录的反复操作
- 使用乐观锁进行控制