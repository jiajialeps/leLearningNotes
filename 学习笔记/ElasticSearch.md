# Elasticsearch

### 1、elasticsearch了解多少，说说你们公司es的集群架构，索引数据大小，分片有多少，以及一些调优手段 。

`面试官`：想了解应聘者之前公司接触的ES使用场景、规模，有没有做过比较大规模的索引设计、规划、调优。 `解答`： 如实结合自己的实践场景回答即可。 比如：ES集群架构13个节点，索引根据通道不同共20+索引，根据日期，每日递增20+，索引：10分片，每日递增1亿+数据， 每个通道每天索引大小控制：150GB之内。

仅索引层面调优手段：

#### **1.1、设计阶段调优**

- 1）根据业务增量需求，采取基于日期模板创建索引，通过roll over API滚动索引； 
- 2）使用别名进行索引管理；
- 3）每天凌晨定时对索引做force_merge操作，以释放空间；
- 4）采取冷热分离机制，热数据存储到SSD，提高检索效率；冷数据定期进行shrink操作，以缩减存储；
- 5）采取curator进行索引的生命周期管理；
- 6）仅针对需要分词的字段，合理的设置分词器；
- 7）Mapping阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。 ……..

#### **1.2、写入调优**

- 1）写入前副本数设置为0； 
- 2）写入前关闭refresh_interval设置为-1，禁用刷新机制；
- 3）写入过程中：采取bulk批量写入；
- 4）写入后恢复副本数和刷新间隔； 
- 5）尽量使用自动生成的id。

#### **1.3、查询调优**

- 1）禁用wildcard；
- 2）禁用批量terms（成百上千的场景）；
- 3）充分利用倒排索引机制，能keyword类型尽量keyword；
- 4）数据量大时候，可以先基于时间敲定索引再检索；
- 5）设置合理的路由机制。 

#### **1.4、其他调优**

部署调优，业务调优等。

上面的提及一部分，面试者就基本对你之前的实践或者[运维](https://cloud.tencent.com/solution/operation?from=10680)经验有所评估了。

### **2、elasticsearch的倒排索引是什么**

`面试官`：想了解你对基础概念的认知。 `解答`：通俗解释一下就可以。

传统的我们的检索是通过文章，逐个遍历找到对应关键词的位置。 而倒排索引，是通过分词策略，形成了词和文章的映射关系表，这种词典+映射表即为倒排索引。 有了倒排索引，就能实现`o（1）时间复杂度`的效率检索文章了，极大的提高了检索效率。

![img](https://pic4.zhimg.com/80/v2-43542fcc0daf345b92c5a674c4197e8b_720w.jpg)





学术的解答方式：

> 倒排索引，相反于一篇文章包含了哪些词，它从词出发，记载了这个词在哪些文档中出现过，由两部分组成——词典和倒排表。

​	`加分项`：倒排索引的底层实现是基于：FST（Finite State Transducer）数据结构。 lucene从4+版本后开始大量使用的数据结构是FST。FST有两个优点：

- 1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；
- 2）查询速度快。O(len(str))的查询时间复杂度。

[字典数据结构-FST(Finite State Transducers)_Javap 指令集-CSDN博客_finite state transducers](https://blog.csdn.net/hczjb/article/details/87915207)

> FST最终存储为一个有向无环图的树状结构

![img](https://images0.cnblogs.com/blog/522490/201411/242226566064202.png)

最终我们得到了如上一个有向无环图。利用该结构可以很方便的进行查询，如给定一个term “dog”，我们可以通过上述结构很方便的查询存不存在，甚至我们在构建过程中可以将单词与某一数字、单词进行关联，从而实现key-value的映射。



### **3、elasticsearch 索引数据多了怎么办，如何调优，部署**

`面试官`：想了解[大数据](https://cloud.tencent.com/solution/bigdata?from=10680)量的运维能力。 `解答`：索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。 如何调优，正如问题1所说，这里细化一下：

#### **3.1 动态索引层面**

基于`模板+时间+rollover api滚动`创建索引，举例：设计阶段定义：blog索引的模板格式为：blog_index_时间戳的形式，每天递增数据。

这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线2的32次幂-1，索引存储达到了TB+甚至更大。

一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。

#### **3.2 存储层面**

`冷热数据分离存储`，热数据（比如最近3天或者一周的数据），其余为冷数据。 对于冷数据不会再写入新数据，可以考虑定期force_merge加shrink压缩操作，节省存储空间和检索效率。

#### **3.3 部署层面**

一旦之前没有规划，这里就属于应急策略。 结合ES自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意：如果之前主节点等`规划合理`，不需要重启集群也能完成动态新增的。

### **4、elasticsearch是如何实现master选举的**

`面试官`：想了解ES集群的底层原理，不再只关注业务层面了。 `解答`： 前置前提：

- 1）只有候选主节点（master：true）的节点才能成为主节点。 
- 2）最小主节点数（min_master_nodes）的目的是防止脑裂。

这个我看了各种网上分析的版本和源码分析的书籍，云里雾里。 核对了一下代码，核心入口为findMaster，选择主节点成功返回对应Master，否则返回null。选举流程大致描述如下：

- 第一步：确认候选主节点数达标，elasticsearch.yml设置的值discovery.zen.minimum_master_nodes；
- 第二步：比较：先判定是否具备master资格，具备候选主节点资格的优先返回；若两节点都为候选主节点，则id小的值会主节点。注意这里的id为string类型。

题外话：获取节点id的方法。

```javascript
1GET /_cat/nodes?v&h=ip,port,heapPercent,heapMax,id,name
2ip        port heapPercent heapMax id   name
3127.0.0.1 9300          39   1.9gb Hk9w Hk9wFwU
```

### **5、详细描述一下Elasticsearch索引文档的过程**

`面试官`：想了解ES的底层原理，不再只关注业务层面了。 `解答`： 这里的索引文档应该理解为文档写入ES，创建索引的过程。 文档写入包含：单文档写入和批量bulk写入，这里只解释一下：单文档写入流程。

记住官方文档中的这个图。

![img](https://pic3.zhimg.com/80/v2-8b8eccec501800436783e3bfe2c8ad86_720w.jpg)

第一步：客户写集群某节点写入数据，发送请求。（如果没有指定路由/协调节点，请求的节点扮演`路由节点`的角色。）

第二步：节点1接受到请求后，使用文档_id来确定文档属于分片0。请求会被转到另外的节点，假定节点3。因此分片0的主分片分配到节点3上。

第三步：节点3在主分片上执行写操作，如果成功，则将请求并行转发到节点1和节点2的副本分片上，等待结果返回。所有的副本分片都报告成功，节点3将向协调节点（节点1）报告成功，节点1向请求客户端报告写入成功。

如果面试官再问：第二步中的文档获取分片的过程？ 回答：借助路由算法获取，路由算法就是根据路由和文档id计算目标的分片id的过程。

```javascript
1shard = hash(_routing) % (num_of_primary_shards)
```

### **6、详细描述一下Elasticsearch搜索的过程？**

`面试官`：想了解ES搜索的底层原理，不再只关注业务层面了。 `解答`： 搜索拆解为“query then fetch” 两个阶段。 **query阶段的目的**：定位到位置，但不取。 步骤拆解如下：

- 1）假设一个索引数据有5主+1副本 共10分片，一次请求会命中（主或者副本分片中）的一个。
- 2）每个分片在本地进行查询，结果返回到本地有序的优先队列中。 
- 3）第2）步骤的结果发送到协调节点，协调节点产生一个全局的排序列表。

**fetch阶段的目的**：取数据。 路由节点获取所有文档，返回给客户端。

### **7、Elasticsearch在部署时，对Linux的设置有哪些优化方法**

`面试官`：想了解对ES集群的运维能力。 `解答`：

- 1）关闭缓存swap; 
- 2）堆内存设置为：Min（节点内存/2, 32GB）; 
- 3)设置最大文件句柄数；
- 4）线程池+队列大小根据业务需要做调整； 
- 5）磁盘存储raid方式——存储有条件使用RAID10，增加单节点性能以及避免单节点存储故障。

### **8、lucence内部结构是什么？**

`面试官`：想了解你的知识面的广度和深度。 `解答`：

![img](https://pic3.zhimg.com/80/v2-d4c024788023660e7a19c8981b5ce2fa_720w.jpg)

Lucene是有索引和搜索的两个过程，包含索引创建，索引，搜索三个要点。可以基于这个脉络展开一些。



### **9、Elasticsearch 是如何实现 Master 选举的？**

1、Elasticsearch 的选主是 ZenDiscovery 模块负责的，主要包含 Ping（节点之间通过这个 RPC 来发现彼此）和 Unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分；

2、对所有可以成为 master 的节点（**node.master: true**）根据 nodeId 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第 0 位）节点，暂且认为它是 master 节点。

3、如果对某个节点的投票数达到一定的值（可以成为 master 节点数 n/2+1）并且该节点自己也选举自己，那这个节点就是 master。否则重新选举一直到满足上述条件。

4、补充：master 节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data 节点可以关闭 http 功能*。



### **10、Elasticsearch 中的节点（比如共 20 个），其中的 10 个**

**选了一个 master，另外 10 个选了另一个 master，怎么办？**

1、当集群 master 候选数量不小于 3 个时，可以通过设置最少投票通过数量（**discovery.zen.minimum_master_nodes**）超过所有候选节点一半以上来解决脑裂问题；



2、当候选数量为两个时，只能修改为唯一的一个 master 候选，其他作为 data节点，避免脑裂问题。



**ES脑裂问题分析及优化**：https://blog.csdn.net/kakaluoteyy/article/details/81068387







### **11、客户端在和集群连接时，如何选择特定的节点执行请求的？**

1、TransportClient 利用 transport 模块远程连接一个 elasticsearch 集群。它并不加入到集群中，只是简单的获得一个或者多个初始化的 transport 地址，并以 **轮询** 的方式与这些地址进行通信。



### **12、详细描述一下 Elasticsearch 索引文档的过程。**

协调节点默认使用文档 ID 参与计算（也支持通过 routing），以便为路由提供合适的分片。shard = hash(document_id) % (num_of_primary_shards)

1、当分片所在的节点接收到来自协调节点的请求后，会将请求写入到 MemoryBuffer，然后定时（默认是每隔 1 秒）写入到 Filesystem Cache，这个从 MomeryBuffer 到 Filesystem Cache 的过程就叫做 refresh；

2、当然在某些情况下，存在 Momery Buffer 和 Filesystem Cache 的数据可能会丢失，ES 是通过 translog 的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到 translog 中，当 Filesystem cache 中的数据写入到磁盘中时，才会清除掉，这个过程叫做 flush；

3、在 flush 过程中，内存中的缓冲将被清除，内容被写入一个新段，段的 fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的 translog 将被删除并开始一个新的 translog。

4、flush 触发的时机是定时触发（默认 30 分钟）或者 translog 变得太大（默认为 512M）时；



![img](https://pic2.zhimg.com/80/v2-98fc4f1cd2b3c13e56dead5850e9db95_720w.jpg)



**补充：关于 Lucene 的 Segement：**

1、Lucene 索引是由多个段组成，段本身是一个功能齐全的倒排索引。

2、段是不可变的，允许 Lucene 将新的文档增量地添加到索引中，而不用从头重建索引。

3、对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU 的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。

4、为了解决这个问题，Elasticsearch 会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。



### **13、详细描述一下 Elasticsearch 更新和删除文档的过程。**

1、删除和更新也都是写操作，但是 Elasticsearch 中的文档是不可变的，因此不能被删除或者改动以展示其变更；

2、磁盘上的每个段都有一个相应的.del 文件。当删除请求发送后，文档并没有真的被删除，而是在.del 文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del 文件中被标记为删除的文档将不会被写入新段。

3、在新的文档被创建时，Elasticsearch 会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del 文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。



### **14、详细描述一下 Elasticsearch 搜索的过程。**



1、搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch；

2、在初始**查询阶段**时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。

PS：在搜索的时候是会查询 Filesystem Cache 的，但是有部分数据还在 MemoryBuffer，所以搜索是近实时的。

3、每个分片返回各自优先队列中 **所有文档的 ID 和排序值** 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。

4、接下来就是 **取回阶段**，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。

5、补充：Query Then Fetch 的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch 增加了一个预查询的处理，询问 Term 和 Document frequency，这个评分更准确，但是性能会变差。*



![img](https://pic2.zhimg.com/80/v2-aac5b983cb1aa9ec2c81e6624292e469_720w.jpg)







### **15、在 Elasticsearch 中，是怎么根据一个词找到对应的倒排索引的？**



![img](https://pic3.zhimg.com/80/v2-5910c28f5bd7e1acb10544c887524596_720w.jpg)



### **16、Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法？**

1、64 GB内存的机器是非常理想的， 但是 32 GB 和 16 GB 机器也是很常见的。少于 8 GB 会适得其反。

2、如果你要在更快的 CPUs 和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。

3、如果你负担得起 SSD，它将远远超出任何旋转介质。 基于 SSD 的节点，查询和索引性能都有提升。如果你负担得起，SSD 是一个好的选择。

4、即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。

5、请确保运行你应用程序的 JVM 和服务器的 JVM 是完全一样的。 在Elasticsearch 的几个地方，使用 Java 的本地序列化。

6、通过设置 gateway.recover_after_nodes、gateway.expected_nodesgateway.recover_after_time 可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。

7、Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。

8、不要随意修改垃圾回收器（CMS）和各个线程池的大小。

9、把你的内存的（少于）一半给 Lucene（但不要超过 32 GB！），通过ES_HEAP_SIZE 环境变量设置。

10、内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个100 微秒的操作可能变成 10 毫秒。 再想想那么多 10 微秒的操作时延累加起来。 不难看出 swapping 对于性能是多么可怕。

11、Lucene 使用了大量 的文件。同时，Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字。 所有这一切都需要足够的文件描述符。你应该增加你的文件描述符，设置一个很大的值，如 64,000。



**补充：索引阶段性能提升方法**

1、使用批量请求并调整其大小：每次批量数据 5–15 MB 大是个不错的起始点。

2、存储：使用 SSD

3、段和合并：Elasticsearch 默认值是 20 MB/s，对机械磁盘应该是个不错的设置。如果你用的是 SSD，可以考虑提高到 100–200 MB/s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。另外还可以增加

index.translog.flush_threshold_size 设置，从默认的 512 MB 到更大一些的值，比如 1 GB，这可以在一次清空触发的时候在事务日志里积累出更大的段。

4、如果你的搜索结果不需要近实时的准确度，考虑把每个索引的index.refresh_interval 改到 30s。

5、如果你在做大批量导入，考虑通过设置 index.number_of_replicas: 0 关闭副本。



### **17、对于 GC 方面，在使用 Elasticsearch 时要注意什么？**

1、SEE：[https://elasticsearch.cn/article/32](https://link.zhihu.com/?target=https%3A//elasticsearch.cn/article/32)

2、倒排词典的索引需要常驻内存，无法 GC，需要监控 data node 上 segment

memory 增长趋势。

3、各类缓存，field cache, filter cache, indexing cache, bulk queue 等等，要设置合理的大小，并且要应该根据最坏的情况来看 heap 是否够用，也就是各类缓存全部占满的时候，还有 heap 空间可以分配给其他任务吗？避免采用 clear cache等“自欺欺人”的方式来释放内存。

4、避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用scan & scroll api 来实现。

5、cluster stats 驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过 tribe node 连接。

6、想知道 heap 够不够，必须结合实际应用场景，并对集群的 heap 使用情况做持续的监控。



### **18、Elasticsearch 对于大数据量（上亿量级）的聚合如何实现？**

Elasticsearch 提供的首个近似聚合是 cardinality 度量。它提供一个字段的基数，即该字段的 distinct 或者 unique 值的数目。它是基于 HLL 算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。



### **19、在并发情况下，Elasticsearch 如果保证读写一致？**

1、可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；

2、另外对于写操作，一致性级别支持 quorum/one/all，默认为 quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。

3、对于读操作，可以设置 replication 为 sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置 replication 为 async 时，也可以通过设置搜索请求参数_preference 为 primary 来查询主分片，确保文档是最新版本。



### **20、如何监控 Elasticsearch 集群状态？**

Marvel 让你可以很简单的通过 Kibana 监控 Elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标。



### **21、介绍下你们电商搜索的整体技术架构。**



![img](https://pic2.zhimg.com/80/v2-6ea22f379bab8d83f9e4697f19e75191_720w.jpg)







### **22、介绍一下你们的个性化搜索方案？**

SEE 基于 word2vec 和 Elasticsearch 实现个性化搜索



### **23、是否了解字典树？**



很多数据结构均能完成字典功能，总结如下。

| 数据结构                       | 优缺点                                                       |
| ------------------------------ | ------------------------------------------------------------ |
| 排序列表Array/List             | 使用二分法查找，不平衡                                       |
| HashMap/TreeMap                | 性能高，内存消耗大，几乎是原始数据的三倍                     |
| Skip List                      | 跳跃表，可快速查找词语，在lucene、redis、Hbase等均有实现。相对于TreeMap等结构，特别适合高并发场景（[Skip List介绍](http://kenby.iteye.com/blog/1187303)） |
| Trie                           | 适合英文词典，如果系统中存在大量字符串且这些字符串基本没有公共前缀，则相应的trie树将非常消耗内存（[数据结构之trie树](http://dongxicheng.org/structure/trietree/)） |
| Double Array Trie              | 适合做中文词典，内存占用小，很多分词工具均采用此种算法（[深入双数组Trie](http://blog.csdn.net/zhoubl668/article/details/6957830)） |
| Ternary Search Tree            | 三叉树，每一个node有3个节点，兼具省空间和查询快的优点（[Ternary Search Tree](http://www.drdobbs.com/database/ternary-search-trees/184410528)） |
| Finite State Transducers (FST) | 有限状态转移机，Lucene 4有开源实现，并大量使用:              |



Trie 的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以

达到提高效率的目的。它有 3 个基本性质：

1、根节点不包含字符，除根节点外每一个节点都只包含一个字符。

2、从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。

3、每个节点的所有子节点包含的字符都不相同。



![img](https://pic3.zhimg.com/80/v2-df4c8cd2e2b1dad444a50bab3f6d9bb2_720w.jpg)







1、可以看到，trie 树每一层的节点数是 26^i 级别的。所以为了节省空间，我们还可以用动态链表，或者用数组来模拟动态。而空间的花费，不会超过单词数×单词长度。

2、实现：对每个结点开一个字母集大小的数组，每个结点挂一个链表，使用左儿子右兄弟表示法记录这棵树；

3、对于中文的字典树，每个节点的子节点用一个哈希表存储，这样就不用浪费太大的空间，而且查询速度上可以保留哈希的复杂度 O(1)。



### **24、拼写纠错是如何实现的？**



1、拼写纠错是基于编辑距离来实现；编辑距离是一种标准的方法，它用来表示经过插入、删除和替换操作从一个字符串转换到另外一个字符串的最小操作步数；

2、编辑距离的计算过程：比如要计算 batyu 和 beauty 的编辑距离，先创建一个7×8 的表（batyu 长度为 5，coffee 长度为 6，各加 2），接着，在如下位置填入黑色数字。其他格的计算过程是取以下三个值的最小值：



如果最上方的字符等于最左方的字符，则为左上方的数字。否则为左上方的数字

+1。（对于 3,3 来说为 0）

左方数字+1（对于 3,3 格来说为 2）

上方数字+1（对于 3,3 格来说为 2）



最终取右下角的值即为编辑距离的值 3。





![img](https://pic4.zhimg.com/80/v2-1f5084b94e47d417b3cebd615ef04647_720w.jpg)



对于拼写纠错，我们考虑构造一个度量空间（Metric Space），该空间内任何关系满足以下三条基本条件：



d(x,y) = 0 -- 假如 x 与 y 的距离为 0，则 x=y

d(x,y) = d(y,x) -- x 到 y 的距离等同于 y 到 x 的距离

d(x,y) + d(y,z) >= d(x,z) -- 三角不等式



1、根据三角不等式，则满足与 query 距离在 n 范围内的另一个字符转 B，其与 A的距离最大为 d+n，最小为 d-n。



2、BK 树的构造就过程如下：每个节点有任意个子节点，每条边有个值表示编辑距离。所有子节点到父节点的边上标注 n 表示编辑距离恰好为 n。比如，我们有棵树父节点是”book”和两个子节点”cake”和”books”，”book”到”books”的边标号 1，”book”到”cake”的边上标号 4。从字典里构造好树后，无论何

时你想插入新单词时，计算该单词与根节点的编辑距离，并且查找数值为d(neweord, root)的边。递归得与各子节点进行比较，直到没有子节点，你就可以创建新的子节点并将新单词保存在那。比如，插入”boo”到刚才上述例子的树中，我们先检查根节点，查找 d(“book”, “boo”) = 1 的边，然后检查标号为1 的边的子节点，得到单”books”。我们再计算距离 d(“books”, “boo”)=2，则将新单词插在”books”之后，边标号为 2。



3、查询相似词如下：计算单词与根节点的编辑距离 d，然后递归查找每个子节点标号为 d-n 到 d+n（包含）的边。假如被检查的节点与搜索单词的距离 d 小于 n，则返回该节点并继续查询。比如输入 cape 且最大容忍距离为 1，则先计算和根的编辑距离 d(“book”, “cape”)=4，然后接着找和根节点之间编辑距离为 3 到5 的，这个就找到了 cake 这个节点，计算 d(“cake”, “cape”)=1，满足条件所以返回 **cake**，然后再找和 cake 节点编辑距离是 0 到 2 的，分别找到 cape 和cart 节点，这样就得到 **cape** 这个满足条件的结果。





![img](https://pic4.zhimg.com/80/v2-d5426155b3c3c0a7e49123954f96e347_720w.jpg)



**最后**，由于这份文档的内容过多，为了节省读者朋友们的时间，我只整理出来了一部分供大家学习参考。



## 一、ES 慢查询之坑

Elasticsearch 是现如今用的最广泛的搜索引擎。它是一个分布式的开源搜索和分析引擎，适用于所有类型的数据，包括文本、数字、地理空间、结构化和非结构化数据。



### **1.1 工作原理：**



**ES 的工作原理**：往 ES 里写数据时，实际是写到磁盘文件，查询时，操作系统会将磁盘文件里的数据自动缓存 `filesystem cache` 里面。如果给 `filesystem cache` 更多的内存，尽量让内存可以容纳所有的 `idx segment file` 索引数据文件，则搜索的时候走内存，性能较好。

![null](https://img.bosszhipin.com/beijin/cms/c29f1b8f83addd64198ec461be1737fc.png)

ES 工作原理

> `坑：` 首先如果访问磁盘那一定很慢，而走缓存会快很多。但如果很多没用的字段数据都丢到缓存里面，则会浪费缓存的空间，所以很多数据还是存在磁盘里面的，那么大部分查询走的数据库，则会带来性能问题。

### **1.2 案例**



ES 节点 3 台机器，每台机器 32 G 内存，总内存 96 G，给 ES JVM 堆内存是 16 G，那么剩下来给 cache 的是 16 G，总共 ES 集群的的 cache 占用内存 48 G ，如果所有的数据占据磁盘空间 600 G，那么每台机器的数据量是 200 G，而查询时，有 150 G 左右的的数据是走磁盘查询的，那么走 cache 的概率是 48 G/ 600 G = 8%，也就是说大量查询是走磁盘的。



### **1.3 避坑指南：**

#### **1.3.1 存储关键信息**

![null](https://img.bosszhipin.com/beijin/cms/ef69c8378c6269f3c7a12a3ce2e8f945.png)

搜索引擎避坑指南

- 将数据中索引字段存到 cache，比如 一行数据有 name、gender、age、city、job 字段，而检索这条数据只需要 name 和 gender 就可以查询出数据，那么 cache 就只需要存 id、name 和 gender 字段。别把所有字段都丢到 cache 里面，纯属浪费空间，资源是有限的。
- 那剩下的字段怎么检索出来？可以把其他字段存到 mysql/hbase 里面。
- hbase 特点：适用于海量数据的在线存储。缺点是不能进行复杂的搜索。根据 name 和 gender 字段从 ES 中拿到 100 条数据 ( 包含 doc id ) ，然后根据 doc id 再去 hbase 中查询每个 doc id 对应的完整数据，将结果组装后返回给前端 ( 需要考虑分页的情况 ) 。

#### **1.3.2 数据预热**

- 将访问量高的数据或者即将访问量高的数据放到 filesystem cache 里面。
- 每隔一段时间就从数据库访问下数据，然后同步到 filesystem cache 里面。

#### **1.3.3 冷热分离**

- 不常访问的数据和经常访问的数据进行隔离。比如 3 台机器存放冷数据的索引，另外 3 台存放热数据的索引。

#### **1.3.4 避免使用关联查询**

- ES 中的关联查询是比较慢的，性能不佳，尽量避免使用。

## 二、ES 架构之坑

通常情况下，我们会使用 ES 的**集群模式**，在集群规模不大的情况下，性能还算可以，但如果集群规模变得很大，则会遇到集群瓶颈，也就是说集群扩大，性能提升甚微，甚至不增反降。



ES 的集群也是采用**中心化**的分布式架构，整个集群只有一个是 Master 节点。而它的职责非常重要：负责整个集群的元数据管理，元数据包含全局的配置信息、索引信息、节点信息，如果元数据发生改变，则需要 master 节点将变更信息发布到集群的其他节点。



另外因为 master 节点的任务处理是单线程的，所以每次处理任务时，需要等待全部节点接收到变更信息，并处理完变更的任务后，才算完成了变更任务。

![null](https://img.bosszhipin.com/beijin/cms/2cb69c376cc9de6dc03f973a209ccfc1.png)

那么这样的架构会带来什么问题：

- **响应时间问题**。如果元数据发生了改变，但某节点 `假死`，比如 JVM 的内存爆了，但是进程还活着，那么响应 master 节点的时间会非常长，进而影响单个同步信息任务的完成时间。
- **任务恢复问题**。有大量恢复任务的时候，任务需要排队，恢复时间变长。
- **任务回调问题**。任务执行完成后，需要回调大量 listener 处理元数据变更。如果分片的数据很大，则处理时间会到 10 秒级，严重影响了集群的恢复能力。

解决方案：采用 ES 的 tribe node 特性实现 ES 多集群。文中后面会介绍下 tribe node 的原理。

## 三、业务场景的坑

ES 的被广泛应用到多个场景，比如查询日志、查询商品资料、数据聚合等。而这些场景的需求又有非常大的差异，这也是一个坑。

- **场景一：前端首页搜索功能**。比如搜索商品，数据实时写入的频率不高，但是读的频率很高。

![null](https://img.bosszhipin.com/beijin/cms/6472765f226e7c953bee0d8da8ace556.png)

**场景二：日志检索的功能。**

日志系统中，我们一般都是 ELK 这种架构模式，对实时写入要求很高，而查询的次数其实不多，毕竟查询日志多工作还是开发和运维人员来做。

![null](https://img.bosszhipin.com/beijin/cms/c33d27681fc9c6c314050b4ef3b671dd.png)

Kibana 界面

- **场景三：监控、分析的功能**。ES 也会被运用到需要监控数据和分析数据的场景中，而这种场景又是对 ES 的内存要求比较高，因为这些分析功能是在内存中完成的，若内存出现太大的压力，则会造成系统的垃圾回收，可能出现短暂的服务抖动。

解决方案：按业务场景划分 ES 集群，同样采用 ES tribe node 功能。

## 四、ES Tribe Node 方案

ES tribe node 功能原理图如下所示：

![null](https://img.bosszhipin.com/beijin/cms/2ae740e532c7850d47a04487edc395ab.png)

有两个 ES 集群，每个集群都有多个 ES 节点。

- Logstash 负责日志搜集。
- Kibana 负责客户端查询，将查询命令传送到 ES Tribe Node。
- ES Tribe Node 还承担了集群管理的职责。