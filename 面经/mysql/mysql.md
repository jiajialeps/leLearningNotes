# **一、数据库基础知识**



## 1、为什么要使用数据库

**1.数据保存在内存**

- 优点：存取速度快
- 缺点：数据不能永久保存

**2.数据保存在文件**

- 优点：数据永久保存
- 缺点：①速度比内存操作慢，频繁的IO操作；②查询数据不方便。

**3.数据保存在数据库**

①数据永久保存；②使用SQL语句，查询方便效率高；③管理数据方便。

## 2、什么是SQL？ 



**定义：**结构化查询语言(Structured Query Language)简称SQL，是一种数据库查询语言。

**作用：**用于存取数据、查询、更新和管理关系数据库系统。

## **3、什么是MySQL?**



MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。在PHP企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。

## 4、数据库三大范式是什么？



- **第一范式：**每个列都不可以再拆分。
- **第二范式：**在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。
- **第三范式：**在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。

在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。

## 5、mysql有关权限的表都有哪几个？ 



MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。

**下面分别介绍一下这些表的结构和内容：**

- user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。
- db权限表：记录各个帐号在各个数据库上的操作权限。
- table_priv权限表：记录数据表级的操作权限。
- columns_priv权限表：记录数据列级的操作权限。
- host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。



## 6、MySQL的binlog有有几种录入格式？分别有什么区别？ 



**有三种格式，statement，row和mixed。**

- statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。
- row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。
- mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。

此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。



# mysql和PostgreSql的区别

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/postgis.png)

# 二、MySQL的事务特性



### 1.什么是事务？百度百科对于事务的定义如下：

事务（Transaction），一般是指要做的或所做的事情。在计算机术语中是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。事务通常由高级数据库操纵语言或编程语言（如SQL，C++或Java）书写的用户程序的执行所引起，并用形如begin transaction和end transaction语句（或函数调用）来界定。事务由事务开始(begin transaction)和事务结束(end transaction)之间执行的全体操作组成。—— [百度百科]

可以看出，事务是一个操作单元，当然，在关系型数据库系统中，事务就是一次SQL的操作，要么都成功，要么都失败。

### 2.事务特性（ACID）：

原子性（Atomicity）：原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生；
一致性（Consistency）：事务前后数据的完整性必须保持一致；
隔离性（Isolation）：事务的隔离性是指多个用户并发访问数据库时，一个用户的事务不能被其它用户的事务所干扰，多个并发事务之间数据要相互隔离；
持久性（Durability）：持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响

### 3.不考虑事务的隔离性会发生什么问题？	

####  脏读

>  指当一个事务正字访问数据，并且对数据进行了修改，而这种数据还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据还没有提交那么另外一个事务读取到的这个数据我们称之为脏数据。依据脏数据所做的操作肯能是不正确的

#### 不可重复读

> 指在一个事务内，多次读同一数据。在这个事务还没有执行结束，另外一个事务也访问该同一数据，那么在第一个事务中的两次读取数据之间，由于第二个事务的修改第一个事务两次读到的数据可能是不一样的，这样就发生了在一个事物内两次连续读到的数据是不一样的，这种情况被称为是不可重复读。

#### 虚读（幻读）

> 一个事务先后读取一个范围的记录，但两次读取的纪录数不同，我们称之为幻象读（两次执行同一条 select 语句会出现不同的结果，第二次读会增加一数据行，并没有说这两次执行是在同一个事务中）

#### 丢失更新

> 两个事务对同一条记录进行操作，后提交的事务，将先提交的事务的修改的数据覆盖了

### 4.隔离级别

对于上述不考虑事务隔离性产生的四个问题我们可以通过事务的隔离级别来解决。

#### 事务的隔离级别有哪些？

| 隔离级别         | 含义                                                         |
| :--------------- | :----------------------------------------------------------- |
| Serializable     | 可避免脏读、不可重复读、虚读情况的发生。（串行化）           |
| Repeatable read  | 可避免脏读、不可重复读情况的发生。（可重复读）不可以避免虚读 |
| Read committed   | 可避免脏读情况发生（读已提交）                               |
| Read uncommitted | 最低级别，以上情况均无法保证。(读未提交)                     |

#### 如何设置事务的隔离级别？

**MySQL**

查看事务隔离级别

> select @@tx_isolation #查询当前事务隔离级别
> mysql中默认的事务隔离级别是 Repeatable read。
> 扩展:oracle 中默认的事务隔离级别是 Read committed

设置事务级别

> set session transaction isolation level #设置事务隔离级别



**JDBC**

> 在jdbc中设置事务隔离级别可以使用java.sql.Connection接口中提供的方法void setTransactionIsolation(int level) throws SQLException
> level取以下Connection 常量之一：
> Connection.TRANSACTION_READ_UNCOMMITTED
> Connection.TRANSACTION_READ_COMMITTED
> Connection.TRANSACTION_REPEATABLE_READ
> Connection.TRANSACTION_SERIALIZABLE
> 注：不能使用 Connection.TRANSACTION_NONE，因为它指定了不受支持的事务。



### 5.丢失更新

如何解决丢失更新的问题？我们可以采用两种方式解决丢失更新。

#### **悲观锁**

悲观锁 （假设丢失更新一定会发生 ） —– 利用数据库内部锁机制，管理事务提供的锁机制，其有两种锁机制：

1、共享锁

> select * from table lock in share mode（读锁、共享锁）

2、排他锁

> select * from table for update （写锁、排它锁）



注：update语句默认添加排它锁

#### **乐观锁**

乐观锁 （假设丢失更新不会发生）——- 采用程序中添加版本字段解决丢失更新问题，在数据表添加版本字段。每次修改过记录后，版本字段都会更新，如果读取是版本字段，与修改时版本字段不一致，说明别人进行修改过数据 （重改） 。



> create table product (
> id int(8),
> name varchar(20),
> updatetime timestamp
> );
>
> insert into product values(1,’冰箱’,null);
> update product set name=’洗衣机’ where id = 1;



### 	6.演示

#### 脏读

```sql
一个事务读取到另一个事务的为提交数据
设置A,B事务隔离级别为Read uncommitted
set session transaction isolation level read uncommitted;

1.在A事务中
    start transaction;
    update account set money=money-500 where name='aaa';
    update account set money=money+500 where name='bbb';

2.在B事务中
    start transaction;
    select * from account;
    这时，B事务读取时，会发现，钱已经汇完。那么就出现了脏读。
当A事务提交前，执行rollback，在commit， B事务在查询，就会发现，钱恢复成原样  
            也出现了两次查询结果不一致问题，出现了不可重复读.

```





#### 解决脏读问题

```sql
将事务的隔离级别设置为 read committed来解决脏读
设置A,B事务隔离级别为   Read committed
set session transaction isolation level  read committed;

1.在A事务中
    start transaction;
    update account set money=money-500 where name='aaa';
    update account set money=money+500 where name='bbb';

2.在B事务中
    start transaction;
    select * from account;
这时B事务中，读取信息时，是不能读到A事务未提交的数据的，也就解决了脏读。

让A事务，提交数据 commit;   
这时，在查询，这次结果与上一次查询结果又不一样了，还存在不可重复读。

```



#### 解决不可重复读

```sql
将事务的隔离级别设置为Repeatable read来解决不可重复读。
设置A,B事务隔离级别为Repeatable read;
set session transaction isolation level  Repeatable read;

1.在A事务中
    start transaction;
    update account set money=money-500 where name='aaa';
    update account set money=money+500 where name='bbb';

2.在B事务中
    start transaction;
    select * from account;

当A事务提交后commit;B事务在查询，与上次查询结果一致，解决了不可重复读。

```

#### 设置事务隔离级别 Serializable ,它可以解决所有问题

```sql
set session transaction isolation level Serializable;

如果设置成这种隔离级别，那么会出现锁表。也就是说，一个事务在对表进行操作时，其它事务操作不了。

```






# 三、mysql数据类型





![img](https://pic4.zhimg.com/80/v2-ed4cc2d902a4fe3a73279cdb0cefb34f_720w.jpg)





![img](https://pic4.zhimg.com/80/v2-2d0ee27282d3a76f9f93c399d809fe93_720w.jpg)







### **1.整数类型，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT。**

- 分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。
- **长度：整数类型可以被指定长度。**例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配合使用才有意义。
- **例子。**假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数据库实际存储数据为00012。

### **2.实数类型，包括FLOAT、DOUBLE、DECIMAL。**

- DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。
- 而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。
- 计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。

### **3.字符串类型，包括VARCHAR、CHAR、TEXT、BLOB。**

- VARCHAR用于存储可变长字符串，它比定长类型更节省空间。
- VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2字节表示。
- VARCHAR存储的内容超出设置的长度时，内容会被截断。
- CHAR是定长的，根据定义的字符串长度分配足够的空间。
- CHAR会根据需要使用空格进行填充方便比较。
- CHAR适合存储很短的字符串，或者所有值都接近同一个长度。
- CHAR存储的内容超出设置的长度时，内容同样会被截断。

**使用策略：**

对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。

对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。

使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。

尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。

### **4.枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。**

- 有时可以使用ENUM代替常用的字符串类型。
- ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。
- ENUM在内部存储时，其实存的是整数。
- 尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。
- 排序是按照内部存储的整数

### **5.日期和时间类型，尽量使用timestamp，空间效率高于datetime，**用整数保存时间戳通常不方便处理。

- 如果需要存储微妙，可以使用bigint存储。
- 看到这里，这道真题是不是就比较容易回答了。



# 四、引擎



## 1.MySQL存储引擎MyISAM与InnoDB区别 



存储引擎Storage engine：MySQL中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。



**常用的存储引擎有以下：**

- Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。
- MyIASM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。
- MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。



## 2.MyISAM与InnoDB区别 



![img](https://pic3.zhimg.com/80/v2-9f3fd70636717718cc776a96e51b1cda_720w.jpg)





## 3.MyISAM索引与InnoDB索引的区别？ 



- InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。
- InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。
- MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。
- InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。



## 4.InnoDB引擎的4大特性 



- 插入缓冲（insert buffer)
- 二次写(double write)
- 自适应哈希索引(ahi)
- 预读(read ahead)

## 5.存储引擎选择 



**如果没有特别的需求，使用默认的Innodb即可。**

- MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。
- Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如OA自动化办公系统。

  

#  五、索引

## 1.什么是索引？

**索引是一种特殊的文件**(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。

**索引是一种数据结构。**数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。

**更通俗的说，索引就相当于目录。**为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。



## **2.索引有哪些优缺点？**

**1).索引的优点**

可以大大加快数据的检索速度，这也是创建索引的最主要的原因。

通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。

**2).索引的缺点**

时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率；

空间方面：索引需要占物理空间。



## **3.索引使用场景（重点）**

**1).where**



![img](https://pic4.zhimg.com/80/v2-92609520847a2e172d269919839d365f_720w.jpg)



上图中，根据id查询记录，因为id字段仅建立了主键索引，因此此SQL执行可选的索引只有主键索引，如果有多个，最终会选一个较优的作为检索的依据。

-- 增加一个没有建立索引的字段altertableinnodb1addsexchar(1);-- 按sex检索时可选的索引为nullEXPLAINSELECT*frominnodb1wheresex='男';



![img](https://pic1.zhimg.com/80/v2-b50593cb8a8ed8ec507cf4507009bd08_720w.jpg)



可以尝试在一个字段未建立索引时，根据该字段查询的效率，然后对该字段建立索引（alter table 表名 add index(字段名)），同样的SQL执行的效率，你会发现查询效率会有明显的提升（数据量越大越明显）。



**2).order by**

当我们使用order by将查询结果按照某个字段排序时，如果该字段没有建立索引，那么执行计划会将查询出的所有数据使用外部排序（将数据从硬盘分批读取到内存使用内部排序，最后合并排序结果），这个操作是很影响性能的，因为需要将查询涉及到的所有数据从磁盘中读到内存（如果单条数据过大或者数据量过多都会降低效率），更无论读到内存之后的排序了。

但是如果我们对该字段建立索引alter table 表名 add index(字段名)，那么由于索引本身是有序的，因此直接按照索引的顺序和映射关系逐条取出数据即可。而且如果分页的，那么只用取出索引表某个范围内的索引对应的数据，而不用像上述那取出所有数据进行排序再返回某个范围内的数据。（从磁盘取数据是最影响性能的）



**3).join**

对join语句匹配关系（on）涉及的字段建立索引能够提高效率

**4).索引覆盖**

如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。

这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。



## **4.索引有哪几种类型？**

**1).主键索引:**数据列不允许重复，不允许为NULL，一个表只能有一个主键。

**2).唯一索引:** 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。

可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引

可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引

**3).普通索引:** 基本的索引类型，没有唯一性的限制，允许为NULL值。

可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引

可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引

**4).全文索引：** 是目前搜索引擎使用的一种关键技术。

可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引



## 5.索引的数据结构（b树，hash） 

索引的数据结构和具体存储引擎的实现有关，在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。

对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。

**1).B树索引**

mysql通过存储引擎取数据，基本上90%的人用的就是InnoDB了，按照实现方式分，InnoDB的索引类型目前只有两种：BTREE（B树）索引和HASH索引。

B树索引是Mysql数据库中使用最频繁的索引类型，基本所有存储引擎都支持BTree索引。通常我们说的索引不出意外指的就是（B树）索引（实际是用B+树实现的，因为在查看表索引时，mysql一律打印BTREE，所以简称为B树索引）



![img](https://pic1.zhimg.com/80/v2-0d45ea135db8931a3be9c4bfcb601290_720w.jpg)



**查询方式：**

主键索引区:PI(关联保存的时数据的地址)按主键查询,

普通索引区:si(关联的id的地址,然后再到达上面的地址)。所以按主键查询,速度最快

**B+tree性质：**

n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。

所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。

所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。

B+ 树中，数据对象的插入和删除仅在叶节点上进行。

B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。



**2).哈希索引**

简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在mysql中用哈希索引时，主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，与这条数据的行指针一并存入Hash表的对应位置；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。当然这只是简略模拟图。



![img](https://pic4.zhimg.com/80/v2-ef44d76f6b1c74bb43933cfedd56545f_720w.jpg)



## **6.索引的基本原理**

索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。**索引的原理很简单，就是把无序的数据变成有序的查询。**

把创建了索引的列的内容进行排序

对排序结果生成倒排表

在倒排表内容上拼上数据地址链

在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据



## **7.索引算法有哪些？**

索引算法有 BTree算法和Hash算法

**1).BTree算法**

BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,>,>=,<,<=和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量， 例如：

-- 只要它的查询条件是一个不以通配符开头的常量select*fromuserwherenamelike'jack%';-- 如果一通配符开头，或者没有使用常量，则不会使用索引，例如：select*fromuserwherenamelike'%jack';

**2).Hash算法**

Hash Hash索引只能用于对等比较，例如=,<=>（相当于=）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到页节点这样多次IO访问，所以检索效率远高于BTree索引。



## **8.索引设计的原则？**

**1).适合索引的列是出现在where子句中的列，**或者连接子句中指定的列。

**2).基数较小的类，**索引效果较差，没有必要在此列建立索引。

**3).使用短索引，**如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间。

**4).不要过度索引。**索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。



## **9.创建索引的原则（重中之重）**

索引虽好，但也不是无限制的使用，最好符合一下几个原则

最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

较频繁作为查询条件的字段才去创建索引

更新频繁字段不适合创建索引

若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)

尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。

定义有外键的数据列一定要建立索引。

对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。

对于定义为text、image和bit的数据类型的列不要建立索引。



## **10.创建索引的三种方式，删除索引**

**1).第一种方式：在执行CREATE TABLE时创建索引**

CREATETABLEuser_index2 (idINTauto_increment PRIMARYKEY,first_nameVARCHAR(16),last_nameVARCHAR(16),id_cardVARCHAR(18),informationtext,KEYname(first_name, last_name),FULLTEXTKEY(information),UNIQUEKEY(id_card));

**2).第二种方式：使用ALTER TABLE命令去增加索引**

ALTERTABLEtable_nameADDINDEXindex_name (column_list);

ALTER TABLE用来创建普通索引、UNIQUE索引或PRIMARY KEY索引。

其中table_name是要增加索引的表名，column_list指出对哪些列进行索引，多列时各列之间用逗号分隔。

索引名index_name可自己命名，缺省时，MySQL将根据第一个索引列赋一个名称。另外，ALTER TABLE允许在单个语句中更改多个表，因此可以在同时创建多个索引。

**3).第三种方式：使用CREATE INDEX命令创建**

CREATEINDEXindex_nameONtable_name (column_list);

CREATE INDEX可对表增加普通索引或UNIQUE索引。（但是，不能创建PRIMARY KEY索引）

**4).删除索引**

根据索引名删除普通索引、唯一索引、全文索引：alter table 表名 drop KEY 索引名

altertableuser_indexdropKEYname;altertableuser_indexdropKEYid_card;altertableuser_indexdropKEYinformation;

**删除主键索引：**alter table 表名 drop primary key（因为主键只有一个）。这里值得注意的是，如果主键自增长，那么不能直接执行此操作（自增长依赖于主键索引）：



![img](https://pic3.zhimg.com/80/v2-19c3d9d913b0029edb18179e24ce7566_720w.jpg)



**需要取消自增长再行删除：**

altertableuser_index-- 重新定义字段MODIFYidint,dropPRIMARYKEY

但通常不会删除主键，因为设计主键一定与业务逻辑无关。



## **11.创建索引时需要注意什么？**

**非空字段：**应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值；

**取值离散大的字段：**（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高；

**索引字段越小越好：**数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。

## **12.使用索引查询一定能提高查询的性能吗？为什么？**

**通常，通过索引查询数据比全表扫描要快。**但是我们也必须注意到它的代价。

索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。这意味着每条记录的INSERT，DELETE，UPDATE将为此多付出4，5 次的磁盘I/O。因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。使用索引查询不一定能提高查询性能，索引范围查询(INDEX RANGE SCAN)适用于两种情况:

基于一个范围的检索，一般查询返回结果集小于表中记录数的30%

基于非唯一性索引的检索

## 13.百万级别或以上的数据如何删除？

**关于索引：**由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。

所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。



所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟）

然后删除其中无用数据（此过程需要不到两分钟）

删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。

与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。

## 14.前缀索引 

语法：index(field(10))，使用字段值的前10个字符建立索引，默认是使用字段的全部内容建立索引。



前提：前缀的标识度高。比如密码就适合建立前缀索引，因为密码几乎各不相同。



实操的难度：在于前缀截取的长度。



我们可以利用select count(*)/count(distinct left(password,prefixLen));，通过从调整prefixLen的值（从1自增）查看不同前缀长度的一个平均匹配度，接近1时就可以了（表示一个密码的前prefixLen个字符几乎能确定唯一一条记录）



## **15.什么是最左前缀原则？什么是最左匹配原则？**

顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。

最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式

## 16.B树和B+树的区别 

在B树中，你可以将键和值存放在内部节点和叶子节点；但在B+树中，内部节点都是键，没有值，叶子节点同时存放键和值。

B+树的叶子节点有一条链相连，而B树的叶子节点各自独立。



![img](https://pic3.zhimg.com/80/v2-5307acbcb413f6ddd76db5438b93f1e2_720w.jpg)



**1.使用B树的好处**

B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。



**2.使用B+树的好处**

由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间



## 17.Hash索引和B+树所有有什么区别或者说优劣呢? 

**首先要知道Hash索引和B+树索引的底层实现原理：**

hash索引底层就是hash表，进行查找时，调用一次hash函数就可以获取到相应的键值，之后进行回表查询获得实际数据。

B+树底层实现是多路平衡查找树。

对于每一次的查询都是从根节点出发，查找到叶子节点方可以获得所查键值，然后根据查询判断是否需要回表查询数据。



**那么可以看出他们有以下的不同：**

hash索引进行等值查询更快(一般情况下)，但是却无法进行范围查询。

因为在hash索引中经过hash函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询。

而B+树的的所有节点皆遵循(左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。



**hash索引不支持使用索引进行排序，原理同上。**

hash索引不支持模糊查询以及多列索引的最左前缀匹配。原理也是因为hash函数的不可预测。AAAA和AAAAB的索引没有相关性。

hash索引任何时候都避免不了回表查询数据，而B+树在符合某些条件(聚簇索引，覆盖索引等)的时候可以只通过索引完成查询。

hash索引虽然在等值查询上较快，但是不稳定。性能不可预测，当某个键值存在大量重复的时候，发生hash碰撞，此时效率可能极差。而B+树的查询效率比较稳定，对于所有的查询都是从根节点到叶子节点，且树的高度较低。

因此，在大多数情况下，直接选择B+树索引可以获得稳定且较好的查询速度。而不需要使用hash索引。



## 18.数据库为什么使用B+树而不是B树？

**1.B树只适合随机检索，而B+树同时支持随机检索和顺序检索；**

**2.B+树空间利用率更高，可减少I/O次数，磁盘读写代价更低。**

一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。

B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素；

**注**：I/o 是指在计算机系统中I/O就是输入(Input)和输出(Output)的意思。



**3.B+树的查询效率更加稳定。**

B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。

而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。

B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。

B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。

增删文件（节点）时，效率更高。因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。

**4.B+树在满足聚簇索引和覆盖索引的时候不需要回表查询数据。**

在B+树的索引中，叶子节点可能存储了当前的key值，也可能存储了当前的key值以及整行的数据，这就是聚簇索引和非聚簇索引。

在InnoDB中，只有主键索引是聚簇索引，如果没有主键，则挑选一个唯一键建立聚簇索引。如果没有唯一键，则隐式的生成一个键来建立聚簇索引。

当查询使用聚簇索引时，在对应的叶子节点，可以获取到整行数据，因此不用再次进行回表查询。



## 19.什么是聚簇索引？何时使用聚簇索引与非聚簇索引 

**1.什么是聚簇索引？**

聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据

非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因

**澄清一个概念**：innodb中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值



**2.何时使用聚簇索引与非聚簇索引？**



![img](https://pic3.zhimg.com/80/v2-fd11f44ea53d4475e262ee648022731a_720w.jpg)





**3.非聚簇索引一定会回表查询吗？**

不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。

举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age < 20的查询时，在索引的叶子节点上，已经包含了age信息，不会再次进行回表查询。



## 20.联合索引是什么？为什么需要注意联合索引中的顺序？ 

MySQL可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。

**具体原因为:**

MySQL使用索引时需要索引有序，假设现在建立了"name，age，school"的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。

当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。

因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。





# 六、说说分库分表？

随着用户量的激增和时间的堆砌，存在数据库里面的数据越来越多，此时的数据库就会产生瓶颈，出现资源报警、查询慢等场景。

首先单机数据库所能承载的连接数、I/O及网络的吞吐等都是有限的，所以当并发量上来了之后，数据库就渐渐顶不住了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/eSdk75TK4nEWAm9e6WibicVVnudWpTXSpbLKZHAvclSNUEknzcbZW02kVoUtBInLibrATW08HISruNjxULAicJSiakw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

再则，如果单表的数据量过大，查询的性能也会下降。因为数据越多 B+ 树就越高，树越高则查询 I/O 的次数就越多，那么性能也就越差。

因为上述的原因，不得已就得上分库分表了。

把以前存在一个数据库实例里的数据拆分成多个数据库实例，部署在不同的服务器中，这是分库。

把以前存在一张表里面的数据拆分成多张表，这是分表。

一般而言：

- 分表：是为了解决由于单张表数据量多大，而导致查询慢的问题。大致三、四千万行数据就得拆分，不过具体还是得看每一行的数据量大小，有些字段都很小的可能支持更多行数，有些字段大的可能一千万就顶不住了。
- 分库：是为了解决服务器资源受单机限制，顶不住高并发访问的问题，把请求分配到多台服务器上，降低服务器压力。

# 你们一般怎么分库的？

一般分库都是按照业务划分的，比如订单库、用户库等等。

有时候会针对一些特殊的库再作切分，比如一些活动相关的库都做了拆分。

因为做活动的时候并发可能会比较高，怕影响现有的核心业务，所以即使有关联，也会单独做拆分。

![图片](https://mmbiz.qpic.cn/mmbiz_png/eSdk75TK4nEWAm9e6WibicVVnudWpTXSpb6yoRNpHXmcfQqEqlOKaXxPqjotTEiaib2uCoXtWA3AebKEgiapcW6f3Aw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

# 那你觉得分库会带来什么问题呢？

> 首先是事务的问题。

我们使用关系型数据库，有很大一点在于它保证事务完整性。

而分库之后单机事务就用不上了，必须使用分布式事务来解决，而分布式事务基本的都是残缺的(我之前文章把分布式事务汇总了一波，后台搜索分布式事务就有了)。

这是很重要的一点需要考虑。

> 连表 JOIN 问题

在一个库中的时候我们还可以利用 JOIN 来连表查询，而跨库了之后就无法使用 JOIN 了。

此时的解决方案就是在业务代码中进行关联，也就是先把一个表的数据查出来，然后通过得到的结果再去查另一张表，然后利用代码来关联得到最终的结果。

这种方式实现起来稍微比较复杂，不过也是可以接受的。

还有可以适当的冗余一些字段。比如以前的表就存储一个关联 ID，但是业务时常要求返回对应的 Name 或者其他字段。这时候就可以把这些字段冗余到当前表中，来去除需要关联的操作。

# 那你们怎么分表的？

分表其实有两种：

- 垂直分表
- 水平分表

垂直分表，来看个图，很直观：

![图片](https://mmbiz.qpic.cn/mmbiz_png/eSdk75TK4nEWAm9e6WibicVVnudWpTXSpb4eaOkdIoQsksMWPEyDJvonV78gfmqqelicN2FFfb7RZ7OJwFF6cSdEg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

垂直分表就是把一些不常用的大字段剥离出去。

像上面的例子：用户名是很常见的搜索结果，性别和年龄占用的空间又不大，而地址和个人简介占用的空间相对而言就较大，我们都知道一个数据页的空间是有限的，把一些无用的数据拆分出去，一页就能存放更多行的数据。

内存存放更多有用的数据，就减少了磁盘的访问次数，性能就得到提升。

水平分表，则是因为一张表内的数据太多了，上文也提到了数据越多 B+ 树就越高，访问的性能就差，所以进行水平拆分。

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

其实不管这些，浅显的理解下，在一百个数据里面找一个数据快，还是在一万个数据里面找一个数据快？

即使有索引，那厚的书目录多，翻目录也慢~

# 那分表会有什么问题？

垂直分表还好，就是需要关联一下，而水平分表就有点麻烦了。

> 排序、count、分页问题

如果一个用户的数据被拆分到多个表中，那查询结果分页就不像以前单张表那样直接就能查出来了，像 count 操作也是一样的。

只能由业务代码来实现或者用中间件将各表中的数据汇总、排序、分页然后返回。

像 count 操作的结果其实可以缓存下来，然后每次数据增删都更新计数。

> 路由问题

分表的路由可以分：

- Hash 路由
- 范围路由
- 路由表

Hash 路由，其实就是选择表中的某一列，然后进行 Hash 运算，将 Hash 运算得到的结果再对子表数进行取模，这样就能均匀的将数据分到不同的子表上。

这跟 HashMap 选哪个桶是一样的原理。

优点就是数据分布均匀。

缺点就是增加子表的时候麻烦，想想 HashMap的扩容，是不是得搬迁数据？这个分表也是一样的，我们可都知道，数据迁移一件麻烦事！

范围路由，其实很简单，可以是时间，也可以是地址，表示一定的范围的即可。

比如本来一张 User 表，我可以分 User_HZ、User_BJ、User_SH，按照地名来划分 User。

再比如 log 表，我可以将表分为 log_202103、 log_202104，把日志按照年月来划分。

优点就是相对而言比较容易扩展，比如现在来个 GZ，那就加个 User_GZ。如果到了 5 月，那就建个 log_202105。

缺点就是数据可能分布不均匀，例如 BJ 的用户特别多或者某个月搞了促销，日志量特别大，等等。

路由表，就是专门搞个表来记录路由信息，来看个图就很清楚了。

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

从图中我们就能得知，UserID 为 2 的用户数据在要去 User_3 这个用户表查询。

优点就是灵活咯，如果要迁移数据，直接迁移然后路由表一改就完事儿了~

缺点就是得多查一次，每次查询都需要访问路由表，不过这个一般会做缓存的。

> 全局主键问题

以前单表的时候很简单，就是主键自增，现在分表了之后就有点尴尬了。

所以需要一些手段来保证全局主键唯一。

1. 还是自增，只不过自增步长设置一下。比如现在有三张表，步长设置为3，三张表 ID 初始值分别是1、2、3。这样第一张表的 ID 增长是 1、4、7。第二张表是2、5、8。第三张表是3、6、9，这样就不会重复了。
2. UUID，这种最简单，但是不连续的主键插入会导致严重的页分裂，性能比较差。
3. 分布式 ID，比较出名的就是 Twitter 开源的 sonwflake 雪花算法，具体就不展开了，不然就又是一篇文章了，简单点利用 redis 来递增也行。

# 那上面说的路由问题的 Sharding-Key 如何设计呢？

我们分表是按照某个列来拆分的，那个列就是 Sharding-Key，查询的时候必须带上这个列才行。

例如上面提到的  log_202103，那表明查询条件一定得带上日期，这样才能找到正确的表。

所以设计上得考虑查询的条件来作为 Sharding-Key。

举个常常会被问的订单表 Sharding-Key 例子。

你想着查找订单的时候会通过订单号去找，所以应该利用订单 ID 来作为 Sharding-Key。

但是你想想，你打开外卖软件想查找你的历史订单的时候，你是没有订单 ID 的，你只有你的 UserID，那此时只能把所有子表都通过 UserID 遍历一遍，这样效率就很低了！

所以你想着那用 UserID 来作为 Sharding-Key 吧！

但是，商家呢？商家肯定关心自己今天卖了多少单，所以他也要查找订单，但他只有自己的商家 ID，所以如果要查询订单，只能把所有子表都通过商家 ID 遍历一遍，这样效率就很低了！

所以 Sharding-Key 是满足不了所有查询需求的，只能曲线救国。

一般做法就是冗余数据。

将订单同步到另一张表中给商家使用，这个表按商家 ID 来作为 Sharding-Key，也可以将数据同步到 ES 中。一般而言这里的数据同步都是异步处理，不会影响正常流程。



# 七、如何保证redis和mysql读写一致

## **前言**

四月份的时候，有位好朋友去美团面试。他说，被问到Redis与MySQL双写一致性如何保证？这道题其实就是在问缓存和数据库在双写场景下，一致性是如何保证的？本文将跟大家一起来探讨如何回答这个问题。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRHLOO7rkqKpuUVApd6OqOhyTyQrRYrE4cOmo5HgPZunajON2lb9E6ZA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 公众号：**捡田螺的小男孩**

## **谈谈一致性**

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRS99F4S1uorz4BGI1I5sv0WYenLTgREsCMMtMwxEHs5Adykt6xZP47g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。

- **强一致性**：这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大
- **弱一致性**：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态
- **最终一致性**：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型

## **三个经典的缓存模式**

缓存可以提升性能、缓解数据库压力，但是使用缓存也会导致数据**不一致性**的问题。一般我们是如何使用缓存呢？有三种经典的缓存使用模式：

- Cache-Aside Pattern
- Read-Through/Write-through
- Write-behind

### Cache-Aside Pattern

Cache-Aside Pattern，即**旁路缓存模式**，它的提出是为了尽可能地解决缓存与数据库的数据不一致问题。

#### Cache-Aside读流程

**Cache-Aside Pattern**的读请求流程如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRAZ68JX9LyG7LT6bJfOHgWhbeXypQd0RE3LcicVxian4UtLW1enj6icCkw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Cache-Aside读请求

1. 读的时候，先读缓存，缓存命中的话，直接返回数据
2. 缓存没有命中的话，就去读数据库，从数据库取出数据，放入缓存后，同时返回响应。

#### Cache-Aside 写流程

**Cache-Aside Pattern**的写请求流程如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCR6HyLYMqNc3o0JAQaSaLqhThRAtNml8mNqBpTDRNAicAXOkakeE00bkA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Cache-Aside写请求

更新的时候，先**更新数据库，然后再删除缓存**。

### Read-Through/Write-Through（读写穿透）

**Read/Write-Through**模式中，服务端把缓存作为主要数据存储。应用程序跟数据库缓存交互，都是通过**抽象缓存层**完成的。

#### Read-Through

**Read-Through**的简要流程如下

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRNg934JicTpmXwHn9orTn4dSYzxF4nA5bAvS78OSylNrTBSr9A7icL74Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Read-Through简要流程

1. 从缓存读取数据，读到直接返回
2. 如果读取不到的话，从数据库加载，写入缓存后，再返回响应。

这个简要流程是不是跟**Cache-Aside**很像呢？其实**Read-Through**就是多了一层**Cache-Provider**而已，流程如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRcUc7P99fMX2P9zHice4nB6S4crMdIOKt7euG6tPFbdRdshVPT7wSDWA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Read-Through流程

Read-Through实际只是在**Cache-Aside**之上进行了一层封装，它会让程序代码变得更简洁，同时也减少数据源上的负载。

#### Write-Through

**Write-Through**模式下，当发生写请求时，也是由**缓存抽象层**完成数据源和缓存数据的更新,流程如下：![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRcOlpgny0miaY8Cll4uAsv96WO1JdBjiaqYUv2TX2XiaP2ZicSDJ99GOS1A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### Write-behind （异步缓存写入）

**Write-behind** 跟Read-Through/Write-Through有相似的地方，都是由**Cache Provider**来负责缓存和数据库的读写。它们又有个很大的不同：**Read/Write-Through**是同步更新缓存和数据的，**Write-Behind**则是只更新缓存，不直接更新数据库，通过**批量异步**的方式来更新数据库。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRfbHibY85ws4ejO2iaCEGuJicXYKIqt4gpdxVsqdxFksutMl0TwGKicdrrw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Write behind流程

这种方式下，缓存和数据库的一致性不强，**对一致性要求高的系统要谨慎使用**。但是它适合频繁写的场景，MySQL的**InnoDB Buffer Pool机制**就使用到这种模式。

## **操作缓存的时候，到底是删除缓存呢，还是更新缓存？**

日常开发中，我们一般使用的就是**Cache-Aside**模式。有些小伙伴可能会问， **Cache-Aside**在写入请求的时候，为什么是**删除缓存而不是更新缓存**呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRjKRUkFM9hicTx41HbEF3ag8E1w3sTSAPArSrWukm2mgHBHqiczEK29Pg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Cache-Aside写入流程

我们在操作缓存的时候，到底应该删除缓存还是更新缓存呢？我们先来看个例子：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRQ6w0X82sktmFkLHHUK7ib6giczGKYgflkUJC0flDDGsficibpCLU6B1u4g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1. 线程A先发起一个写操作，第一步先更新数据库
2. 线程B再发起一个写操作，第二步更新了数据库
3. 由于网络等原因，线程B先更新了缓存
4. 线程A更新缓存。

这时候，缓存保存的是A的数据（老数据），数据库保存的是B的数据（新数据），数据**不一致**了，脏数据出现啦。如果是**删除缓存取代更新缓存**则不会出现这个脏数据问题。

**更新缓存相对于删除缓存**，还有两点劣势：

- 如果你写入的缓存值，是经过复杂计算才得到的话。更新缓存频率高的话，就浪费性能啦。
- 在写数据库场景多，读数据场景少的情况下，数据很多时候还没被读取到，又被更新了，这也浪费了性能呢(实际上，写多的场景，用缓存也不是很划算的,哈哈)

## **双写的情况下，先操作数据库还是先操作缓存？**

`Cache-Aside`缓存模式中，有些小伙伴还是会有疑问，在写请求过来的时候，为什么是**先操作数据库呢**？为什么**不先操作缓存**呢？

假设有A、B两个请求，请求A做更新操作，请求B做查询读取操作。![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCR5feBdDendzWqLo1AjBdmoPPiaVROOOnvbyia7QxeomOy863JlrlxHTBw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1. 线程A发起一个写操作，第一步del cache
2. 此时线程B发起一个读操作，cache miss
3. 线程B继续读DB，读出来一个老数据
4. 然后线程B把老数据设置入cache
5. 线程A写入DB最新的数据

酱紫就有问题啦，**缓存和数据库的数据不一致了。缓存保存的是老数据，数据库保存的是新数据**。因此，Cache-Aside缓存模式，选择了先操作数据库而不是先操作缓存。

- 个别小伙伴可能会问，先操作数据库再操作缓存，不一样也会导致数据不一致嘛？它俩又不是原子性操作的。这个是**会的**，但是这种方式，一般因为删除缓存失败等原因，才会导致脏数据，这个概率就很低。小伙伴们可以画下操作流程图，自己先分析下哈。接下来我们再来分析这种**删除缓存失败**的情况，**如何保证一致性**。

## **数据库和缓存数据保持强一致，可以嘛？**

实际上，没办法做到数据库与缓存**绝对的一致性**。

- 加锁可以嘛？并发写期间加锁，任何读操作不写入缓存？
- 缓存及数据库封装CAS乐观锁，更新缓存时通过lua脚本？
- 分布式事务，3PC？TCC？

其实，这是由**CAP理论**决定的。缓存系统适用的场景就是非强一致性的场景，它属于CAP中的AP。**个人觉得，追求绝对一致性的业务场景，不适合引入缓存**。

> ★
>
> CAP理论，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。
>
> ”

但是，通过一些方案优化处理，是可以**保证弱一致性，最终一致性**的。

## **3种方案保证数据库与缓存的一致性**

### 缓存延时双删

有些小伙伴可能会说，并不一定要先操作数据库呀，采用**缓存延时双删**策略，就可以保证数据的一致性啦。什么是延时双删呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRx1UG6aQZibAnC9yS5UezHXE4H8QXN9ahFyVxbPwiaPRXtm96logZpSAA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)延时双删流程

1. 先删除缓存
2. 再更新数据库
3. 休眠一会（比如1秒），再次删除缓存。

这个休眠一会，一般多久呢？都是1秒？

> ★
>
> 这个休眠时间 =  读业务逻辑数据的耗时 + 几百毫秒。为了确保读请求结束，写请求可以删除读请求可能带来的缓存脏数据。
>
> ”

这种方案还算可以，只有休眠那一会（比如就那1秒），可能有脏数据，一般业务也会接受的。但是如果**第二次删除缓存失败**呢？缓存和数据库的数据还是可能不一致，对吧？给Key设置一个自然的expire过期时间，让它自动过期怎样？那业务要接受**过期时间**内，数据的不一致咯？还是有其他更佳方案呢？

### 删除缓存重试机制

不管是**延时双删**还是**Cache-Aside的先操作数据库再删除缓存**，都可能会存在第二步的删除缓存失败，导致的数据不一致问题。可以使用这个方案优化：删除失败就多删除几次呀,保证删除缓存成功就可以了呀~ 所以可以引入**删除缓存重试机制**

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRDvhO050QGHAEyVgsvtmSBg6OeNsDLt0PEWajMHwzElgKLpqcAfZ1xA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)删除缓存重试流程

1. 写请求更新数据库
2. 缓存因为某些原因，删除失败
3. 把删除失败的key放到消息队列
4. 消费消息队列的消息，获取要删除的key
5. 重试删除缓存操作

### 读取biglog异步删除缓存

重试删除缓存机制还可以吧，就是会造成好多**业务代码入侵**。其实，还可以这样优化：通过数据库的**binlog来异步淘汰key**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1Pmpzr8VflicsVn7fr1ksck7SCRM182GqMXbibp67wye0xXvUqrAEQiby4VLrIiaLfia3VRYm6CHXraJuXiaXQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

以mysql为例吧

- 可以使用阿里的canal将binlog日志采集发送到MQ队列里面
- 然后通过ACK机制确认处理这条更新消息，删除缓存，保证数据缓存一致性